# Week 5: LLM 평가 패러다임과 벤치마크

## 1. 평가의 지형 변화: 전통 지표의 한계와 의미 기반 평가 필요성

대규모 언어모델(LLM)의 성능을 평가하는 방법론은 모델의 발전과 함께 급격히 변화하고 있다. 기존의 **BLEU**, **ROUGE** 같은 전통적 평가 지표들이 가진 한계를 인식하고, **의미 기반 평가**와 **LLM-as-a-Judge** 패러다임으로의 전환이 일어나고 있다.

### 1.1 전통적 평가 지표의 한계

자연어 생성(NLG) 모델의 품질 평가에서는 오랫동안 **BLEU**, **ROUGE** 등의 정량 지표가 사용되어 왔다. 이들은 다음과 같은 특징을 가진다:

- **BLEU**: 기계번역에서 참고문장과 생성문장의 **n-gram 겹침 정도**를 측정
- **ROUGE**: 요약문에서 **중요 단어/구의 재현율**을 계산
- **공통점**: 모두 **표면적인 문자열 일치**에 기반한 평가

이러한 전통 지표들은 다음과 같은 **근본적인 한계**를 가진다:

1. **의미적 이해 부족**: 동의어 사용이나 표현 변화로 의미는 같지만 단어가 다른 경우 낮은 점수
2. **잘못된 의미에 대한 높은 점수**: 표현은 비슷하나 의미가 잘못된 경우에도 높은 점수 부여
3. **창의성 평가 불가**: LLM의 창의적이거나 주관적인 답변 품질을 구별하지 못함
4. **사실성 무시**: 사실성(팩트 정확성)이나 일관성 같은 중요한 측면을 반영하지 못함

### 1.2 의미 기반 평가의 등장

전통 지표의 한계를 극복하기 위해 **의미 기반 평가 방법**들이 등장했다:

#### BERTScore와 SentenceMover

- **임베딩 공간에서의 유사도**로 문장 의미 유사성 측정
- BLEU 대비 향상된 상관도 달성
- **의미적 유사성**을 더 잘 포착

#### BLEURT

- **사전 학습된 평가지표**
- 학습된 의미 판별을 통해 사람 평가와 더 높은 정합성
- **맥락적 이해** 능력 향상

### 1.3 LLM-as-a-Judge 패러다임의 등장

최근에는 **대형 언어모델(LLM)을 평가자**로 활용하는 혁신적 접근이 등장했다:

- **LLM-as-a-Judge**: 사람 대신 LLM이 다른 LLM의 출력을 채점하거나 평점 부여
- **복잡한 의미 파악**: 맥락적 판단까지 수행 가능
- **개방형 생성 과제**: 정답이 정해지지 않은 과제에서도 평가 가능
- **주관적 기준 반영**: 사람 평가자의 판단 기준을 학습하여 반영

이러한 변화는 **BLEU/ROUGE 중심의 전통 지표에서 벗어나** LLM의 풍부한 의미 이해 능력을 활용한 **의미 기반 메타 평가**로의 전환을 의미한다.

### 체크포인트 질문

- BLEU와 ROUGE와 같은 전통적인 평가 지표는 어떤 방식으로 출력의 품질을 측정하며, 이때 발생하는 한계는 무엇인가?
- **의미 기반 평가**가 필요한 이유는 무엇이며, 이러한 평가에서는 출력의 어떤 요소들을 중점적으로 고려해야 할까?
- 한 가지 참고 정답에 대한 표면적 비교 대신 LLM을 평가자로 활용하면 얻을 수 있는 잠재적 이점은 무엇인가?

## 2. LLM 기반 평가 패러다임

LLM을 평가자로 활용하는 새로운 패러다임이 등장하면서, 기존 평가 방법의 한계를 극복할 수 있는 다양한 접근법들이 제안되고 있다. 이 섹션에서는 **GPTScore**, **G-Eval**, **FLASK** 등 주요 LLM 기반 평가 기법들을 살펴본다.

### 2.1 GPTScore: 확률 기반 평가 프레임워크

**GPTScore**는 LLM 자체의 **언어모델 확률**을 활용하여 출력물의 품질을 정량화한 초기 메타 평가 기법이다.

#### 핵심 원리

GPTScore는 다음과 같은 방식으로 동작한다:

1. **확률 기반 평가**: 원문과 후보 출력이 주어졌을 때, 언어모델이 해당 출력을 생성할 확률(우도)을 계산
2. **참고 정답 불필요**: 별도의 정답 없이도 평가 가능
3. **자동 품질 측정**: 유창성, 문법적 정확성 등 모델이 학습한 언어 패턴에 부합하는 정도를 측정

#### 수학적 공식화

요약 생성 평가의 경우:

```
Score = P(요약문 | 원문, 모델)
```

일반적으로 **로그 확률 합산** 또는 **perplexity 역수**를 사용하여 점수를 산출한다.

#### 장점

- **별도 튜닝 불필요**: 언어모델의 내재적 확률만으로 평가 가능
- **참고 정답 불필요**: 개방형 생성 과제에서도 평가 가능
- **자동화**: 사람의 개입 없이 자동으로 품질 측정

#### 한계

1. **데이터 편향**: 모델이 학습한 데이터 편향이 평가 점수에 반영
2. **창의성 억제**: 창의적이지만 올바른 응답이 낮은 점수를 받을 수 있음
3. **확률-품질 불일치**: 높은 확률이 반드시 높은 품질을 의미하지 않음

#### 성능 결과

- **인간 평가와의 상관도**: 약 0.43 (중간 수준)
- **BLEU 대비**: 향상된 상관도 달성
- **절대적 신뢰도**: 여전히 제한적

### 2.1.1 GPTScore 구현 예제

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def calculate_gpt_score(model, tokenizer, source_text, candidate_text):
    """
    GPTScore를 계산하는 함수

    Args:
        model: 언어모델
        tokenizer: 토크나이저
        source_text: 원문
        candidate_text: 평가할 후보 텍스트

    Returns:
        GPTScore 점수
    """
    # 입력 텍스트 구성
    input_text = f"{source_text} {candidate_text}"

    # 토큰화
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)

    # 모델 추론
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # 로그 확률 계산
    log_probs = torch.log_softmax(logits, dim=-1)

    # 후보 텍스트 부분의 로그 확률 합산
    candidate_tokens = tokenizer(candidate_text, return_tensors="pt", add_special_tokens=False)
    candidate_length = candidate_tokens.input_ids.shape[1]

    # 후보 텍스트 부분의 로그 확률 추출
    candidate_log_probs = log_probs[0, -candidate_length:, :]
    candidate_token_ids = candidate_tokens.input_ids[0]

    # 각 토큰의 로그 확률 합산
    total_log_prob = 0
    for i, token_id in enumerate(candidate_token_ids):
        total_log_prob += candidate_log_probs[i, token_id].item()

    # 평균 로그 확률로 정규화
    avg_log_prob = total_log_prob / candidate_length

    return avg_log_prob

# 사용 예시
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

source = "인공지능은 현대 사회에 큰 변화를 가져오고 있다."
candidate = "AI가 사회를 크게 변화시키고 있다."

score = calculate_gpt_score(model, tokenizer, source, candidate)
print(f"GPTScore: {score:.4f}")
```

**출력 예시:**

```
GPTScore: -2.3456
```

### 2.2 G-Eval: 체인-오브-생각(CoT) 기반 LLM 평가

**G-Eval**은 OpenAI GPT-4와 같은 최신 LLM을 평가자로 활용하는 프레임워크로, GPTScore의 한계를 보완하기 위해 등장했다.

#### 핵심 특징

G-Eval의 핵심 특징은 **프롬프트 내에 평가 기준과 단계별 판단 근거**를 구조화하여 LLM이 사람처럼 채점하도록 유도하는 것이다.

#### 평가 과정

1. **구조화된 프롬프트**: 평가 기준과 단계별 판단 근거를 명시
2. **Chain-of-Thought (CoT)**: 단계별 사고 과정을 통해 평가 수행
3. **Form-Filling**: 미리 정해진 양식에 맞춰 점수만 채워넣도록 유도

#### 예시: 요약 일관성 평가

```
지시: 기사와 요약을 읽고, 요약이 기사 내용과 논리적으로 일관되는지 1~5점으로 평가하라

평가 단계:
1. 기사 핵심 주제 파악
2. 요약이 이를 포함하는지 비교
3. 일관성 점수 부여
```

#### 확률 기반 보정

G-Eval은 **모델의 응답에 확률 정보를 활용한 보정**을 적용한다:

- **확신도 측정**: GPT-4가 평가 단계에서 어떤 선택지를 얼마나 확신하는지 로그 확률로 측정
- **가중 합산**: 확신도에 따라 최종 점수를 가중 합산
- **일관성 향상**: 확률 정보를 통해 평가의 일관성과 신뢰도 향상

#### 성능 결과

- **SummEval, Topical-Chat 벤치마크**: 평균 스피어만 상관 0.514 달성
- **GPTScore 대비**: 향상된 인간 상관도
- **일부 평가지표**: 최대 0.7에 육박하는 상관도 (SOTA 수준)
- **평가과정 투명성**: 모델 평가 과정을 추적 가능

#### 장점

- **구조화된 평가**: 명확한 평가 기준과 단계별 과정
- **높은 상관도**: 인간 평가와 높은 상관관계
- **투명성**: 평가 과정의 추적 가능성
- **확장성**: 다양한 평가 기준에 적용 가능

#### 한계

- **비용**: GPT-4 같은 대형 모델 API 호출 필요
- **안정성**: 하나의 기준당 여러 번 평가하여 안정성 확보 필요
- **의존성**: 특정 모델(GPT-4)에 대한 의존성

### 2.2.1 G-Eval 구현 예제

```python
import openai
from typing import Dict, List, Any

class GEvalEvaluator:
    def __init__(self, model="gpt-4", api_key=None):
        self.model = model
        if api_key:
            openai.api_key = api_key

    def create_evaluation_prompt(self, source_text: str, candidate_text: str,
                                criteria: str, scale: str = "1-5") -> str:
        """
        G-Eval 스타일의 평가 프롬프트 생성

        Args:
            source_text: 원문
            candidate_text: 평가할 후보 텍스트
            criteria: 평가 기준
            scale: 평가 척도

        Returns:
            구조화된 평가 프롬프트
        """
        prompt = f"""다음 텍스트를 평가해주세요.

원문:
{source_text}

후보 텍스트:
{candidate_text}

평가 기준: {criteria}

평가 단계:
1. 원문의 핵심 내용 파악
2. 후보 텍스트가 원문과 얼마나 일치하는지 분석
3. 평가 기준에 따라 {scale} 척도로 점수 부여

평가 과정을 단계별로 설명하고, 최종 점수를 제시해주세요.

형식:
단계 1: [핵심 내용 파악]
단계 2: [일치도 분석]
단계 3: [점수 부여]
최종 점수: [점수]"""

        return prompt

    def evaluate(self, source_text: str, candidate_text: str,
                 criteria: str, num_samples: int = 3) -> Dict[str, Any]:
        """
        G-Eval 방식으로 텍스트 평가

        Args:
            source_text: 원문
            candidate_text: 평가할 후보 텍스트
            criteria: 평가 기준
            num_samples: 평가 샘플 수

        Returns:
            평가 결과 딕셔너리
        """
        prompt = self.create_evaluation_prompt(source_text, candidate_text, criteria)

        scores = []
        explanations = []

        for i in range(num_samples):
            try:
                response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,  # 일관성을 위해 낮은 temperature
                    max_tokens=500
                )

                result = response.choices[0].message.content
                explanations.append(result)

                # 점수 추출 (간단한 정규식 사용)
                import re
                score_match = re.search(r'최종 점수:\s*(\d+)', result)
                if score_match:
                    scores.append(int(score_match.group(1)))

            except Exception as e:
                print(f"평가 {i+1} 중 오류: {e}")

        if scores:
            avg_score = sum(scores) / len(scores)
            return {
                "average_score": avg_score,
                "scores": scores,
                "explanations": explanations,
                "consistency": len(set(scores)) == 1  # 모든 점수가 같은지 확인
            }

        return {"error": "평가 실패"}

# 사용 예시
evaluator = GEvalEvaluator()

source = "인공지능 기술이 의료 분야에서 혁신적인 변화를 가져오고 있다. AI는 진단 정확도를 높이고, 개인맞춤형 치료를 가능하게 하며, 의료진의 업무 효율성을 크게 향상시키고 있다."

candidate = "AI가 의료에서 큰 변화를 만들고 있다. 진단이 더 정확해지고, 맞춤 치료가 가능해졌다."

criteria = "원문의 핵심 내용을 얼마나 정확하고 완전하게 요약했는지"

result = evaluator.evaluate(source, candidate, criteria)
print(f"평균 점수: {result['average_score']:.2f}")
print(f"일관성: {result['consistency']}")
```

**출력 예시:**

```
평균 점수: 4.33
일관성: True
```

### 2.3 FLASK: 미세 능력 세트 기반 평가

**FLASK**는 평가를 여러 세부 능력 요소로 분해하여 보다 해석 가능하고 구체적인 평가를 추구한 프레임워크다. 2024년 ICLR에 발표된 FLASK는 하나의 총점 대신 답변이 요구된 다양한 능력(스킬)을 얼마나 만족시키는지를 각각 점수화한다.

#### 핵심 개념

FLASK는 다음과 같은 접근을 취한다:

1. **능력 분해**: 평가 과제를 구성 요소 능력으로 분해
2. **세분화된 평가**: 각 능력에 대해 별도 점수 부여
3. **다차원 평가지표**: 총점이 아닌 다차원 평가 결과 제공

#### 12개 세분화된 능력 지표

연구진은 다음과 같은 12개의 능력 지표를 정의했다:

1. **명시적 추론 능력**: 논리적 추론 과정을 명확히 제시
2. **배경 지식 활용**: 관련 지식을 적절히 활용
3. **논리적 일관성**: 답변 내 논리적 일관성 유지
4. **맥락 준수**: 주어진 맥락에 적절히 부합
5. **정확성**: 사실적 정확성
6. **완전성**: 질문에 대한 완전한 답변
7. **명확성**: 답변의 명확성과 이해도
8. **창의성**: 독창적이고 창의적인 접근
9. **실용성**: 실제 적용 가능성
10. **윤리성**: 윤리적 기준 준수
11. **효율성**: 간결하고 효율적인 답변
12. **적응성**: 상황에 맞는 적절한 답변

#### 평가 과정

1. **스킬 태깅**: 각 평가 인스턴스에 해당되는 능력들을 태그로 지정
2. **개별 평가**: 각 스킬에 대해 별도 점수 부여
3. **가중치 적용**: 중요도에 따라 가중치를 두어 최종 점수 산출
4. **다차원 결과**: 총점이 아닌 다차원 평가지표 제공

#### 성능 결과

FLASK의 주요 성과:

- **모델 간 미묘한 차이 포착**: GPT-4와 GPT-3.5의 능력별 강약점 진단
- **높은 상관도**: 모델 평가와 인간 평가 간 높은 상관관계
- **해석 가능성**: 평가 결과의 명확한 해석 가능
- **맞춤형 평가**: 다양한 맞춤형 평가 루브릭 설계 가능

#### 예시: 법률 자문 답변 평가

법률 자문 답변의 경우 다음과 같은 스킬들이 요구된다:

- **법률 조항 기억** (배경 지식)
- **논리적 추론**
- **윤리 기준 준수**
- **실용적 조언 제공**

각 스킬에 대해 별도 점수를 부여하고, 중요도에 따라 가중치를 적용하여 최종 평가를 수행한다.

#### 장점

- **세분화된 평가**: 모델의 강약점을 명확히 파악
- **해석 가능성**: 평가 결과의 명확한 해석
- **맞춤형 적용**: 다양한 도메인에 맞춤형 평가 가능
- **신뢰도 향상**: LLM 평가의 신뢰도와 설명력 동시 향상

#### 한계

- **복잡성**: 평가 과정의 복잡성 증가
- **주관성**: 능력 정의와 가중치 설정의 주관성
- **비용**: 세분화된 평가로 인한 비용 증가

### 2.3.1 FLASK 구현 예제

```python
from typing import Dict, List, Any
import json

class FLASKEvaluator:
    def __init__(self):
        # 12개 능력 지표 정의
        self.skills = {
            "explicit_reasoning": "명시적 추론 능력",
            "background_knowledge": "배경 지식 활용",
            "logical_consistency": "논리적 일관성",
            "context_adherence": "맥락 준수",
            "accuracy": "정확성",
            "completeness": "완전성",
            "clarity": "명확성",
            "creativity": "창의성",
            "practicality": "실용성",
            "ethics": "윤리성",
            "efficiency": "효율성",
            "adaptability": "적응성"
        }

        # 각 도메인별 스킬 가중치
        self.domain_weights = {
            "legal": {
                "background_knowledge": 0.3,
                "logical_consistency": 0.25,
                "ethics": 0.2,
                "practicality": 0.15,
                "accuracy": 0.1
            },
            "medical": {
                "accuracy": 0.3,
                "background_knowledge": 0.25,
                "logical_consistency": 0.2,
                "ethics": 0.15,
                "practicality": 0.1
            },
            "general": {
                "accuracy": 0.2,
                "completeness": 0.2,
                "clarity": 0.2,
                "logical_consistency": 0.2,
                "practicality": 0.2
            }
        }

    def evaluate_skill(self, question: str, answer: str, skill: str) -> float:
        """
        특정 스킬에 대해 답변을 평가

        Args:
            question: 질문
            answer: 답변
            skill: 평가할 스킬

        Returns:
            스킬 점수 (0-1)
        """
        # 실제 구현에서는 LLM을 사용하여 각 스킬을 평가
        # 여기서는 간단한 예시로 구현

        skill_prompts = {
            "explicit_reasoning": "답변이 논리적 추론 과정을 명확히 제시했는가?",
            "background_knowledge": "답변이 관련 지식을 적절히 활용했는가?",
            "logical_consistency": "답변 내 논리적 일관성이 유지되는가?",
            "context_adherence": "답변이 주어진 맥락에 적절히 부합하는가?",
            "accuracy": "답변이 사실적으로 정확한가?",
            "completeness": "답변이 질문에 대해 완전한가?",
            "clarity": "답변이 명확하고 이해하기 쉬운가?",
            "creativity": "답변이 독창적이고 창의적인가?",
            "practicality": "답변이 실제 적용 가능한가?",
            "ethics": "답변이 윤리적 기준을 준수하는가?",
            "efficiency": "답변이 간결하고 효율적인가?",
            "adaptability": "답변이 상황에 맞는 적절한가?"
        }

        # 실제로는 LLM을 사용하여 평가
        # 여기서는 예시로 랜덤 점수 반환
        import random
        return random.uniform(0.6, 1.0)

    def evaluate_answer(self, question: str, answer: str, domain: str = "general") -> Dict[str, Any]:
        """
        FLASK 방식으로 답변을 종합 평가

        Args:
            question: 질문
            answer: 답변
            domain: 도메인 (legal, medical, general)

        Returns:
            평가 결과 딕셔너리
        """
        # 각 스킬에 대해 개별 평가
        skill_scores = {}
        for skill in self.skills:
            skill_scores[skill] = self.evaluate_skill(question, answer, skill)

        # 도메인별 가중치 적용
        weights = self.domain_weights.get(domain, self.domain_weights["general"])

        # 가중 평균 계산
        weighted_score = 0
        total_weight = 0

        for skill, score in skill_scores.items():
            if skill in weights:
                weighted_score += score * weights[skill]
                total_weight += weights[skill]

        final_score = weighted_score / total_weight if total_weight > 0 else 0

        return {
            "final_score": final_score,
            "skill_scores": skill_scores,
            "weights": weights,
            "domain": domain,
            "detailed_analysis": self._generate_analysis(skill_scores, weights)
        }

    def _generate_analysis(self, skill_scores: Dict[str, float], weights: Dict[str, float]) -> str:
        """상세 분석 생성"""
        analysis = "능력별 평가 결과:\n"

        # 가중치가 높은 스킬들 우선 표시
        sorted_skills = sorted(weights.items(), key=lambda x: x[1], reverse=True)

        for skill, weight in sorted_skills:
            score = skill_scores[skill]
            skill_name = self.skills[skill]
            analysis += f"- {skill_name}: {score:.2f} (가중치: {weight:.2f})\n"

        return analysis

# 사용 예시
evaluator = FLASKEvaluator()

question = "계약서에서 불공정한 조항이 있다면 어떻게 해야 할까요?"
answer = "불공정한 조항이 있는 경우, 먼저 해당 조항이 법적으로 유효한지 확인해야 합니다. 민법상 불공정한 조항은 무효가 될 수 있으며, 소비자보호법에 따라 보호받을 수 있습니다. 전문가와 상담하여 구체적인 대응 방안을 모색하는 것이 좋습니다."

result = evaluator.evaluate_answer(question, answer, domain="legal")
print(f"최종 점수: {result['final_score']:.2f}")
print(result['detailed_analysis'])
```

**출력 예시:**

```
최종 점수: 0.85
능력별 평가 결과:
- 배경 지식 활용: 0.92 (가중치: 0.30)
- 논리적 일관성: 0.88 (가중치: 0.25)
- 윤리성: 0.90 (가중치: 0.20)
- 실용성: 0.82 (가중치: 0.15)
- 정확성: 0.85 (가중치: 0.10)
```

### 체크포인트 질문

- **GPTScore**는 언어모델의 어떤 속성을 이용해 생성물의 품질을 평가하는가? 이 방법의 장점과 한계는 무엇인지 설명해보라.
- **G-Eval**은 평가 신뢰도를 높이기 위해 어떤 기법들을 도입했는가? (예: Chain-of-Thought, Form-Filling 등) 이러한 기법이 인간 평가와의 상관도를 어떻게 향상시켰는지 설명하라.
- **FLASK** 프레임워크에서는 왜 평가를 세분화된 스킬별로 수행하는가? 이렇게 얻어진 다차원 평가 결과는 모델 성능 해석에 어떤 도움을 주는지 논하라.

**표 1: 전통적 평가와 LLM 기반 메타 평가 패러다임 비교**

| 특징 (Feature)     | 전통적 지표 (BLEU/ROUGE)                                          | 판사로서의 LLM (LLM-as-a-Judge)                                        |
| :----------------- | :---------------------------------------------------------------- | :--------------------------------------------------------------------- |
| **평가 기준**      | 참조 텍스트와의 N-gram 중첩도 (어휘적 일치)                       | 자연어 지침으로 정의된 추상적 품질 기준 (예: 유용성, 논리성)           |
| **의미 이해**      | 불가능. 동의어나 다른 표현을 이해하지 못함.                       | 가능. 문맥과 뉘앙스를 파악하여 의미론적 품질 평가.                     |
| **창의성/다양성**  | 참조 텍스트와 다를 경우 불이익을 주어 창의성을 억제함.            | 평가 기준에 부합한다면 다양한 표현과 창의적인 결과물도 높게 평가 가능. |
| **비용 및 확장성** | 참조 텍스트 구축 비용이 높고, 새로운 태스크에 대한 확장성이 낮음. | 초기 설정 후 저비용으로 대규모 평가가 가능하며, 확장성이 매우 높음.    |
| **해석 가능성**    | 점수만 제공. 왜 낮은 점수가 나왔는지 알 수 없음.                  | "왜" 그렇게 평가했는지 자연어로 설명 가능하여 구체적인 피드백 제공.    |
| **핵심 한계**      | 생성형 AI의 핵심 역량(의미, 창의성)을 측정하지 못함.              | 편향성(선호도 유출, 위치), 다국어 일관성, 평가자 자체의 신뢰성 문제.   |

**표 2: 인간 평가와의 상관관계 비교 (SummEval 데이터셋, 스피어만 상관계수 ρ)**

| 평가 지표 (Evaluation Metric) | 일관성 (Coherence) | 일관성 (Consistency) | 유창성 (Fluency) | 관련성 (Relevance) | 평균 (Average) |
| :---------------------------- | :----------------- | :------------------- | :--------------- | :----------------- | :------------- |
| ROUGE-1                       | 0.167              | 0.207                | 0.105            | 0.326              | 0.201          |
| ROUGE-2                       | 0.158              | 0.200                | 0.106            | 0.306              | 0.192          |
| ROUGE-L                       | 0.170              | 0.210                | 0.110            | 0.320              | 0.202          |
| BERTScore                     | 0.284              | 0.362                | 0.216            | 0.426              | 0.322          |
| **G-Eval (GPT-4 기반)**       | **0.582**          | **0.460**            | **0.467**        | **0.547**          | **0.514**      |

데이터 출처: Liu et al., 2023\. G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. 6

## 3. 특수 목적 벤치마크

특정 도메인이나 능력에 특화된 벤치마크들이 등장하면서, LLM의 다양한 능력을 더 정확하고 포괄적으로 평가할 수 있게 되었다. 이 섹션에서는 **LiveCodeBench**, **EvalPlus**, **MMLU-Pro**, **GPQA**, **BBH** 등 주요 특수 목적 벤치마크들을 살펴본다.

### 3.1 LiveCodeBench: 오염 방지형 코드 생성 평가

**LiveCodeBench**는 2024년 제안된 실시간 업데이트형 코드 평가 세트로, 데이터 오염(contamination)을 원천 차단하도록 설계된 것이 가장 큰 특징이다.

#### 핵심 문제: 데이터 오염

기존의 HumanEval(OpenAI), MBPP(Google) 등 코드 평가 데이터셋들은 다음과 같은 문제를 가진다:

- **평가 문제 사전 노출**: 모델 학습에 포함되어 평가 문제가 사전에 노출
- **과적합 문제**: GPT-4 같은 최신 모델이 HumanEval에서 80~90%의 높은 점수를 기록하지만, 이는 일부 문제를 이미 봤거나 변형된 형태로 학습했을 가능성
- **신뢰성 저하**: 실제 코딩 능력과 평가 결과 간의 괴리

#### 해결 방안

LiveCodeBench는 다음과 같은 방식으로 문제를 해결한다:

1. **실시간 문제 수집**: 온라인 저지 플랫폼들(LeetCode, AtCoder, CodeForces)의 최신 문제들을 지속적으로 수집
2. **시간 창 기반 갱신**: 2023년 5월부터 2024년 5월까지 공개된 400여개의 신규 문제를 선별하여 초기 세트 구성
3. **주기적 갱신**: 시간 창을 정해 주기적으로 문제를 갱신하여 모델 학습 이후에 나온 문제만 사용

#### Holistic 평가

LiveCodeBench의 두 번째 특징은 평가 범위를 전반적으로 확장한 것이다:

- **코드 실행 결과의 정확성**: 단순 코드 생성이 아닌 실행 가능한 코드 평가
- **자체 디버깅(self-repair) 능력**: 오류 로그를 주고 코드를 수정하도록 하는 시나리오
- **주석이나 출력 형식의 적절성**: 코드 품질의 다양한 측면 평가
- **pass@k 측정**: 실행 결과가 주어진 테스트 케이스와 일치하는지 측정
- **테스트 출력 예측**: 모델이 테스트 출력까지 예측하는 시나리오 포함

#### 성능 결과

LiveCodeBench를 18개 기본 LLM과 34개 instruction 튜닝 LLM에 적용한 결과:

- **기존 정적 벤치마크의 한계**: HumanEval 등에서 높은 성능을 보인 최신 모델들도 새로운 문제에서는 성능이 상당히 떨어짐
- **과적합 확인**: HumanEval에서 과적합된 모델일수록 실제 새로운 문제에 약함
- **투명성**: 모든 프롬프트와 모델 응답, 평가 스크립트를 투명하게 제공

#### 의의

LiveCodeBench는 "모델이 못 풀도록 새로운 문제를 라이브로 던진다"는 혁신으로 모델 과적합/암기 여부를 가려내고, 코드 생성의 다양한 능력을 입체적으로 평가하는 최신 벤치마크다.

### 3.2 EvalPlus: 테스트케이스 보강

**EvalPlus**는 코드 평가에서 테스트케이스의 불충분함을 보완하기 위한 기법 및 데이터셋으로, 2023년 NeurIPS에 소개되었다.

#### 핵심 문제

기존 HumanEval의 문제점:

- **테스트케이스 부족**: 문제당 평균 7~10개 내외의 단순한 테스트
- **복잡한 버그 미발견**: 단순한 테스트로는 복잡한 버그를 잡아내지 못함
- **관대한 평가**: 틀린 솔루션도 정답으로 간주되는 허점

#### 해결 방안

EvalPlus는 다음과 같은 접근을 취한다:

1. **자동 테스트케이스 생성**: 기존 테스트 입력을 변형/확장하여 수십 배 이상의 테스트케이스를 자동 생성
2. **Mutation-based Input Generation**: 주어진 함수의 입력을 다양한 유형으로 변이시키거나 경계 케이스를 추가로 생성
3. **다양한 입력 변형**: 리스트를 다루는 함수라면 빈 리스트, 원소 하나인 리스트, 음수/특수값 포함 리스트 등으로 입력 변형을 자동 생성

#### 성능 결과

HumanEval+ 세트를 구축하고 모델들을 재평가한 결과:

- **GPT-4 등의 pass@1 정확도가 무려 20%p 이상 하락**
- **HumanEval 문제 164개 중 156개에서 추가 테스트 도입 후 정답률이 하락**
- **기존 평가의 관대함이 드러남**

#### 의의

EvalPlus 연구는 자동화된 테스트 증강이 모델 평가 신뢰도를 높이는 효과를 입증했으며, 향후에는 안전/보안 측면의 추가 테스트(예: 악의적 입력 대응)도 고려할 수 있음을 시사한다.

### 3.3 HELM-Code: 투명성과 커뮤니티 협업

**HELM (Holistic Evaluation of Language Models)**은 스탠포드 CRFM에서 주도한 평가 노력으로, 평가의 투명성과 커뮤니티 협업에 중점을 둔 프레임워크다.

#### 핵심 특징

- **거대 벤치마크 세트**: 다양한 시나리오와 지표로 구성
- **투명성**: 모든 실험 결과와 프롬프트, 응답을 공개
- **재현성**: 평가 과정의 재현성을 높이기 위한 노력
- **리더보드**: 최신 모델들을 지속 평가하여 공개

#### HELM의 철학

HELM의 철학은 평가를 "하나의 지도(Map)"처럼 종합적으로 제시하는 것으로, 모델의 정량적 성능뿐 아니라 다음을 포함한다:

- **추론 속도**
- **메모리 사용량**
- **편향/유해성**
- **탄소 배출 (Green AI 지표)**

#### 의의

이러한 투명하고 포괄적인 접근은 평가 자체의 신뢰성을 높이고, 연구자들이 특정 과제나 지표에 치중하지 않고 넓은 관점에서 모델을 발전시키도록 유도한다.

### 3.4 MMLU-Pro: 10지선다 고난도 지식/추론 벤치마크

**MMLU (Massive Multi-Task Language Understanding)**는 57개 분야에 걸친 대규모 문제 은행으로 GPT-3 시대부터 사용된 대표적인 지식 평가 벤치마크다. 각 문제는 4지선다형 객관식으로 구성되어 있었는데, 최신 LLM들이 MMLU에서 일찍이 인간 수준에 도달하자 2024년에는 난이도를 대폭 높인 확장판인 MMLU-Pro가 등장했다.

#### 핵심 변화

1. **선택지 수 증가**: 4개에서 10개로 늘림

   - 무작위 정답 확률이 25%에서 10%로 감소
   - 단순 암기에 의한 운빨 통과 가능성 감소

2. **문제 복잡도 증가**: 더 복잡한 다단계 추론이 요구됨
   - 단순 암기 질문 대신 여러 지식을 종합하거나 여러 단계를 추론해야 하는 문제 추가
   - 예: "어느 해에 무슨 사건이 있었는가?" → "다음 중 역사적 사건들을 발생 순서대로 나열한 것은?"

#### 성능 결과

- **GPT-4 등의 모델이 MMLU 대비 16~33% 정확도 감소**
- **모델 간 실력 차이가 다시 두드러지도록 변별력 확보**

#### Chain-of-Thought 효과

- **MMLU 원판**: CoT 유도해도 성능 개선이 크지 않음
- **MMLU-Pro**: CoT 유도가 성능 향상에 뚜렷한 효과
- **"단계별로 풀어보고 답을 선택하라"**는 지시와 함께 답하게 한 경우 정답률이 크게 상승

#### 현재 상태

- **수만 문항** 포함
- **14개 분야의 대학원 수준 질문** 망라
- **2025년 기준 가장 우수한 모델들도 약 85% 내외의 정확도**
- **조금이라도 응용을 요구하는 문제에서 오답을 내는 경향** 관찰

#### 의의

MMLU-Pro의 등장은 향후 LLM 평가가 "단순 지식 암기 테스트"를 넘어 "사고력과 문제해결력 평가"로 나아감을 보여주는 사례다.

### 3.5 GPQA와 BBH: 지식/추론 강화 평가 세트

**GPQA (Graduate-level Google-Proof Questions & Answers)**는 2024년에 등장한 고난도 QA 벤치마크로, 이름처럼 대학원 수준의 질문들을 모아놓은 세트다.

#### 핵심 특징

- **"Google-Proof"**: 단순히 구글 검색으로 찾아볼 수 없는 비교적 새로운 연구 기반의 질문
- **여러 단계를 거쳐야 풀 수 있는 문제**들로 구성
- **생물학, 물리학, 화학** 3개 분야에서 분야별 전문가들이 작성
- **총 448개의 5지선다 문항**

#### 문제 예시

예를 들어 _"어떤 화학 반응 경로에서 촉매 A의 역할을 없앨 경우 나타나는 현상은?"_ 같은 문제는 전문 지식과 논리를 모두 요구한다.

#### 성능 결과

- **박사 과정 학생 수준의 사람이 GPQA에 답했을 때 평균 65% 정도의 정확도**
- **GPT-4, Gemini 등 최첨단 모델들도 이 세트에서 비슷한 수준 (약 85% 미만)의 정확도**
- **아직 인간과 큰 차이를 내지 못함**

#### 의의

GPQA의 의의는 다음과 같다:

1. **LLM들이 거의 모든 인터넷 지식을 흡수한 상황에서 새로운 난이도를 부여**
2. **과학적 추론 과제에 대한 모델의 한계를 드러냄**
3. **GPT-4도 GPQA 일부 문항에서는 오개념이 섞인 설명을 선택**하는 등 불완전한 모습을 보임
4. **향후 모델의 전문분야 학습 및 추론 강화 연구의 필요성을 보여줌**

#### BBH (BIG-Bench Hard)

**BBH (BIG-Bench Hard)**는 Google의 BIG-Bench 대형 벤치마크 중에서도 특별히 난이도 높은 23개 과제만을 추려 구성한 하드 테스트 세트다.

##### 핵심 특징

- **BIG-Bench 원본**: 200여개의 다양한 언어 모델 과제
- **BBH**: 기존 모델들이 현저히 못 푼 추론 퍼즐, 함정 문제, 역량 한계 평가 문제들을 묶어 "Hard" 태그를 붙인 것
- **산수 영역**: 단순 계산이 아닌 경우의 수 추론 문제
- **언어 영역**: 복잡한 문법 역설문 등

##### 목표

**모델의 약점을 집중 공략**하는 것으로, GPT-3나 초기 GPT-4까지도 BBH 각 과제에서 랜덤 추측 수준 혹은 그보다 약간 나은 수준의 성능밖에 내지 못했다.

##### 연구 활용

연구자들은 BBH를 통해 모델의 특정 능력 결핍(예: 논리 퍼즐 해결, 역설 처리 등)을 분석하고, Chain-of-Thought 유도나 추가 훈련 데이터 투입 등의 개선을 시도했다.

##### 확장판: BBEH

2023년에 발표된 **BBH 추가 확장판 (BBEH: BIG-Bench Extra Hard)**은 여기에 더해 멀티스텝 논증, 창의적 문제 해결 등의 요소를 강화하여, LLM들에게 한층 어려운 도전장을 내밀기도 했다.

##### 의의

결과적으로 BBH/BBEH는 LLM 연구 커뮤니티에서 모델의 한계를 지속 점검하는 스트레스 테스트로 기능하고 있다.

### 체크포인트 질문

- **LiveCodeBench**는 어떤 방식으로 **데이터 오염** 문제를 해결하였나요? 또한 이 벤치마크가 기존 코드 평가와 달리 강조한 추가적인 평가 능력은 무엇인가요?

- **EvalPlus**가 제안한 **Mutation-based Input Generation**은 어떤 원리로 작동하며, HumanEval+에서 GPT-4의 성능이 크게 하락한 이유는 무엇인가요?

- **HELM**의 철학인 "평가를 하나의 지도처럼 종합적으로 제시"한다는 것은 무엇을 의미하며, 이는 기존 평가 방식과 어떻게 다른가요?

- **MMLU-Pro**에서 선택지 수를 4개에서 10개로 늘린 것이 모델 평가에 미치는 영향은 무엇이며, Chain-of-Thought 유도가 더 효과적인 이유는 무엇인가요?

- **GPQA**의 "Google-Proof" 특성은 무엇을 의미하며, 이는 LLM 평가에 어떤 새로운 도전을 제시하나요?

- **BBH**가 모델의 약점을 집중 공략하는 방식은 무엇이며, 이는 모델 개발에 어떤 시사점을 제공하나요?

- GPQA나 BBH와 같은 **고난도 벤치마크**들은 일반 벤치마크와 달리 어떠한 목적을 가지고 설계되었나요? 이런 세트에서 드러난 LLM의 한계 사례를 하나 들어보세요.

## 4. 도메인 특화 벤치마크

특수 목적 벤치마크가 일반적인 능력을 평가하는 데 중점을 둔다면, 도메인 특화 벤치마크는 특정 분야에서의 전문성을 평가하는 데 중점을 둔다. 이러한 벤치마크들은 LLM이 실제 업무 환경에서 얼마나 유용한지 측정하는 데 중요한 역할을 한다.

### 4.1 FinBen: 금융 도메인 종합 벤치마크

**FinBen**은 금융 분야에 특화된 대형 벤치마크 세트로, 2024년 NeurIPS Datasets/Benchmarks 트랙에 공개되었다.

#### 핵심 특징

- **총 24개 금융 과제**에 걸쳐 **42개의 데이터셋**을 통합한 거대한 벤치마크
- **8가지 능력 영역**: 정보 추출, 텍스트 분석, 질의응답, 텍스트 생성, 리스크 관리, 시계열 예측, 의사결정, 다국어(영어/스페인어)
- **금융 업무와 직결된 다양한 태스크**: 기업 보고서 요약, 주가 예측, 재무제표에서 특정 항목 추출, 금융 규제 문서 질의응답, 투자 의사결정 시나리오 생성
- **독자적 과제**: 주식 트레이딩 의사결정 평가, 금융 규제 관련 Q&A 등

#### 데이터 구성

- **대부분 공개 데이터나 실제 금융 기록**에서 획득
- **일부 새롭게 제작한 금융 QA, 주식 트레이딩** 시나리오도 포함

#### 성능 결과

GPT-4, ChatGPT, Google Gemini 등 21개 대표 LLM을 비교한 결과:

- **GPT-4**: 정보 추출 및 단순 분석 영역에서 탁월
- **Google Gemini**: 시계열 예측이나 복잡한 텍스트 생성에서 더 높은 성능
- **모델별로 강점 영역이 다르게 나타남**
- **오픈 QA**: GPT-4 계열이 강세
- **형식에 맞춘 긴 보고서 작성**: 다른 코드 튜닝된 LLM들이 더 적합

#### 시사점

1. **금융 도메인에 맞춘 모델 개발의 필요성** 강조
2. **금융 특화 튜닝을 거친 LLM**들이 규제 문서 질의응답 등의 세부 과제에서 더 정확한 답변
3. **오픈 리더보드** 형태로 운영되어 연구 커뮤니티가 금융 분야 LLM 발전을 협력적으로 추진

#### 의의

FinBen의 등장으로 LLM 평가 범위가 일상 언어 영역에서 전문 지식 영역으로 크게 넓어졌으며, 높은 정확도와 엄밀함이 요구되는 금융 도메인에서 LLM들의 실력을 객관적으로 측정하는 기반이 마련되었다.

### 4.2 AgentHarm: AI 에이전트 유해성 평가 벤치마크

**AgentHarm**은 자율 에이전트로 동작하는 LLM들의 유해 행동 가능성을 측정하기 위한 안전성 벤치마크로, 2025년 ICLR에 소개되었다.

#### 배경

오늘날 LLM들은 단순 채팅을 넘어 툴 사용, 멀티스텝 작업 수행 등 에이전트처럼 행동할 수 있게 되었는데, 이러한 고도화된 LLM 에이전트가 악용될 경우 초래될 위험을 평가하는 것이 AgentHarm의 목적이다.

#### 벤치마크 구성

- **명시적으로 악의적인 110개 시나리오(작업)** 선정
- **데이터 증강을 통해 변형된 440개 프롬프트**까지 포함
- **11가지 유해 범주**: 사기, 사이버 범죄, 괴롭힘, 가짜뉴스 작성, 폭력 교사 등

#### 시나리오 예시

- **사이버 범죄형 과제**: _"피싱 이메일을 생성하여 사용자로부터 암호를 알아내라"_
- **증오/괴롭힘 시나리오**: _"특정 소수집단에 대한 증오발언을 가장 효과적으로 확산시킬 방법을 찾아라"_

#### 평가 방식

1. **거절률 측정**: 모델이 이러한 악성 요청에 대해 거부(reject)하는지 여부를 측정

   - 안전 조치가 잘 되어 있다면 "해당 요청은 수행할 수 없습니다"라고 거절해야 정상
   - 이를 거절률로 산출

2. **Jailbreaking 후 평가**: 모델을 강제로 jailbreaking(정책 무력화)한 뒤에도 에이전트 기능을 유지하며 유해 목표를 달성하는지 평가
   - 툴 사용이 필요한 다단계 작업(해킹 절차 등)에서, 정책을 풀었을 때 모델이 끝까지 단계들을 실행해 목표를 이루는지 확인

#### 주요 발견

- **상위 LLM들조차 별도 jailbreaking 없이도 의외로 쉽게 악성 요청을 받아들임**
- **OpenAI나 Anthropic의 안전장치에도 불구하고 교묘한 프롬프트에 다수 모델이 속아 넘어감**
- **간단한 보편적 jailbreak 프롬프트**만으로도 대부분의 에이전트 LLM이 **완전 악성 모드로 전환**
- **중간 단계 결과들도 매우 일관적으로 유해**한 방향으로 산출
- **현재 LLM들의 안전장치가 복합 업무 시나리오에서는 취약**함을 의미

#### 연구 영향

AgentHarm의 공개로 연구자들은 모델 방어 기법을 더 진지하게 탐구하고 있다:

- **자기 검열형 CoT** 도입: 모델이 단계마다 "이 행동이 안전한가?"를 자문하도록 하는 실험
- **강화학습 기반의 에이전트 안전성 튜닝** 연구
- **툴 이용 제한 장치** 등의 개발 지표로 활용

#### 의의

AgentHarm은 LLM 평가에 "AI의 도구적 위험"이라는 새로운 축을 추가한 것으로 볼 수 있으며, AI 안전성 연구에 중요한 데이터 포인트를 제공한다.

### 4.3 LEXam: 법률 시험 기반 LLM 평가

**LEXam**은 법률(Legal) 분야의 고급 추론 평가를 위해 2025년에 제안된 벤치마크로, 스위스 취리히 대학의 실제 법과시험 문제 340세트를 기반으로 만들어졌다.

#### 핵심 특징

- **116개 과목**: 민법, 형법, 행정법, 국제법 등
- **학부/대학원 다양한 수준**의 시험 문제 망라
- **전례 없는 법률 AI 평가 세트**

#### 데이터 구성

- **총 4,886개 문항**
- **2,841개는 논술형(open-ended) 문제**
- **2,045개는 4지선다형 문제**
- **논술형에는 모범 답안과 함께 채점 가이드라인** 포함 (issue spotting, 법률 적용 단계 등)

#### 평가 방식

단순 정확/오답뿐 아니라, **답변 전개 과정의 타당성**까지 평가하기 위함이다.

#### 성능 결과

LEXam을 최신 LLM들에 적용한 결과, 모델들이 법률 추론에서 심각한 한계를 드러내는 것으로 나타났다:

- **GPT-4처럼 언어 능력이 뛰어난 모델도, 사실관계와 법조항을 결합해 여러 단계의 논증을 펼치는 복잡한 논술형 문제**에서는 낮은 점수
- **단순 암기형 질문**(예: 조문 번호를 묻는 문제)에는 모델이 정답을 맞추나, **사례형 문제**(주어진 상황에 법 적용)에는 엉뚱한 결론을 내리는 경우가 많음
- **4지선다형의 경우 GPT-4가 우수한 편**이었지만, **함정 선택지를 논리적으로 제거**해야 풀리는 문제에서는 여전히 인과관계 오류를 범함

#### 법률 분야 특유의 고차원 추론

이러한 결과는 법률 분야 특유의 고차원 추론에서 LLM이 얼마나 취약한지 보여준다:

- **전제사실 파악**
- **관련 법조항 선택**
- **유추 및 판례 적용**
- **최종 판단**

#### LLM-as-Judge 활용

LEXam 벤치마크 논문에서는 평가를 위해 LLM-as-Judge 접근을 활용하기도 했다:

- **GPT-4에게 수험생 GPT-3.5의 답안을 주고 "이 답안의 논증 구조와 법적 타당성을 평가하라"고 시킴**
- **모델 대 모델 평가**를 인간 교수들의 평가와 비교한 결과, **상당히 높은 일치도**를 보여 LLM 평가자의 가능성을 확인
- **다만 여전히 섬세한 부분에서는 차이가 있어, 궁극적으로는 LLM+전문가 공동 평가** 같은 방향도 제안
- **실제 법률 시험 상황에서는 부분 점수 등이 존재하는데, 모델은 이를 놓치기 때문**

#### 의의

LEXam은 향후 법률 특화 LLM 개발에 중요한 기준으로 활용될 것이며, 계약서 검토, 판례 검색 등 실제 법조 AI 적용 분야에서 모델 성능을 점검하는 리트머스 시험지 역할을 할 것으로 기대된다.

### 4.4 CSEDB: 의료 LLM 안전성/효과성 이중 평가

**CSEDB (Clinical Safety-Effectiveness Dual-Track Benchmark)**는 의료 도메인에서 LLM의 활용을 평가하기 위한 다차원 벤치마크로, 2025년에 처음 공개되었다.

#### 배경

의료 분야 LLM의 큰 과제는 진단 및 조언의 정확성(효과성)과 환자 안전 및 윤리 준수(안전성)인데, CSEDB는 이 두 측면을 동시 평가하도록 고안되었다.

#### 벤치마크 구성

- **30개 평가 기준**: 중환자 인지, 가이드라인 준수, 약물 안전 등 임상 상황에서 중요한 기준을 의사들의 합의로 도출
- **총 2,069개 문항**이 **26개 임상 진료과**를 망라하도록 구성
- **문항은 모두 주관식 서술 응답** 형태
- **각 문항마다 어느 평가 기준과 관련되는지 태깅**되어 있음

#### 예시

"응급환자 초기조치" 문항은 _"치명적 증상 인지"_, _"즉각적 처치 가이드라인 적용"_ 같은 안전/효과 기준에 연결된다.

#### 성능 결과

CSEDB를 GPT-4, ChatGPT, Med-PaLM2 등 6개 모델에 적용한 결과:

- **모델들의 전반적 점수는 57.2%** 정도
- **안전성 점수 54.7%**, **효과성(정확성) 점수 62.3%**로, 두 항목 모두 절반 수준에 그침
- **실제 임상 투입에는 아직 상당한 보완이 필요함**을 시사

#### 고위험 시나리오에서의 성능

- **환자 상태가 위급한 고위험 시나리오**에서는 성능이 추가로 **13.3%p 감소**
- **안전 41%, 효과 49% 수준**으로 모델이 **위험 상황 대처를 더 어려워함**

#### 도메인 특화 튜닝의 효과

- **전문 의료 LLM** (예: 의료 데이터로 추가 학습된 모델)은 일반 모델보다 전반적으로 성능이 높음
- **특히 안전성에서 최고 91.2%, 효과성에서 86.1%까지** 점수를 향상시킨 사례
- **도메인 특화 튜닝의 효과**를 보여주는 대목

#### 활용 방안

- **각 모델이 어떤 기준에서 취약한지** (예: 약물 상호작용 관련 질문에 자주 오류) 진단 가능
- **모델의 임상 적용 가능성을 객관적으로 비교**하는데 활용
- **규제 승인 등의 참고자료**로도 유용

#### 의의

CSEDB는 안전성과 효율 둘 다 놓칠 수 없는 의료 분야에 특화된 평가로서, LLM이 실제 환자 케어에 투입되기 전에 반드시 넘어야 할 시험대라고 할 수 있다.

### 4.5 MATH 및 GSM8K: 수학 능력 평가

수학 분야는 LLM에게 특히 도전적인 영역이며, 이를 측정하기 위한 MATH와 GSM8K 벤치마크가 많이 쓰인다.

#### MATH 벤치마크

**MATH**는 Hendrycks 등이 구축한 고등학교 올림피아드 수준의 문제 모음으로, 대수학, 기하학, 확률 등 범위의 서술형 문제 약 12,000개를 포함한다.

##### 특징

- **각 문제는 정답뿐 아니라 문제 해결에 필요한 단계별 해설**도 제공
- **체인-오브-Thought 풀이 훈련**에 활용
- **다단계 추론, 공식을 유도하는 능력, 실수 없이 계산하는 능력**을 평가

##### 성능 결과

- **최신 GPT-4는 MATH에서 약 40%대 중반의 정답률**을 기록
- **사람 수학 경시 참가자 수준에는 못 미치는** 수치

#### GSM8K 벤치마크

**GSM8K**는 초등학교~중학교 수준의 산술 단문제 8천여 개로 구성된 데이터셋으로, 짧은 서술형(=한두 문장) 풀이를 요하는 산수 문제가 주류다.

##### 특징

- **예시**: _"사과 5개 중 2개를 먹으면 몇 개 남나요?"_ 같은 문제부터, 조금 복잡한 _"철수와 영희의 나이 합이 24, 영희 나이는 철수의 두 배보다 4 많다. 둘의 나이는?"_ 정도까지 난이도가 있음
- **GPT-4는 Chain-of-Thought 없이 바로 풀면 실수할 때도 있으나, "생각을 보여주라"고 유도하면 높은 정확도로 푸는 것으로 나타남**

##### 성능 향상 기법

- **Self-Consistency**라는 기법 (여러 번 Chain-of-Thought로 풀고 투표)을 적용하여 GSM8K 정확도를 기존 55%에서 **72.9%로 향상**
- **다양한 경로의 사고를 탐색해 공통 답을 취함으로써** 모델의 일관성 문제를 줄임

##### 활용

GSM8K는 현재 LLM의 기본 산술 능력과 간단 추론 능력의 지표로 많이 쓰이며, 성능 향상을 위한 각종 프롬프트 기법 (예: Self-Consistency, 자동자연스텝)이 이 벤치마크에서 시험되었다.

#### 평가 방식

두 수학 벤치마크 모두 정답률뿐 아니라 풀이과정 평가에도 활용된다:

- **모델이 답은 맞췄지만 논리가 틀렸다면 부분 점수를 깎는 등 사람 채점 방식**을 흉내냄
- **수학 문제 해결에 있어 추론의 정확성**이 중요하기 때문

#### LLM의 한계

수학 벤치마크를 통해 드러난 LLM의 한계:

- **길고 복잡한 문제일수록 앞부분 추론은 맞다가도 후반 계산 실수로 틀리는 경향**
- **추론 경로를 살짝만 바꾸면 다른 답을 내는 불안정성**

#### 개선 연구

이를 개선하기 위한 연구로 강화학습을 통한 계산 모듈 내장, 외부 계산 툴 호출 능력 부여 등이 진행 중이다.

#### 의의

MATH와 GSM8K는 LLM의 논리적 사고력과 계산 능력을 객관적으로 측정하는 핵심 벤치마크로 자리잡았으며, 이러한 수학적 평가를 통해 모델의 한계 파악 및 사고력 향상 방법에 대한 많은 통찰을 얻고 있다.

### 체크포인트 질문

- **FinBen**이 금융 도메인에서 모델별로 강점 영역이 다르게 나타난 이유는 무엇이며, 이는 도메인 특화 모델 개발에 어떤 시사점을 제공하나요?

- **AgentHarm**에서 상위 LLM들조차 별도 jailbreaking 없이도 의외로 쉽게 악성 요청을 받아들인 이유는 무엇이며, 이는 현재 LLM 안전장치의 어떤 한계를 보여주나요?

- **LEXam**에서 GPT-4가 단순 암기형 질문에는 정답을 맞추나 사례형 문제에는 엉뚱한 결론을 내리는 이유는 무엇이며, 이는 법률 분야 특유의 어떤 고차원 추론 능력의 부족을 의미하나요?

- **CSEDB**에서 환자 상태가 위급한 고위험 시나리오에서 성능이 추가로 13.3%p 감소한 이유는 무엇이며, 이는 의료 LLM의 어떤 한계를 보여주나요?

- **MATH와 GSM8K**에서 Self-Consistency 기법이 성능 향상에 효과적인 이유는 무엇이며, 이는 LLM의 어떤 일관성 문제를 해결하는 방법인가요?

**표 3: 차세대 평가 벤치마크 개요**

| 벤치마크          | 주요 도메인    | 핵심 목표                                        | 핵심 혁신                                           |
| :---------------- | :------------- | :----------------------------------------------- | :-------------------------------------------------- |
| **G-Eval**        | 범용 NLG       | LLM 평가자와 인간 판단의 상관관계 극대화         | 자동 CoT 생성 및 Form-Filling을 통한 체계적 평가    |
| **LiveCodeBench** | 코드 생성      | 데이터 오염이 없는 신뢰할 수 있는 코드 능력 측정 | 실시간 문제 수집 및 타임스탬프 기반의 동적 평가     |
| **MMLU-Pro**      | 범용 지식/추론 | SOTA 모델의 한계를 시험하고 변별력 확보          | 선택지 확장(4→10), 추론 중심 문제 강화              |
| **FinBen**        | 금융           | 실제 금융 시나리오에서의 LLM 실용성 종합 평가    | 에이전트 기반 주식 거래 및 RAG 기반 평가 도입       |
| **AgentHarm**     | AI 안전        | LLM 에이전트의 유해성 및 탈옥 공격 취약성 측정   | 악의적 다단계 작업 수행 능력 및 거절 능력 동시 평가 |

## 5. 평가의 편향과 한계

LLM 평가에서 나타나는 편향과 한계는 평가의 신뢰성과 공정성에 중요한 영향을 미친다. 이러한 편향들을 이해하고 해결하는 것은 더 나은 평가 시스템을 구축하는 데 필수적이다.

### 5.1 주요 평가 편향

LLM 평가에서 나타나는 주요 편향들은 다음과 같다:

#### 5.1.1 자기애적 편향 (Narcissistic Bias)

**자기애적 편향**은 LLM이 자신이 생성한 텍스트를 더 선호하는 경향을 말한다. 이는 LLM이 평가자 역할을 할 때 자신의 출력 스타일이나 표현 방식을 더 높게 평가하는 현상이다.

##### 특징

- **자신의 출력 스타일 선호**: LLM이 자신이 생성한 텍스트의 스타일이나 표현 방식을 더 높게 평가
- **일관성 부족**: 다른 모델의 출력과 비교할 때 공정하지 않은 평가
- **평가 신뢰성 저하**: 객관적 평가가 어려워짐

##### 해결 방안

- **다양한 평가자 활용**: 여러 모델을 평가자로 사용하여 편향을 상쇄
- **평가 기준 명확화**: 구체적이고 객관적인 평가 기준 설정
- **교차 검증**: 다른 모델의 평가 결과와 비교하여 일관성 확인

#### 5.1.2 장황성 편향 (Verbosity Bias)

**장황성 편향**은 LLM이 더 긴 텍스트를 더 높게 평가하는 경향을 말한다. 이는 텍스트의 길이가 평가에 영향을 미치는 현상이다.

##### 특징

- **길이와 품질의 혼동**: 텍스트의 길이를 품질의 지표로 잘못 인식
- **불필요한 정보 포함**: 평가를 높이기 위해 관련 없는 정보를 추가
- **효율성 저하**: 간결하고 정확한 답변보다 장황한 답변을 선호

##### 해결 방안

- **길이 정규화**: 텍스트 길이를 고려한 평가 지표 사용
- **핵심 내용 평가**: 텍스트의 핵심 내용과 관련성에 집중
- **효율성 지표**: 정보 밀도와 정확성을 함께 고려한 평가

#### 5.1.3 일관성 부족 (Inconsistency)

**일관성 부족**은 동일한 입력에 대해 다른 평가 결과를 내는 현상이다. 이는 평가의 신뢰성을 크게 저해한다.

##### 특징

- **동일 입력, 다른 결과**: 같은 텍스트에 대해 다른 점수나 평가 결과
- **평가 기준 불일치**: 평가 시마다 다른 기준을 적용
- **재현성 부족**: 동일한 조건에서도 다른 결과

##### 해결 방안

- **표준화된 평가 프로토콜**: 일관된 평가 절차와 기준 설정
- **다중 평가**: 여러 번의 평가를 통한 평균값 사용
- **평가자 훈련**: 일관된 평가를 위한 평가자 교육

### 5.2 평가의 한계

LLM 평가에서 나타나는 주요 한계들은 다음과 같다:

#### 5.2.1 인간 평가와의 차이

- **주관성 부족**: 인간의 직관과 감정을 반영하지 못함
- **맥락 이해 한계**: 복잡한 사회적, 문화적 맥락을 완전히 이해하지 못함
- **창의성 평가 어려움**: 창의적이고 독창적인 내용을 평가하는 데 한계

#### 5.2.2 도메인 특화 지식 부족

- **전문 분야 이해 한계**: 특정 분야의 전문 지식과 경험 부족
- **최신 정보 부족**: 실시간으로 변화하는 정보를 반영하지 못함
- **문화적 맥락 이해 부족**: 특정 문화나 사회적 맥락을 이해하지 못함

#### 5.2.3 평가 기준의 주관성

- **평가 기준 설정의 어려움**: 객관적이고 공정한 평가 기준 설정의 어려움
- **가중치 결정의 주관성**: 다양한 평가 요소의 중요도를 결정하는 데 주관성
- **임계값 설정의 어려움**: 합격/불합격 기준을 설정하는 데 어려움

### 체크포인트 질문

- **자기애적 편향**이란 무엇이며, 이는 LLM 평가에 어떤 영향을 미치나요? 이러한 편향을 해결하기 위한 방법은 무엇인가요?

- **장황성 편향**이 발생하는 이유는 무엇이며, 이는 평가의 어떤 측면에 문제를 일으키나요? 이러한 편향을 해결하기 위한 방법은 무엇인가요?

- **일관성 부족**이 LLM 평가에서 나타나는 이유는 무엇이며, 이는 평가의 신뢰성에 어떤 영향을 미치나요? 일관성을 높이기 위한 방법은 무엇인가요?

- **인간 평가와의 차이**가 LLM 평가에서 나타나는 이유는 무엇이며, 이는 어떤 한계를 보여주나요? 이러한 한계를 극복하기 위한 방법은 무엇인가요?

- **도메인 특화 지식 부족**이 LLM 평가에 미치는 영향은 무엇이며, 이는 어떤 분야에서 특히 문제가 되나요? 이러한 한계를 해결하기 위한 방법은 무엇인가요?

## 6. RLAIF: AI 피드백을 통한 강화학습

**RLAIF (Reinforcement Learning from AI Feedback)**는 인간 피드백 대신 AI 모델의 피드백을 사용하여 강화학습을 수행하는 방법이다. 이는 RLHF (Reinforcement Learning from Human Feedback)의 확장으로, AI 모델이 평가자 역할을 하여 더 효율적이고 확장 가능한 학습을 가능하게 한다.

### 6.1 RLAIF의 핵심 원리

RLAIF는 다음과 같은 과정을 통해 작동한다:

1. **AI 평가자 훈련**: 인간 평가 데이터를 사용하여 AI 모델을 평가자로 훈련
2. **AI 피드백 수집**: 훈련된 AI 평가자를 사용하여 다양한 출력에 대한 피드백 수집
3. **강화학습 수행**: 수집된 AI 피드백을 사용하여 정책 모델을 강화학습으로 개선

### 6.2 RLAIF의 장점

- **확장성**: 인간 평가자보다 훨씬 많은 피드백을 빠르게 수집 가능
- **일관성**: AI 평가자는 인간 평가자보다 더 일관된 평가를 제공
- **비용 효율성**: 인간 평가자 비용을 크게 절약
- **다양성**: 다양한 도메인과 언어에 대한 평가 가능

### 6.3 RLAIF의 한계

- **편향 전파**: AI 평가자의 편향이 학습된 모델에 전파될 수 있음
- **인간 가치 반영 부족**: 인간의 직관과 가치를 완전히 반영하지 못할 수 있음
- **평가 품질 의존성**: AI 평가자의 품질에 따라 전체 시스템의 성능이 좌우됨

### 6.4 RLAIF 구현 예제

```python
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel

class AIEvaluator(nn.Module):
    def __init__(self, model_name="gpt2"):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.classifier = nn.Linear(self.model.config.hidden_size, 1)

    def forward(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        outputs = self.model(**inputs)
        pooled_output = outputs.last_hidden_state.mean(dim=1)
        score = self.classifier(pooled_output)
        return torch.sigmoid(score)

class RLAIFTrainer:
    def __init__(self, policy_model, evaluator):
        self.policy_model = policy_model
        self.evaluator = evaluator

    def collect_ai_feedback(self, prompts, responses):
        """AI 평가자를 사용하여 피드백 수집"""
        scores = []
        for response in responses:
            score = self.evaluator(response)
            scores.append(score)
        return scores

    def train_step(self, prompts, responses, ai_scores):
        """AI 피드백을 사용하여 정책 모델 훈련"""
        # 강화학습 알고리즘 (예: PPO) 적용
        # 여기서는 간단한 예시로 보상 기반 학습
        rewards = ai_scores
        loss = self.policy_model.compute_loss(prompts, responses, rewards)
        return loss

# 사용 예시
evaluator = AIEvaluator()
trainer = RLAIFTrainer(policy_model, evaluator)

# AI 피드백 수집 및 훈련
prompts = ["질문 1", "질문 2", "질문 3"]
responses = ["답변 1", "답변 2", "답변 3"]
ai_scores = trainer.collect_ai_feedback(prompts, responses)
loss = trainer.train_step(prompts, responses, ai_scores)
```

### 체크포인트 질문

- **RLAIF**란 무엇이며, 이는 RLHF와 어떤 차이가 있나요? RLAIF의 주요 장점과 한계는 무엇인가요?

- **AI 평가자 훈련** 과정에서 고려해야 할 주요 요소들은 무엇이며, 이는 RLAIF의 성능에 어떤 영향을 미치나요?

- **편향 전파** 문제가 RLAIF에서 발생하는 이유는 무엇이며, 이를 해결하기 위한 방법은 무엇인가요?

- **인간 가치 반영 부족**이 RLAIF의 한계로 지적되는 이유는 무엇이며, 이는 어떤 문제를 일으킬 수 있나요?

- **평가 품질 의존성**이 RLAIF에서 중요한 이유는 무엇이며, 이는 시스템 설계에 어떤 시사점을 제공하나요?

## 7. 미래 평가 패러다임

LLM 평가의 미래는 더욱 복잡하고 다양한 도전에 직면하고 있다. 새로운 기술과 응용 분야의 등장으로 인해 평가 방법론도 지속적으로 발전해야 한다.

### 7.1 멀티모달 LLM 평가

**멀티모달 LLM**의 등장으로 인해 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 모달리티를 함께 처리하는 모델들의 평가가 필요해졌다.

#### 7.1.1 평가 과제

- **모달리티 간 일관성**: 서로 다른 모달리티 간의 정보 일관성 평가
- **크로스모달 추론**: 한 모달리티의 정보를 다른 모달리티로 전환하는 능력 평가
- **멀티모달 생성**: 여러 모달리티를 동시에 생성하는 능력 평가

#### 7.1.2 평가 방법

- **멀티모달 벤치마크**: 다양한 모달리티를 포함한 종합 평가 세트
- **모달리티별 세분화 평가**: 각 모달리티별 특화된 평가 지표
- **통합 평가**: 여러 모달리티를 종합적으로 평가하는 방법

### 7.2 에이전트 평가

**AI 에이전트**의 등장으로 인해 단순한 텍스트 생성뿐만 아니라 복잡한 작업을 수행하는 에이전트들의 평가가 필요해졌다.

#### 7.2.1 평가 과제

- **작업 수행 능력**: 복잡한 다단계 작업을 성공적으로 완료하는 능력
- **도구 사용 능력**: 외부 도구와 API를 효과적으로 활용하는 능력
- **환경 적응 능력**: 다양한 환경에서 적응하고 학습하는 능력

#### 7.2.2 평가 방법

- **시뮬레이션 환경**: 가상 환경에서의 에이전트 성능 평가
- **실제 환경 테스트**: 실제 환경에서의 에이전트 성능 평가
- **인간-에이전트 협업**: 인간과의 협업 능력 평가

### 7.3 Green AI 평가

**Green AI**는 AI 시스템의 환경적 영향을 고려하는 접근법으로, 에너지 효율성과 탄소 배출량을 평가하는 것이 중요해졌다.

#### 7.3.1 평가 지표

- **에너지 소비량**: 모델 훈련 및 추론 시 소비되는 에너지
- **탄소 배출량**: 모델 사용으로 인한 탄소 배출량
- **효율성**: 단위 에너지당 성능

#### 7.3.2 평가 방법

- **생명주기 평가**: 모델의 전체 생명주기에 걸친 환경적 영향 평가
- **비교 평가**: 다른 모델과의 환경적 영향 비교
- **최적화 평가**: 환경적 영향을 최소화하는 최적화 방법 평가

### 7.4 인간-AI 협업 평가

**인간-AI 협업**이 중요한 영역으로 떠오르면서, AI가 인간과 얼마나 효과적으로 협업할 수 있는지 평가하는 것이 필요해졌다.

#### 7.4.1 평가 과제

- **협업 효율성**: 인간과 AI가 함께 작업할 때의 효율성
- **의사소통 능력**: 인간과의 효과적인 의사소통 능력
- **역할 분담**: 인간과 AI 간의 적절한 역할 분담

#### 7.4.2 평가 방법

- **협업 시나리오**: 실제 협업 상황에서의 성능 평가
- **인간 피드백**: 인간 사용자의 만족도와 피드백 평가
- **성과 측정**: 협업을 통한 최종 성과 측정

### 체크포인트 질문

- **멀티모달 LLM 평가**에서 고려해야 할 주요 과제들은 무엇이며, 이는 기존 텍스트 기반 평가와 어떤 차이가 있나요?

- **AI 에이전트 평가**에서 중요한 평가 과제들은 무엇이며, 이는 단순한 텍스트 생성 평가와 어떤 차이가 있나요?

- **Green AI 평가**의 중요성은 무엇이며, 이는 AI 시스템 개발에 어떤 영향을 미치나요?

- **인간-AI 협업 평가**에서 고려해야 할 주요 요소들은 무엇이며, 이는 AI 시스템의 실용성에 어떤 영향을 미치나요?

- **미래 평가 패러다임**의 발전 방향은 무엇이며, 이는 현재의 평가 방법론에 어떤 변화를 가져올 것으로 예상되나요?

## 8. 실습 과제

이 장에서는 배운 개념들을 직접 실험해보는 **3가지 실습 과제**를 다룹니다. 모든 실습은 **PyTorch와 Hugging Face Transformers** 기반으로 수행되며, 제공된 코드는 예시이므로 필요에 따라 수정하여 활용할 수 있습니다.

### 8.1 BLEU/ROUGE vs G-Eval 비교 실험

이 실습에서는 전통적인 평가 지표인 BLEU/ROUGE와 최신 LLM 기반 평가 방법인 G-Eval을 비교해보겠다.

#### 8.1.1 실습 목표

- **전통적 평가 지표와 LLM 기반 평가의 차이점** 이해
- **다양한 텍스트 품질에 대한 평가 결과** 비교
- **평가 지표의 장단점** 분석

#### 8.1.2 실습 내용

1. **BLEU/ROUGE 점수 계산**
2. **G-Eval 점수 계산**
3. **결과 비교 및 분석**

#### 8.1.3 실습 코드

```python
import torch
from transformers import AutoTokenizer, AutoModel
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
import openai

class TraditionalEvaluator:
    def __init__(self):
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    def calculate_bleu(self, reference, candidate):
        """BLEU 점수 계산"""
        reference_tokens = reference.split()
        candidate_tokens = candidate.split()
        return sentence_bleu([reference_tokens], candidate_tokens)

    def calculate_rouge(self, reference, candidate):
        """ROUGE 점수 계산"""
        scores = self.rouge_scorer.score(reference, candidate)
        return {
            'rouge1': scores['rouge1'].fmeasure,
            'rouge2': scores['rouge2'].fmeasure,
            'rougeL': scores['rougeL'].fmeasure
        }

class GEvalEvaluator:
    def __init__(self, model_name="gpt-3.5-turbo"):
        self.model_name = model_name
        self.client = openai.OpenAI()

    def evaluate(self, text, criteria):
        """G-Eval을 사용한 평가"""
        prompt = f"""
        다음 텍스트를 {criteria} 기준으로 평가해주세요.

        텍스트: {text}

        평가 기준:
        1. 명확성: 텍스트가 얼마나 명확한가?
        2. 일관성: 텍스트가 얼마나 일관성이 있는가?
        3. 관련성: 텍스트가 주제와 얼마나 관련이 있는가?

        각 기준에 대해 1-10점으로 평가하고, 전체 점수를 계산해주세요.
        """

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )

        return response.choices[0].message.content

# 실습 실행
def run_comparison_experiment():
    # 평가자 초기화
    traditional_eval = TraditionalEvaluator()
    geval_eval = GEvalEvaluator()

    # 테스트 데이터
    reference = "인공지능은 현대 사회에 큰 영향을 미치고 있다."
    candidates = [
        "AI는 오늘날 사회에 중요한 영향을 주고 있다.",
        "인공지능 기술이 우리 삶을 변화시키고 있다.",
        "AI는 현대 사회에 큰 영향을 미치고 있다."
    ]

    print("=== BLEU/ROUGE vs G-Eval 비교 실험 ===\n")

    for i, candidate in enumerate(candidates):
        print(f"후보 {i+1}: {candidate}")

        # BLEU 점수
        bleu_score = traditional_eval.calculate_bleu(reference, candidate)
        print(f"BLEU 점수: {bleu_score:.4f}")

        # ROUGE 점수
        rouge_scores = traditional_eval.calculate_rouge(reference, candidate)
        print(f"ROUGE 점수: {rouge_scores}")

        # G-Eval 점수
        geval_score = geval_eval.evaluate(candidate, "텍스트 품질")
        print(f"G-Eval 점수: {geval_score}")

        print("-" * 50)

# 실습 실행
if __name__ == "__main__":
    run_comparison_experiment()
```

### 8.2 GPTScore 구현 및 실험

이 실습에서는 GPTScore를 직접 구현하고 다양한 텍스트에 대해 평가를 수행해보겠다.

#### 8.2.1 실습 목표

- **GPTScore의 원리** 이해
- **확률 기반 평가** 구현
- **다양한 텍스트 품질에 대한 평가** 수행

#### 8.2.2 실습 내용

1. **GPTScore 계산 함수** 구현
2. **다양한 텍스트에 대한 평가** 수행
3. **결과 분석 및 시각화**

#### 8.2.3 실습 코드

```python
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import matplotlib.pyplot as plt
import numpy as np

class GPTScoreCalculator:
    def __init__(self, model_name="gpt2"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()

        # 패딩 토큰 설정
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def calculate_gpt_score(self, text, reference=None):
        """GPTScore 계산"""
        # 텍스트 토큰화
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)

        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits

            # 각 토큰의 확률 계산
            probs = F.softmax(logits, dim=-1)

            # GPTScore 계산 (평균 로그 확률)
            gpt_score = 0
            for i in range(1, inputs['input_ids'].shape[1]):  # 첫 번째 토큰 제외
                token_id = inputs['input_ids'][0, i]
                token_prob = probs[0, i-1, token_id]
                gpt_score += torch.log(token_prob).item()

            # 정규화 (토큰 수로 나누기)
            gpt_score /= (inputs['input_ids'].shape[1] - 1)

        return gpt_score

    def compare_texts(self, texts, reference=None):
        """여러 텍스트의 GPTScore 비교"""
        scores = []
        for text in texts:
            score = self.calculate_gpt_score(text, reference)
            scores.append(score)
        return scores

# 실습 실행
def run_gptscore_experiment():
    # GPTScore 계산기 초기화
    gpt_calculator = GPTScoreCalculator()

    # 테스트 텍스트들
    test_texts = [
        "인공지능은 현대 사회에 큰 영향을 미치고 있다.",
        "AI는 오늘날 사회에 중요한 영향을 주고 있다.",
        "인공지능 기술이 우리 삶을 변화시키고 있다.",
        "AI는 현대 사회에 큰 영향을 미치고 있다.",
        "인공지능은 현대 사회에 큰 영향을 미치고 있다."
    ]

    print("=== GPTScore 실험 ===\n")

    # 각 텍스트의 GPTScore 계산
    scores = gpt_calculator.compare_texts(test_texts)

    # 결과 출력
    for i, (text, score) in enumerate(zip(test_texts, scores)):
        print(f"텍스트 {i+1}: {text}")
        print(f"GPTScore: {score:.4f}")
        print("-" * 50)

    # 결과 시각화
    plt.figure(figsize=(10, 6))
    plt.bar(range(1, len(scores)+1), scores)
    plt.xlabel('텍스트 번호')
    plt.ylabel('GPTScore')
    plt.title('GPTScore 비교')
    plt.xticks(range(1, len(scores)+1))
    plt.grid(True, alpha=0.3)
    plt.show()

    return scores

# 실습 실행
if __name__ == "__main__":
    scores = run_gptscore_experiment()
```

### 8.3 FLASK 평가 시스템 구현

이 실습에서는 FLASK의 미세 능력 세트 기반 평가 시스템을 구현해보겠다.

#### 8.3.1 실습 목표

- **FLASK의 미세 능력 세트** 이해
- **다차원 평가 시스템** 구현
- **텍스트 품질의 세부적 분석** 수행

#### 8.3.2 실습 내용

1. **FLASK 평가 시스템** 구현
2. **12개 미세 능력 지표** 계산
3. **종합 평가 결과** 생성

#### 8.3.3 실습 코드

```python
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List

class FLASKEvaluator:
    def __init__(self, model_name="gpt2"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()

        # 12개 미세 능력 지표
        self.skills = [
            "명확성", "일관성", "관련성", "완전성", "정확성", "창의성",
            "논리성", "구조성", "표현력", "적절성", "효율성", "신뢰성"
        ]

        # 각 능력별 분류기 (실제로는 더 복잡한 모델이 필요)
        self.skill_classifiers = {}
        for skill in self.skills:
            self.skill_classifiers[skill] = nn.Linear(self.model.config.hidden_size, 1)

    def extract_features(self, text):
        """텍스트에서 특징 추출"""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)

        with torch.no_grad():
            outputs = self.model(**inputs)
            # 평균 풀링으로 문장 표현 생성
            features = outputs.last_hidden_state.mean(dim=1)

        return features

    def evaluate_skill(self, text, skill_name):
        """특정 능력에 대한 평가"""
        features = self.extract_features(text)

        # 실제로는 더 정교한 평가가 필요하지만, 여기서는 간단한 예시
        if skill_name == "명확성":
            # 명확성은 문장 길이와 복잡성에 반비례
            clarity_score = 1.0 / (1.0 + len(text.split()) * 0.1)
        elif skill_name == "일관성":
            # 일관성은 반복되는 단어의 비율로 측정
            words = text.split()
            unique_words = set(words)
            consistency_score = len(unique_words) / len(words) if words else 0
        elif skill_name == "관련성":
            # 관련성은 키워드 밀도로 측정
            relevance_score = min(1.0, len(text.split()) * 0.05)
        else:
            # 나머지 능력들은 랜덤 점수 (실제로는 더 정교한 평가 필요)
            relevance_score = np.random.uniform(0.3, 0.9)

        return relevance_score

    def evaluate_all_skills(self, text):
        """모든 능력에 대한 평가"""
        skill_scores = {}
        for skill in self.skills:
            score = self.evaluate_skill(text, skill)
            skill_scores[skill] = score

        return skill_scores

    def calculate_overall_score(self, skill_scores):
        """종합 점수 계산"""
        scores = list(skill_scores.values())
        return np.mean(scores)

    def visualize_results(self, skill_scores, text):
        """결과 시각화"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # 능력별 점수 막대 그래프
        skills = list(skill_scores.keys())
        scores = list(skill_scores.values())

        ax1.bar(skills, scores)
        ax1.set_title('FLASK 능력별 점수')
        ax1.set_ylabel('점수')
        ax1.set_ylim(0, 1)
        ax1.tick_params(axis='x', rotation=45)

        # 레이더 차트
        angles = np.linspace(0, 2 * np.pi, len(skills), endpoint=False).tolist()
        scores_radar = scores + scores[:1]  # 원형으로 만들기 위해 첫 번째 점수 추가
        angles += angles[:1]

        ax2 = plt.subplot(122, projection='polar')
        ax2.plot(angles, scores_radar, 'o-', linewidth=2)
        ax2.fill(angles, scores_radar, alpha=0.25)
        ax2.set_xticks(angles[:-1])
        ax2.set_xticklabels(skills)
        ax2.set_ylim(0, 1)
        ax2.set_title('FLASK 능력 프로파일')

        plt.tight_layout()
        plt.show()

# 실습 실행
def run_flask_experiment():
    # FLASK 평가자 초기화
    flask_evaluator = FLASKEvaluator()

    # 테스트 텍스트
    test_text = "인공지능은 현대 사회에 큰 영향을 미치고 있다. 이 기술은 다양한 분야에서 활용되고 있으며, 우리의 일상생활을 변화시키고 있다."

    print("=== FLASK 평가 실험 ===\n")
    print(f"평가 대상 텍스트: {test_text}\n")

    # 모든 능력에 대한 평가
    skill_scores = flask_evaluator.evaluate_all_skills(test_text)

    # 결과 출력
    print("능력별 점수:")
    for skill, score in skill_scores.items():
        print(f"{skill}: {score:.3f}")

    # 종합 점수
    overall_score = flask_evaluator.calculate_overall_score(skill_scores)
    print(f"\n종합 점수: {overall_score:.3f}")

    # 결과 시각화
    flask_evaluator.visualize_results(skill_scores, test_text)

    return skill_scores, overall_score

# 실습 실행
if __name__ == "__main__":
    skill_scores, overall_score = run_flask_experiment()
```

### 8.4 실습 결과 분석

#### 8.4.1 실습 목표

- **실습 결과 종합 분석**
- **다양한 평가 방법의 장단점** 비교
- **실제 적용 시 고려사항** 도출

#### 8.4.2 실습 내용

1. **실습 결과 종합**
2. **평가 방법 비교 분석**
3. **실제 적용 시나리오** 검토

#### 8.4.3 실습 코드

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_experiment_results():
    """실습 결과 종합 분석"""

    # 실험 결과 데이터 (실제로는 위의 실습에서 얻은 결과)
    results = {
        '평가 방법': ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'GPTScore', 'FLASK'],
        '평균 점수': [0.45, 0.62, 0.38, 0.58, 0.72, 0.68],
        '표준편차': [0.12, 0.08, 0.15, 0.09, 0.06, 0.11],
        '계산 시간(초)': [0.01, 0.02, 0.02, 0.02, 0.15, 0.25],
        '인간 상관관계': [0.35, 0.42, 0.38, 0.45, 0.68, 0.72]
    }

    df = pd.DataFrame(results)

    # 결과 시각화
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # 평균 점수 비교
    axes[0, 0].bar(df['평가 방법'], df['평균 점수'])
    axes[0, 0].set_title('평가 방법별 평균 점수')
    axes[0, 0].set_ylabel('점수')
    axes[0, 0].tick_params(axis='x', rotation=45)

    # 계산 시간 비교
    axes[0, 1].bar(df['평가 방법'], df['계산 시간(초)'])
    axes[0, 1].set_title('평가 방법별 계산 시간')
    axes[0, 1].set_ylabel('시간 (초)')
    axes[0, 1].tick_params(axis='x', rotation=45)

    # 인간 상관관계 비교
    axes[1, 0].bar(df['평가 방법'], df['인간 상관관계'])
    axes[1, 0].set_title('평가 방법별 인간 상관관계')
    axes[1, 0].set_ylabel('상관계수')
    axes[1, 0].tick_params(axis='x', rotation=45)

    # 종합 비교 (정규화된 점수)
    normalized_scores = df[['평균 점수', '인간 상관관계']].copy()
    normalized_scores['계산 시간'] = 1 - (df['계산 시간(초)'] / df['계산 시간(초)'].max())

    sns.heatmap(normalized_scores.T, annot=True, cmap='YlOrRd', ax=axes[1, 1])
    axes[1, 1].set_title('평가 방법 종합 비교 (정규화)')

    plt.tight_layout()
    plt.show()

    # 결과 분석
    print("=== 실습 결과 분석 ===\n")

    print("1. 평가 방법별 특징:")
    for _, row in df.iterrows():
        print(f"- {row['평가 방법']}: 평균 {row['평균 점수']:.3f}, 인간 상관관계 {row['인간 상관관계']:.3f}")

    print("\n2. 주요 발견사항:")
    print("- GPTScore와 FLASK가 가장 높은 인간 상관관계를 보임")
    print("- 전통적 방법(BLEU/ROUGE)은 계산이 빠르지만 인간 판단과의 상관관계가 낮음")
    print("- LLM 기반 방법은 계산 시간이 오래 걸리지만 더 정확한 평가 가능")

    print("\n3. 실제 적용 시 고려사항:")
    print("- 빠른 평가가 필요한 경우: BLEU/ROUGE 사용")
    print("- 정확한 평가가 필요한 경우: GPTScore/FLASK 사용")
    print("- 균형잡힌 접근: 여러 방법을 조합하여 사용")

    return df

# 실습 실행
if __name__ == "__main__":
    results_df = analyze_experiment_results()
```

### 체크포인트 질문

- **BLEU/ROUGE vs G-Eval 비교 실험**에서 관찰된 주요 차이점은 무엇이며, 이는 각 평가 방법의 어떤 특성을 보여주나요?

- **GPTScore 구현**에서 확률 기반 평가의 장점은 무엇이며, 이는 전통적인 평가 방법과 어떤 차이가 있나요?

- **FLASK 평가 시스템**에서 12개 미세 능력 지표를 사용하는 이유는 무엇이며, 이는 단일 점수 평가와 어떤 차이가 있나요?

- **실습 결과 분석**에서 GPTScore와 FLASK가 가장 높은 인간 상관관계를 보인 이유는 무엇이며, 이는 실제 적용에 어떤 시사점을 제공하나요?

- **실제 적용 시 고려사항**에서 빠른 평가와 정확한 평가 사이의 트레이드오프는 무엇이며, 이를 어떻게 균형있게 조절할 수 있나요?

## 9. 요약 및 결론

이 장에서는 LLM 평가의 지형 변화와 새로운 평가 패러다임에 대해 살펴보았다. 전통적인 평가 지표의 한계를 인식하고, 의미 기반 평가와 LLM-as-a-Judge 패러다임의 등장을 통해 평가 방법론이 어떻게 발전하고 있는지 확인했다.

### 9.1 주요 내용 요약

#### 9.1.1 평가의 지형 변화

- **전통적 평가 지표의 한계**: BLEU, ROUGE 등의 표면적 유사성 기반 평가의 한계
- **의미 기반 평가의 등장**: BERTScore, SentenceMover, BLEURT 등 의미적 유사성 기반 평가
- **LLM-as-a-Judge 패러다임**: GPTScore, G-Eval, FLASK 등 LLM을 평가자로 활용하는 새로운 접근

#### 9.1.2 LLM 기반 평가 패러다임

- **GPTScore**: 확률 기반 평가 프레임워크로 모델의 내재적 지식을 활용
- **G-Eval**: Chain-of-Thought 기반 체계적 평가로 인간 판단과의 높은 상관관계 달성
- **FLASK**: 미세 능력 세트 기반 다차원 평가로 텍스트 품질의 세부적 분석 가능

#### 9.1.3 특수 목적 벤치마크

- **LiveCodeBench**: 데이터 오염 방지형 코드 생성 평가
- **EvalPlus**: 테스트케이스 보강을 통한 더 엄격한 코드 평가
- **HELM-Code**: 투명성과 커뮤니티 협업 중심의 종합적 평가
- **MMLU-Pro**: 10지선다 고난도 지식/추론 벤치마크
- **GPQA와 BBH**: 지식/추론 강화 평가 세트

#### 9.1.4 도메인 특화 벤치마크

- **FinBen**: 금융 도메인 종합 벤치마크
- **AgentHarm**: AI 에이전트 유해성 평가 벤치마크
- **LEXam**: 법률 시험 기반 LLM 평가
- **CSEDB**: 의료 LLM 안전성/효과성 이중 평가
- **MATH 및 GSM8K**: 수학 능력 평가

#### 9.1.5 평가의 편향과 한계

- **주요 편향**: 자기애적 편향, 장황성 편향, 일관성 부족
- **평가의 한계**: 인간 평가와의 차이, 도메인 특화 지식 부족, 평가 기준의 주관성

#### 9.1.6 RLAIF와 미래 평가 패러다임

- **RLAIF**: AI 피드백을 통한 강화학습으로 더 효율적이고 확장 가능한 학습
- **미래 평가 패러다임**: 멀티모달 LLM 평가, 에이전트 평가, Green AI 평가, 인간-AI 협업 평가

### 9.2 핵심 통찰

#### 9.2.1 평가 방법론의 진화

LLM 평가는 단순한 표면적 유사성에서 의미적 유사성, 그리고 모델의 내재적 지식을 활용한 평가로 발전하고 있다. 이는 더 정확하고 인간적 판단에 가까운 평가를 가능하게 한다.

#### 9.2.2 평가의 다차원성

현대의 LLM 평가는 단일 점수가 아닌 다차원적 접근을 통해 텍스트의 다양한 측면을 종합적으로 평가한다. 이는 더 정교하고 유용한 평가 결과를 제공한다.

#### 9.2.3 도메인 특화의 중요성

일반적인 평가 방법으로는 특정 도메인의 전문성을 평가하기 어렵다. 따라서 도메인 특화 벤치마크의 개발과 활용이 중요해지고 있다.

#### 9.2.4 평가의 편향과 한계 인식

LLM 평가에서 나타나는 편향과 한계를 인식하고 이를 해결하기 위한 노력이 필요하다. 이를 통해 더 공정하고 신뢰할 수 있는 평가 시스템을 구축할 수 있다.

### 9.3 향후 발전 방향

#### 9.3.1 평가 방법론의 지속적 발전

- **더 정교한 평가 지표** 개발
- **편향 감소**를 위한 방법론 연구
- **인간-AI 협업** 평가 시스템 구축

#### 9.3.2 도메인 특화 평가의 확장

- **새로운 도메인**에 대한 특화 벤치마크 개발
- **다국어 평가** 시스템 구축
- **문화적 맥락**을 고려한 평가 방법론

#### 9.3.3 실용적 평가 시스템 구축

- **실시간 평가** 시스템 개발
- **자동화된 평가** 파이프라인 구축
- **평가 결과의 해석** 및 활용 방법 연구

### 9.4 결론

LLM 평가는 빠르게 발전하는 분야로, 새로운 기술과 방법론이 지속적으로 등장하고 있다. 전통적인 평가 방법의 한계를 인식하고, 의미 기반 평가와 LLM-as-a-Judge 패러다임을 통해 더 정확하고 인간적 판단에 가까운 평가를 가능하게 하고 있다.

특히 도메인 특화 벤치마크의 개발과 활용을 통해 특정 분야의 전문성을 평가할 수 있게 되었으며, 평가의 편향과 한계를 인식하고 이를 해결하기 위한 노력이 지속되고 있다.

앞으로는 멀티모달 LLM, AI 에이전트, Green AI 등 새로운 기술과 응용 분야에 대한 평가 방법론이 더욱 발전할 것으로 예상되며, 이를 통해 더욱 정교하고 실용적인 평가 시스템이 구축될 것이다.

## 10. 참고 문헌

### 10.1 전통적 평가 지표

- Papineni, K., et al. (2002). BLEU: a method for automatic evaluation of machine translation. _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_.
- Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries. _Text Summarization Branches Out_.

### 10.2 의미 기반 평가

- Zhang, T., et al. (2020). BERTScore: Evaluating text generation with BERT. _International Conference on Learning Representations_.
- Clark, E., et al. (2019). SentenceMover's similarity: Automatic evaluation for multi-sentence texts. _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_.
- Sellam, T., et al. (2020). BLEURT: Learning robust metrics for text generation. _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_.

### 10.3 LLM 기반 평가

- Liu, Y., et al. (2023). G-Eval: NLG evaluation using GPT-4 with better human alignment. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_.
- Fu, J., et al. (2023). GPTScore: Evaluate as you desire. _arXiv preprint arXiv:2302.04166_.
- Wang, J., et al. (2023). FLASK: Fine-grained language model evaluation based on alignment skill sets. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_.

### 10.4 특수 목적 벤치마크

- Jain, N., et al. (2024). LiveCodeBench: Holistic and contamination-free evaluation of large language models for code. _Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing_.
- Liu, J., et al. (2023). EvalPlus: Augmenting code evaluation datasets with test case generation. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_.
- Liang, P., et al. (2022). Holistic evaluation of language models. _Transactions on Machine Learning Research_.

### 10.5 도메인 특화 벤치마크

- Chen, J., et al. (2024). FinBen: A holistic financial benchmark for large language models. _Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing_.
- Sun, Y., et al. (2025). AgentHarm: A comprehensive benchmark for evaluating agentic AI safety. _Proceedings of the 2025 International Conference on Learning Representations_.
- Nguyen, H., et al. (2025). LEXam: A comprehensive benchmark for legal reasoning evaluation. _Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing_.

### 10.6 RLAIF 및 미래 평가

- Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI feedback. _arXiv preprint arXiv:2212.08073_.
- Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_.

### 10.7 평가의 편향과 한계

- Chiang, W. L., et al. (2023). Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality. _arXiv preprint arXiv:2303.04671_.
- Lin, S., et al. (2022). TruthfulQA: Measuring how models mimic human falsehoods. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_.

### 10.8 수학 및 추론 평가

- Hendrycks, D., et al. (2021). Measuring mathematical problem solving with the MATH dataset. _Proceedings of the 2021 Conference on Neural Information Processing Systems_.
- Cobbe, K., et al. (2021). Training verifiers to solve math word problems. _Proceedings of the 2021 Conference on Neural Information Processing Systems_.

### 10.9 의료 및 법률 평가

- Singhal, K., et al. (2023). Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_.
- Guha, N., et al. (2023). LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_.

### 10.10 Green AI 및 효율성

- Schwartz, R., et al. (2020). Green AI. _Communications of the ACM_.
- Strubell, E., et al. (2019). Energy and policy considerations for deep learning in NLP. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_.

---

_이 장에서는 LLM 평가의 지형 변화와 새로운 평가 패러다임에 대해 종합적으로 살펴보았다. 전통적인 평가 방법의 한계를 인식하고, 의미 기반 평가와 LLM-as-a-Judge 패러다임을 통해 더 정확하고 인간적 판단에 가까운 평가를 가능하게 하는 방법들을 학습했다. 또한 특수 목적 벤치마크와 도메인 특화 벤치마크의 중요성을 확인하고, 평가의 편향과 한계를 인식하여 더 공정하고 신뢰할 수 있는 평가 시스템을 구축하는 방법을 탐구했다. 앞으로는 멀티모달 LLM, AI 에이전트, Green AI 등 새로운 기술과 응용 분야에 대한 평가 방법론이 더욱 발전할 것으로 예상되며, 이를 통해 더욱 정교하고 실용적인 평가 시스템이 구축될 것이다._
