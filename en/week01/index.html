
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 1 - Transformer and Next-Generation Architectures &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week01/index';</script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A" href="qna.html" />
    <link rel="prev" title="Deep Learning for Natural Language Processing (131307379A)" href="../index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Deep Learning for Natural Language Processing (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Week 1 - Transformer and Next-Generation Architectures</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qna.html">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2 - PyTorch 2.x and Latest Deep Learning Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: Advanced Prompting Techniques and Optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/en/week01/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek01/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week01/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 1 - Transformer and Next-Generation Architectures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-structure-of-transformer-architecture">1. Basic Structure of Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-operation-example-code">Self-Attention Operation Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-architecture-selective-state-space-model">2. Mamba Architecture – Selective State Space Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-structure-and-usage-example-code">Mamba Structure and Usage Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv-architecture-efficient-processing-with-rnn-like-structure">3. RWKV Architecture – Efficient Processing with RNN-like Structure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv-model-usage-example-code">RWKV Model Usage Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-architecture-moe-based-transformer-mamba-hybrid">4. Jamba Architecture – MoE-based Transformer+Mamba Hybrid</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-s-model-structure">Jamba’s Model Structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#large-scale-context-window-and-cost-efficiency">Large-Scale Context Window and Cost-Efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe-mixture-of-experts-utilization">MoE (Mixture of Experts) Utilization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#usage">Usage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-model-utilization-example-code">Jamba Model Utilization Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison-by-architecture">5. Performance Comparison by Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-latest-open-source-llms-and-characteristics">6. Introduction to Latest Open Source LLMs and Characteristics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">Llama 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixtral-87b">Mixtral 8×7B</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qwen2-72b">Qwen2-72B</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-guidelines">7. Practice Guidelines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#major-papers-and-research-materials">Major Papers and Research Materials</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-documents-and-implementations">Technical Documents and Implementations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-resources-and-blogs">Online Resources and Blogs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarks-and-evaluation-materials">Benchmarks and Evaluation Materials</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-1-transformer-and-next-generation-architectures">
<h1>Week 1 - Transformer and Next-Generation Architectures<a class="headerlink" href="#week-1-transformer-and-next-generation-architectures" title="Link to this heading">#</a></h1>
<section id="basic-structure-of-transformer-architecture">
<h2>1. Basic Structure of Transformer Architecture<a class="headerlink" href="#basic-structure-of-transformer-architecture" title="Link to this heading">#</a></h2>
<p>Transformer is a model based on <strong>Self-Attention</strong> mechanism that processes input sentences through an encoder-decoder structure. The encoder encodes the input sequence to generate context, and the decoder generates the output sequence by referring to this context. Self-Attention is a method where one token calculates its relationship (similarity) with all other tokens in the sequence to adjust its own representation. This enables <strong>parallel processing</strong>, making learning faster than RNN-based models and effectively learning long dependency relationships. However, <strong>computation is required for each token pair</strong>, so the attention operation cost increases to <span class="math notranslate nohighlight">\(O(n^2)\)</span>, making it inefficient as sequence length increases. Especially during <strong>inference</strong>, new tokens must be generated one by one while calculating attention with all previous tokens, requiring operations proportional to approximately <span class="math notranslate nohighlight">\(L^2\)</span> for sequence length <em>L</em>. This causes <strong>speed to slow down</strong> and <strong>memory usage to increase linearly</strong> during long context processing, becoming a major limitation.</p>
<p><em>Complete Transformer structure.</em> The left side represents the encoder, and the right side represents the decoder. Each encoder block consists of <strong>Multi-Head Self-Attention</strong> and feedforward (FFN), while decoder blocks additionally include <strong>masked Self-Attention</strong> and <strong>encoder-decoder attention</strong> (cross-attention). The attention module references all previous tokens to form the <strong>context</strong> for the current token. Transformer has no recurrent structure, making it advantageous for parallelization, and <strong>multi-head attention</strong> learns patterns in various representation spaces. However, due to the <strong>quadratic complexity</strong> of attention operations, <strong>memory and computation increase dramatically with sequence length</strong>, which is a disadvantage.</p>
<section id="self-attention-operation-example-code">
<h3>Self-Attention Operation Example Code<a class="headerlink" href="#self-attention-operation-example-code" title="Link to this heading">#</a></h3>
<p>The code below is an example implementing the core operations of Self-Attention using PyTorch (for single batch, sequence length <em>L</em>, dimension <em>d</em>). It calculates <strong>attention weights</strong> using query <span class="math notranslate nohighlight">\(Q\)</span>, key <span class="math notranslate nohighlight">\(K\)</span>, and value <span class="math notranslate nohighlight">\(V\)</span> vectors for each token, then computes output values through weighted sum:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span><span class="o">,</span><span class="w"> </span><span class="nn">math</span>

<span class="n">L</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">64</span>  <span class="c1"># sequence length L=5, embedding dimension d=64</span>
<span class="c1"># example Q, K, V matrices (batch=1, seq_len=L, dim=d)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="c1"># 1. Calculate attention score matrix (Q * K^T / sqrt(d))</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>  <span class="c1"># (1, L, L)</span>
<span class="c1"># 2. Get attention weight probability distribution through Softmax</span>
<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                 <span class="c1"># (1, L, L)</span>
<span class="c1"># 3. Calculate output by multiplying weights with V</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>                       <span class="c1"># (1, L, d)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1, 5, 64])</span>
</pre></div>
</div>
<p><strong>Code Explanation:</strong> In the above code, scores is the attention score matrix indicating how much attention each token <em>i</em> pays to other tokens <em>j</em>. Through softmax, attn<em>weights becomes a probability distribution with sum of 1 for each _i</em>. Finally, multiplying these weights with each <em>j</em>’s value vector <span class="math notranslate nohighlight">\(V_j\)</span> and summing gives the Self-Attention output for each position <em>i</em>.</p>
</section>
<section id="checkpoint-questions">
<h3>Checkpoint Questions<a class="headerlink" href="#checkpoint-questions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What are the main advantages of <strong>Self-Attention</strong> in Transformer? Also, where does the <strong>inference bottleneck</strong> occur?</p></li>
<li><p>Explain the difference between Transformer <strong>encoder-decoder structure</strong> and <strong>decoder-only structure</strong> like GPT.</p></li>
<li><p>How do Transformer’s <strong>time complexity</strong> and <strong>space complexity</strong> scale with sequence length <em>L</em>?</p></li>
</ul>
</section>
</section>
<section id="mamba-architecture-selective-state-space-model">
<h2>2. Mamba Architecture – Selective State Space Model<a class="headerlink" href="#mamba-architecture-selective-state-space-model" title="Link to this heading">#</a></h2>
<p><strong>Mamba</strong> is a new sequence model proposed in 2024 that presents an <strong>alternative to Transformer</strong> by utilizing <strong>Selective State Space Model</strong> (SSM). The core idea of Mamba is to apply <strong>continuous-time state space models</strong> to language modeling while <strong>dynamically controlling state transitions based on input</strong>. This allows <strong>selective retention or forgetting of past information</strong> at each time point, expressing <strong>content-based dependencies between tokens</strong>.</p>
<p>Mamba operates like <strong>recurrent neural networks</strong> (RNN) by updating previous <strong>hidden states</strong> while processing tokens sequentially, but overcame parallelization constraints through <strong>efficient algorithms</strong>. It can exchange information between tokens without attention or massive MLP feedforward layers by introducing <strong>selective scan algorithms</strong> and <strong>hardware-friendly parallelization techniques</strong>, optimizing to process <strong>long sequences in linear time</strong>. As a result, Mamba shows <strong>5x higher token processing throughput during inference</strong> and achieves performance that <strong>scales linearly with sequence length</strong>. In fact, the <strong>Mamba-3B model</strong> reportedly showed superior performance to same-size Transformers and <strong>performance comparable to 2x larger Transformers</strong>.</p>
<p>Mamba’s <strong>block structure</strong> is similar to Transformer blocks, consisting of multiple stacked layers. However, the internal composition is as follows:</p>
<ul class="simple">
<li><p>Apply <strong>linear projection</strong> to input embeddings to increase dimensions.</p></li>
<li><p>Pass through <strong>local convolution</strong> (layer) to prevent independent computation between tokens and mix adjacent token information.</p></li>
<li><p>Then <strong>Selective SSM layer</strong> is applied, performing <strong>continuous state updates</strong> based on <strong>HiPPO</strong> initialized state matrix <span class="math notranslate nohighlight">\(A\)</span>. At this stage, <strong>selective state compression</strong> algorithm is applied to maintain important information and forget unnecessary information.</p></li>
<li><p>Finally, token output is produced through <strong>normalization</strong> (norm) and output layer Softmax, etc.</p></li>
</ul>
<p>With this composition, <strong>Mamba blocks</strong> <strong>replace Self-Attention</strong> while including <strong>local operations similar to MLP</strong> to perform <strong>computation within tokens</strong>. Stacking multiple Mamba blocks can build <strong>deep sequence models</strong> identical to Transformer. Mamba has <strong>removed attention bottlenecks</strong> in this way, enabling practical handling of <strong>virtually unlimited context lengths</strong> and <strong>significantly improved learning and inference speeds</strong>.</p>
<p><strong>Note:</strong> In “Selective” SSM, the <strong>coefficients</strong> of the state space (state transition matrices, etc.) are dynamically determined as functions of token values. This overcomes the limitations of existing fixed SSMs and is the key to achieving high performance on <strong>discrete token data</strong> (natural language).</p>
<section id="mamba-structure-and-usage-example-code">
<h3>Mamba Structure and Usage Example Code<a class="headerlink" href="#mamba-structure-and-usage-example-code" title="Link to this heading">#</a></h3>
<p>Mamba is currently provided as a Python package and can be easily utilized. Below is an example of creating a <strong>Mamba block</strong> through the mamba-ssm library and processing tensor input:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="c1"># Import Mamba package</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mamba_ssm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mamba</span>

<span class="n">batch</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span>  <span class="c1"># example: batch=2, sequence=64, dimension=16</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>   <span class="c1"># arbitrary input tensor (GPU usage)</span>

<span class="c1"># Create Mamba block model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Mamba</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>  <span class="c1"># model dimension (embedding dimension)</span>
    <span class="n">d_state</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>   <span class="c1"># SSM state dimension (expansion degree)</span>
    <span class="n">d_conv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>     <span class="c1"># local convolution size</span>
    <span class="n">expand</span><span class="o">=</span><span class="mi">2</span>      <span class="c1"># internal channel expansion ratio</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># process input with Mamba block</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># check output tensor size (should be batch, length, dim)</span>
</pre></div>
</div>
<p>In the above code, Mamba(…) creates <strong>one Mamba block</strong>. Parameters like d_model, d_state, etc. use the recommended values presented in the paper. Calling model(x) performs <strong>selective state space operations</strong> on input x and returns output y of the same size. (In actual language models, such blocks are stacked in multiple layers, and LM head is attached to the output to predict vocabulary distributions.)</p>
<p><strong>Practice Tip:</strong> Mamba is implemented to work on GPU, so you need to move tensors and models with .to(“cuda”) like in the example above to properly utilize performance. Also, you need to install the package with pip install mamba-ssm and require NVIDIA CUDA 11.6+ environment.</p>
</section>
<section id="id1">
<h3>Checkpoint Questions<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What is the biggest advantage of Mamba <strong>compared to Transformer</strong>? Explain how Mamba can avoid the <strong><span class="math notranslate nohighlight">\(O(n^2)\)</span> bottleneck of attention</strong>.</p></li>
<li><p>What does the <strong>“selective”</strong> operation mean in Mamba’s Selective SSM? What effects can be obtained in language models as a result?</p></li>
<li><p>Mention the performance-related characteristics shown by the Mamba-3B model (e.g., comparison with same-size Transformer, comparison with 2x larger Transformer, etc.).</p></li>
</ul>
</section>
</section>
<section id="rwkv-architecture-efficient-processing-with-rnn-like-structure">
<h2>3. RWKV Architecture – Efficient Processing with RNN-like Structure<a class="headerlink" href="#rwkv-architecture-efficient-processing-with-rnn-like-structure" title="Link to this heading">#</a></h2>
<p><strong>RWKV</strong> is a <strong>hybrid architecture</strong> that combines the advantages of <strong>RNN (Recurrent Neural Network)</strong> and <strong>Transformer</strong>. The name <em>RWKV</em> derives from the four main parameters of the network: <strong>Receptance (R)</strong>, <strong>Weight (W)</strong>, <strong>Key (K)</strong>, <strong>Value (V)</strong>, which serve as <strong>past information acceptance gate</strong>, <strong>exponential time weight</strong>, <strong>key</strong>, and <strong>value vector</strong> respectively. RWKV internally has an RNN structure that alternately performs <strong>time-axis processing</strong> and <strong>channel (feedforward) processing</strong>, operating by <strong>decaying past states with exponential weights</strong> while accumulating Key/Value information at each step. This achieves <strong>effects similar to attention</strong> while maintaining linear processing cost per token.</p>
<p>RWKV’s <strong>two major characteristics</strong> are as follows:</p>
<ul class="simple">
<li><p><strong>Parallel learning possible like Transformer:</strong> While existing RNNs could only learn sequentially (time-step), making parallelization difficult, RWKV achieved <strong>parallelization during training</strong> by <strong>transforming to attention-like formulas</strong>. That is, <strong>during training, it processes the entire sequence at once like Transformer</strong> (using special linear attention forms), and <strong>during inference, it generates tokens one by one like RNN</strong>. This achieves both <strong>training efficiency</strong> and <strong>inference efficiency</strong>.</p></li>
<li><p><strong>Nearly infinite context:</strong> RWKV, being RNN-based, summarizes context with <strong>one fixed hidden state</strong> and continuously updates it, so theoretically <strong>there is no limit to context length</strong>. When generating new tokens, there’s no need to store all previous token information in massive KV cache, just <strong>maintain the previous step’s state</strong>. Therefore, memory usage is hardly affected by input length, and long context can be handled (though in practice, generalization performance may be limited beyond the context length used during training).</p></li>
</ul>
<p>RWKV’s performance has shown to be <strong>comparable to Transformer</strong>. Community-driven open RWKV models have been developed up to <strong>1.4 billion to 14 billion parameter scale</strong>, showing <strong>similar language modeling capabilities</strong> to GPT-series Transformers of the same parameter scale. Particularly, the RWKV-14B model reportedly showed performance improvement following <strong>scaling laws similar to 14B parameter GPT</strong> and research is ongoing to scale to large scale (e.g., 175B).</p>
<p><strong>Message (information) processing efficiency</strong> is also RWKV’s strength. Each layer has its own hidden state and updates it token by token, maintaining accumulation of past keys/values through <strong>Time-mix</strong> structure using <strong>exponential moving average (EMA)</strong>, and applying <strong>non-linear transformations</strong> similar to FFN through <strong>Channel-mix</strong>. This composition allows <strong>influence of previous tokens to propagate far</strong> (long dependency processing) while naturally <strong>decaying</strong> without unnecessarily persisting for too long. Also, unlike attention, RWKV’s <strong>computation per token remains constant</strong> even as token count increases, so <strong>processing speed degradation is gradual</strong> even as context length increases.</p>
<p><strong>Note:</strong> In RWKV’s case, <strong>during training</strong> it operates internally in attention form, so it uses <span class="math notranslate nohighlight">\(O(n^2)\)</span> operations but this is offset by GPU parallelization. <strong>During inference</strong> it actually processes one token at a time like RNN with <span class="math notranslate nohighlight">\(O(n)\)</span>. Therefore, when handling very long context, it has <strong>great advantages in memory and time</strong> compared to Transformer.</p>
<section id="rwkv-model-usage-example-code">
<h3>RWKV Model Usage Example Code<a class="headerlink" href="#rwkv-model-usage-example-code" title="Link to this heading">#</a></h3>
<p>RWKV is integrated into the Hugging Face transformers library, so it can be used like existing GPT models. For example, let’s load the publicly available <strong>RWKV-4 169M</strong> model and generate text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># 1. Load tokenizer and model</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;RWKV/rwkv-4-169m-pile&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;RWKV/rwkv-4-169m-pile&quot;</span><span class="p">)</span>

<span class="c1"># 2. Define input prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Once upon a time, &quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="c1"># 3. Generate text</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code generates text starting with the prompt “Once upon a time, “ using the RWKV model. RWKV series are memory efficient for large models and can be moved to GPU with <a class="reference external" href="http://model.to">model.to</a>(‘cuda’) if needed. RWKV is an autoregressive LM, so the generate function usage is identical to GPT. However, you can confirm that memory usage is low and speed is fast compared to Transformer when processing very long prompts.</p>
<p><strong>Note:</strong> RWKV is an <strong>open-source LLM developed community-centered</strong>, actively developed on Discord, etc. Currently RWKV-14B is publicly available, and lightweight implementations like rwkv.cpp for fast inference have also emerged.</p>
</section>
<section id="id2">
<h3>Checkpoint Questions<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Explain what <strong>limitations of Transformer</strong> RWKV architecture was designed to solve. Also, how did it combine the <strong>advantages of Transformer and RNN</strong> respectively?</p></li>
<li><p>How does <strong>RWKV’s inference method</strong> differ from Transformer, and what benefits does this provide (hint: KV cache vs hidden state)?</p></li>
<li><p>What does RWKV’s name mean, and briefly summarize what Time-mix and Channel-mix do.</p></li>
</ul>
</section>
</section>
<section id="jamba-architecture-moe-based-transformer-mamba-hybrid">
<h2>4. Jamba Architecture – MoE-based Transformer+Mamba Hybrid<a class="headerlink" href="#jamba-architecture-moe-based-transformer-mamba-hybrid" title="Link to this heading">#</a></h2>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h3>
<p><img alt="Jamba Introduction" src="../_images/jamba-1.jpeg" /></p>
<p><strong>Jamba</strong> is short for <strong>Joint Attention + Mamba</strong>, a <strong>hybrid architecture</strong> that combines <strong>Transformer</strong> and <strong>Mamba</strong> and applies <strong>MoE (Mixture-of-Experts)</strong> technology. This model was announced by AI21 Labs in 2024 and is called the <strong>world’s first commercial-level hybrid SSM-Transformer model</strong>. Jamba is the first production-grade model based on Mamba, a new SSM architecture. This model emerged from attempts to overcome the limitations of the Transformer architecture, but Mamba itself also had limitations. Jamba combines the advantages of both Transformer and SSM, showing superior performance compared to existing models while improving throughput in long contexts by nearly 3x, prioritizing cost efficiency and accessibility.</p>
</section>
<section id="key-features">
<h3>Key Features<a class="headerlink" href="#key-features" title="Link to this heading">#</a></h3>
<p><img alt="Jamba Performance Comparison" src="../_images/jamba-2.jpeg" /></p>
<ul class="simple">
<li><p>First production-level Mamba-based model built on new SSM-Transformer hybrid architecture</p></li>
<li><p>Provides 3x throughput in long contexts compared to Mixtral 8x7B</p></li>
<li><p>Democratizes access to large-scale 256K context windows</p></li>
<li><p>Only model in its class supporting up to 140K context on a single GPU</p></li>
<li><p>Open LLM (OpenLLM) with model weights released under Apache 2.0 license</p></li>
<li><p>Available on Hugging Face and soon to be added to NVIDIA API catalog</p></li>
</ul>
<p>Jamba shows superior or comparable performance compared to other models of similar size. It demonstrates good results in reasoning-related benchmarks.</p>
</section>
<section id="jamba-s-model-structure">
<h3>Jamba’s Model Structure<a class="headerlink" href="#jamba-s-model-structure" title="Link to this heading">#</a></h3>
<p><img alt="Jamba Model Structure" src="../_images/jamba-3.jpeg" /></p>
<p>Jamba is the world’s first production-grade Mamba-based model that combines the advantages of SSM and Transformer architectures. This hybrid structure leverages both the powerful language understanding capabilities of Transformers and the efficient memory management and processing speed of SSMs. As a result, Jamba significantly improves the memory usage increase and processing speed degradation problems that existing language models had.</p>
</section>
<section id="large-scale-context-window-and-cost-efficiency">
<h3>Large-Scale Context Window and Cost-Efficiency<a class="headerlink" href="#large-scale-context-window-and-cost-efficiency" title="Link to this heading">#</a></h3>
<p><img alt="Context Size Comparison" src="../_images/jamba-4.jpeg" /></p>
<p>Jamba provides a 256K context window, enabling efficient processing of very long documents or conversations. This allows AI developers to perform more complex natural language processing tasks and contributes to understanding long contexts that existing models could not handle.</p>
</section>
<section id="moe-mixture-of-experts-utilization">
<h3>MoE (Mixture of Experts) Utilization<a class="headerlink" href="#moe-mixture-of-experts-utilization" title="Link to this heading">#</a></h3>
<p><img alt="Jamba Throughput" src="../_images/jamba-5.jpeg" /></p>
<p>Jamba utilizes only 12B out of 52B available parameters during inference through MoE layers. This makes the model’s activated parameters more efficiently used and shows better performance than Transformer-only models of the same size.</p>
<p>The main characteristics of Jamba architecture are as follows:</p>
<ul class="simple">
<li><p><strong>Alternating block structure</strong>: Jamba stacks layers in a <strong>1:7 ratio</strong> where <strong>1 out of 8 layers is Transformer (Attention)</strong> and the remaining 7 are Mamba. For example, in a 32-layer model, only 4 layers use attention, and the remaining 28 layers are Mamba. The <strong>block structure diagram</strong> is as follows (forming one block with Transformer or Mamba layer + MLP):</p>
<ul>
<li><p>[{Transformer Attention} + MLP] → [{Mamba SSM} + MLP] → [{Mamba SSM} + MLP] → … (pattern where 1 Attention block is followed by 7 Mamba blocks).</p></li>
</ul>
</li>
</ul>
<p>Through this structure, <strong>global content extraction</strong> is handled by occasionally inserted attention layers, and <strong>remaining most interactions</strong> are efficiently processed by Mamba layers. As a result, <strong>overall memory footprint</strong> is greatly reduced by using less KV cache, and <strong>even with long context processing</strong>, sufficient performance is designed to be achieved with only a few attentions.</p>
<ul class="simple">
<li><p><strong>Mixture-of-Experts (MoE) utilization</strong>: Jamba replaced some of Transformer’s MLP parts with <strong>MoE</strong>. Specifically, <strong>one MoE layer is inserted every 2 layers</strong>, each MoE layer has <strong>16 Expert MLPs</strong>, and only the top 2 Experts are activated per token (top-2 gating). This greatly increases <strong>total parameter count (52B)</strong> but limits <strong>actually activated parameters during inference to 12B level</strong>. That is, while increasing <strong>model capacity</strong>, <strong>computation cost is suppressed</strong> (Jamba 7B base model being <em>active 12B / total 52B</em> through MoE is an example).</p></li>
<li><p><strong>Long context and high efficiency</strong>: Jamba supports <strong>256K tokens</strong>, a very long context window. This is among the longest levels among publicly available Transformer-based models, and it’s reportedly possible to process <strong>128K token input with 8-bit compression on a single 80GB GPU</strong>. Equivalent general Transformers (like Mixtral-8×7B) cannot load such long context on single GPU, so <strong>2x+ memory gains</strong> are achieved. Also, <strong>token processing speed (throughput) in long context is very high</strong>, achieving <strong>3x+ faster generation speed</strong> compared to equivalent Transformers with 128K token input. This is because attention operations occur only in some layers when processing long input, so the overall burden is light. Jamba maintains <strong>performance comparable to Mixtral-8x7B (active 39B) or Llama2-70B</strong> while achieving such efficiency improvements.</p></li>
</ul>
<p>In summary, <strong>Jamba is an innovative structure</strong> that <strong>replaces part of Transformer with Mamba</strong> and <strong>increases model capacity with MoE</strong>. This <strong>dramatically improves memory usage and inference speed</strong> to make large-scale LLMs more suitable for actual applications. Jamba released weights as open source (Apache 2.0) upon release, allowing researchers to continue additional tuning and improvements.</p>
<p><strong>Comparison:</strong> While there were attempts at small hybrid models combining Mamba and attention in prior research (e.g., H3, Hyena, etc.), <strong>expanding to tens of billions of parameters and integrating MoE</strong> like Jamba is a first. Also, Jamba is evaluated as the first case showing stable performance at actual productization level.</p>
</section>
<section id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Link to this heading">#</a></h3>
<p>To use the Jamba model, you need Hugging Face’s transformers library. The following is example code for loading the Jamba model using Python and performing simple text generation. Before running this code, you need to install the transformers library along with mamba-ssm and causal-conv1d libraries. This is to use Jamba’s optimized Mamba implementation.</p>
</section>
<section id="jamba-model-utilization-example-code">
<h3>Jamba Model Utilization Example Code<a class="headerlink" href="#jamba-model-utilization-example-code" title="Link to this heading">#</a></h3>
<p>Jamba is a model released by AI21 Labs and checkpoints are also available on Hugging Face Hub. It can be used through the transformers library as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># Load Jamba v0.1 model (HF model card: ai21labs/Jamba-v0.1)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ai21labs/Jamba-v0.1&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ai21labs/Jamba-v0.1&quot;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;New language model architectures in the AI era&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>In the above code, the trust_remote_code=True option may be needed to load Jamba’s custom model structure (trusting and loading the model definition provided by AI21). To use Jamba model on GPU, you need to move it with .to(‘cuda’) call, and since it includes all 52B parameters even though it’s 7B-based, memory requirements should be considered during loading.</p>
<p>Jamba’s context window is set to 256K by default, but you can check the currently supported context through model.config.max_position_embeddings, etc. Even with long context usage, Jamba can <strong>fast inference with less memory</strong>, so it’s suitable for experiments like question-answering or summarization of very long documents of 100K+ tokens.</p>
</section>
<section id="id3">
<h3>Checkpoint Questions<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What ratio are <strong>Transformer layers and Mamba layers</strong> arranged in Jamba architecture? Explain what advantages this design provides in terms of <strong>memory and speed</strong>.</p></li>
<li><p>Why did Jamba introduce MoE? Explain using the concepts of <em>active parameters</em> and <em>total parameters</em>.</p></li>
<li><p>What is the maximum context length that Jamba model supports, and what practical meaning does this have (e.g., application cases)?</p></li>
</ul>
</section>
</section>
<section id="performance-comparison-by-architecture">
<h2>5. Performance Comparison by Architecture<a class="headerlink" href="#performance-comparison-by-architecture" title="Link to this heading">#</a></h2>
<p>Comparing the characteristics and performance of <strong>Transformer, Mamba, RWKV, Jamba</strong> examined earlier by major indicators:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Architecture</p></th>
<th class="head text-left"><p>Supported Context Length</p></th>
<th class="head text-left"><p>Time Complexity (During Inference)</p></th>
<th class="head text-left"><p>Inference Speed (Throughput)</p></th>
<th class="head text-left"><p>Parameter Efficiency</p></th>
<th class="head text-left"><p>Memory Usage Characteristics</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Transformer (Existing)</strong></p></td>
<td class="text-left"><p>Usually 2K~4K (Extended up to 32K+)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n^2)\)</span> (Computing all token pairs)<br/><em>(O(n) per new token generation)</em></p></td>
<td class="text-left"><p>Baseline 1× (vs same size)</p></td>
<td class="text-left"><p>- (Performance ~ proportional to parameter count)</p></td>
<td class="text-left"><p>KV cache memory O(<em>n</em>) (proportional to context length)<br/><em>GPU memory limits with long context</em></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Mamba (SSM)</strong></p></td>
<td class="text-left"><p>Theoretically unlimited (1M tokens in experiments)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n)\)</span> (Linear time)</p></td>
<td class="text-left"><p>~5× faster than Transformer</p></td>
<td class="text-left"><p>High: <em>3B performance of 6B Transformer</em></p></td>
<td class="text-left"><p><strong>Maintains only state</strong>, memory O(1) per token (little effect from token length)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>RWKV (RNN)</strong></p></td>
<td class="text-left"><p>Practically very long (within training limits)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n)\)</span> (Linear)</p></td>
<td class="text-left"><p>Faster than Transformer (similar to SSM)</p></td>
<td class="text-left"><p>High: <em>14B performance of GPT 13B level</em></p></td>
<td class="text-left"><p><strong>Maintains only hidden state</strong>, very memory efficient</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Jamba (Hybrid)</strong></p></td>
<td class="text-left"><p>Up to 256K</p></td>
<td class="text-left"><p>Mixed: Some <span class="math notranslate nohighlight">\(O(n^2)\)</span> (4 layers) + Majority <span class="math notranslate nohighlight">\(O(n)\)</span></p></td>
<td class="text-left"><p>~3× faster in long context (vs Mixtral)</p></td>
<td class="text-left"><p>High: <em>Active 12B / Total 52B</em></p></td>
<td class="text-left"><p>Uses only part of KV cache -&gt; <strong>Memory savings</strong><br/><em>128K context loadable on single 80GB GPU</em></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Comparison Explanation:</strong> Transformer is advantageous for <strong>parallel learning</strong> but <strong>inference time increases linearly</strong> and memory usage increases as context length increases. Mamba and RWKV use <strong>sequential processing</strong> but are implemented with <strong>selective state space, RNN formulas</strong> respectively, so <strong>inference complexity is linear</strong>, and they <strong>summarize context</strong> to carry around, so they show <strong>consistent speed</strong> even with long input. Particularly, Mamba shows <strong>5x+ token processing speed</strong> compared to Transformers even in small models due to <strong>hardware-friendly optimization</strong>. RWKV also shows <strong>performance insensitive to context length</strong> similarly, enabling fast/low-memory inference.</p>
<p>Jamba shows <strong>2x+ advantages in memory and 3x+ speed</strong> compared to Transformer under identical conditions due to its <strong>hybrid structure</strong>. This is possible because most layers are Mamba, so <strong>sufficient performance is secured with only some attention layers</strong> while reducing unnecessary computation. MoE application also improves <strong>parameter efficiency</strong>, so when viewed by <strong>active parameters</strong>, it achieves similar performance to Transformer with less computation.</p>
<p>As a result, in applications where <strong>memory constraints or real-time processing</strong> are important (e.g., long input, limited GPU memory environment), alternatives like Mamba, RWKV, Jamba can replace or complement Transformer. On the other hand, <strong>highest precision</strong> cases still favor Transformer series, but this gap is also narrowing rapidly with the success of Jamba, etc.</p>
</section>
<section id="introduction-to-latest-open-source-llms-and-characteristics">
<h2>6. Introduction to Latest Open Source LLMs and Characteristics<a class="headerlink" href="#introduction-to-latest-open-source-llms-and-characteristics" title="Link to this heading">#</a></h2>
<p>Now let’s examine several <strong>latest publicly available large language models (LLMs)</strong> that apply or are inspired by the architectures mentioned earlier. Here we introduce <strong>Llama 3</strong>, <strong>Mixtral 8×7B</strong>, <strong>Qwen2-72B</strong> and briefly explain each model’s structural characteristics and industrial application cases.</p>
<section id="llama-3">
<h3>Llama 3<a class="headerlink" href="#llama-3" title="Link to this heading">#</a></h3>
<p>Meta AI’s <strong>Llama 3</strong> is the latest version of the Llama series released in 2024, consisting of <strong>8B</strong>, <strong>70B</strong>, and ultra-large <strong>405B</strong> parameter models as a <strong>model family</strong>. It was trained on <strong>much more extensive data</strong> compared to Llama2 (especially the 405B model with 15.6 trillion tokens, 50x more data than Llama2-70B), and was designed to handle <strong>long context up to 128K tokens</strong>. For long context support, it adopted a method of <strong>pre-training with 8K context first</strong>, then <strong>gradually increasing context length</strong>, and achieved 128K through 6 stages with additional training of 800 billion tokens. This <strong>context window expansion technique</strong> helps the model perform stable inference even with long input.</p>
<p>Llama 3’s <strong>model structure</strong> itself is basically a <strong>standard Transformer decoder</strong>. Like Llama2, it uses <strong>GPT-style Decoder-Only</strong> structure, with techniques like SwiGLU activation function, RoPE positional embedding applied (Llama2’s improvements are inherited). Performance improvements were mainly obtained from <strong>data quality improvements</strong> and <strong>learning scale increases</strong>, and <strong>multilingual</strong> support and <strong>tool usage capabilities</strong> were strengthened through Llama 3.1 (improved version). Particularly, the 405B model is the <strong>largest open LLM</strong> among publicly available ones, with <strong>massive resources</strong> invested, being trained for 54 days using hundreds of GPUs.</p>
<p>Llama 3 is being utilized in various fields after release. For example, it’s being used to replace Llama2 in <strong>customer service chatbots</strong>, <strong>professional knowledge question answering</strong>, etc., and the 70B model is used for language understanding/generation tasks with commercial-level performance. <strong>Tokenizer</strong> uses SentencePiece BPE and is compatible with Llama2, making it good for continuing to utilize existing model assets (prompts, tokenizers). It’s also publicly available on Hugging Face as meta-llama/Meta-Llama-3-8B, etc., making it easy to load and use.</p>
<p><strong>Summary:</strong> Llama 3 is a next-generation LLM that maximizes performance through <strong>large-scale data</strong> and <strong>long context learning</strong>. While there are no revolutionary structural changes, <strong>outstanding performance</strong> obtained through <strong>model size/data scale-up</strong> is its strength, and practical features like multilingual/tool usage have been added.</p>
</section>
<section id="mixtral-87b">
<h3>Mixtral 8×7B<a class="headerlink" href="#mixtral-87b" title="Link to this heading">#</a></h3>
<p><strong>Mixtral 8×7B</strong> is a <strong>Sparse Mixture-of-Experts (SMoE)</strong> model announced by <strong>Mistral AI</strong> in 2024, an expanded version of the <strong>Mistral 7B</strong> model with <strong>8 Experts for each MLP layer</strong>. The meaning of the name can be understood as a model that <em>“mixes 8 experts of 7B each”</em>. Mixtral-8×7B’s <strong>total parameter count increases to about 46.7B</strong> (about 6.7x compared to 7B model), but <strong>only about 39B parameters are activated per token during inference</strong>. In other words, since each token uses only the top 2 out of 8 experts (Top-2 gating), <strong>actual computation is maintained at 14B-level model</strong> while <strong>model capacity achieves 46B-level performance</strong>.</p>
<p>As a result of Mixtral, <strong>performance greatly exceeds the existing 7B model</strong>. According to paper reports, it <strong>surpasses Llama2-70B in most benchmarks</strong> and has areas comparable to OpenAI GPT3.5. For example, it recorded excellent scores in knowledge tests not learned like MMLU. This efficiency is thanks to the <strong>“expert specialization”</strong> advantage of MoE, where each expert learns <strong>representations specialized for subtasks</strong> and is mobilized only when needed.</p>
<p><strong>Utilization examples</strong> of Mixtral 8×7B include <strong>open-source chatbots</strong> or <strong>embedding generators</strong>, etc. For example, <strong>FriendliAI</strong> reportedly deployed Mixtral-8×7B to real-time services and achieved <strong>faster response times and higher throughput</strong> on identical hardware. Also, Mixtral appeared in MLPerf Inference benchmarks, proving the efficiency of MoE models.</p>
<p>Mixtral model is publicly available on Hugging Face and can be used as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="c1"># Memory optimization possible with device_map or load_in_4bit options during model loading</span>
</pre></div>
</div>
<p><strong>Note:</strong> Mixtral is not <strong>fully supported in transformers library currently</strong>, so loading like the example above internally loads MoE structure as general Linear. To maximize inference efficiency, it’s recommended to use MoE-optimized engines like <strong>vLLM</strong> or <strong>FlexGen</strong>. Hugging Face model cards also guide memory reduction settings like half-precision, bitsandbytes (4bit quantization), FlashAttention 2, etc.</p>
</section>
<section id="qwen2-72b">
<h3>Qwen2-72B<a class="headerlink" href="#qwen2-72b" title="Link to this heading">#</a></h3>
<p><strong>Qwen2-72B</strong> is a <strong>next-generation large model</strong> released by <strong>Alibaba</strong>, a <strong>72 billion × 10^8^ parameter (about 72.7 billion)</strong> scale LLM focused on multilingual and multimodal support. Qwen series is known as a model with strengths in Chinese/English, but <strong>Qwen2 was trained with data from 27+ additional languages</strong> to secure <strong>global language capabilities</strong>. Also, <strong>programming code</strong> and <strong>mathematical problem solving</strong> capabilities were greatly strengthened, achieving top-level performance in various benchmarks among same-class models.</p>
<p>Qwen2-72B’s architecture is <strong>Transformer Decoder</strong> based, with characteristics including <strong>SwiGLU</strong> activation function, <strong>Attention QKV bias</strong>, and <strong>Grouped Query Attention (GQA)</strong> technique. GQA is a method to speed up large models by <strong>grouping attention heads for computation</strong>, and Qwen2 introduced GQA to all models to <strong>optimize inference memory and speed</strong>. Along with this, techniques like <strong>Embedding Tying</strong> that shares weights between embedding layers and output layers were also used to improve parameter efficiency.</p>
<p><strong>Multimodal support</strong> is Qwen2’s major strength. <strong>Qwen2-VL-72B</strong>, a variant model, is a model that can understand <strong>image-text-video</strong> as input, introducing <strong>Multimodal-ROPE (M-ROPE)</strong>, a <strong>multi-dimensional positional embedding</strong>. M-ROPE processes 1D positional information (text sequence), 2D positional information (image position), 3D positional information (video time frames) as <strong>one integrated positional embedding</strong>, enabling Qwen2-VL to understand long videos (20+ minutes). In fact, Qwen2.5-VL (improved version) showed performance comparable to GPT-4 in <strong>complex video question answering, OCR</strong>, etc., gaining attention.</p>
<p><strong>Performance-wise</strong>, Qwen2-72B belongs to the top tier among publicly available models. According to internal evaluation results, it surpassed previous generation Qwen-14B or competing models in many language understanding benchmarks, and also recorded the highest level among publicly available models with 64.6% pass rate in code generation related HumanEval. Also, it received high scores in non-English evaluations like Chinese, showing high industrial application value.</p>
<p><strong>Utilization examples:</strong> Qwen2-72B is provided as <strong>API through Alibaba Cloud</strong>, etc., and companies are utilizing it multilingually in their search engines, e-commerce Q&amp;A, etc. Weights are publicly available as open source and can be downloaded through HuggingFace Qwen/Qwen2-72B path, and can be loaded and used immediately in Transformers 4.37.0+. However, 72B model is large, so 4-8+ GPUs are needed. Multimodal model Qwen2-VL-72B is provided as a separate checkpoint, and processing logic including VisionEncoder is integrated, so it can be used in <strong>image+text input format</strong> (e.g., processor = QwenImageProcessor(); model = QwenVLModel.from_pretrained(…) form).</p>
<p><strong>Summary:</strong> Qwen2-72B is a model aimed at <strong>enterprise-level multimodal AI</strong>, achieving evaluation as one of the best publicly available models through <strong>wide language support</strong>, <strong>enhanced reasoning performance</strong>, and <strong>structural improvements for multimodal processing</strong> (M-ROPE, GQA, etc.). In industrial settings, it shows application possibilities in various fields like multilingual customer support bots, real-time video analysis, etc.</p>
</section>
</section>
<section id="practice-guidelines">
<h2>7. Practice Guidelines<a class="headerlink" href="#practice-guidelines" title="Link to this heading">#</a></h2>
<p>Finally, let’s present a <strong>practice guide</strong> to directly handle the concepts and models introduced above. Follow the steps below to set up the environment and perform <strong>speed comparison experiments</strong> between Transformer and Mamba models.</p>
<ol class="arabic">
<li><p><strong>Conda Virtual Environment Setup:</strong>
Create a new Python virtual environment using Anaconda or Miniconda for practice. Execute the following in terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>llm_env<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="w"> </span>-y
conda<span class="w"> </span>activate<span class="w"> </span>llm_env
</pre></div>
</div>
<p>This creates and activates an llm_env environment based on Python 3.10.</p>
</li>
<li><p><strong>PyTorch and Hugging Face Transformers Installation:</strong>
Install PyTorch framework and transformers library. If you have CUDA-supported GPU, select PyTorch version matching your version (here CPU version installation as example):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w">    </span><span class="c1"># PyTorch installation</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="o">[</span>torch<span class="o">]</span><span class="w">             </span><span class="c1"># Hugging Face Transformers installation</span>
</pre></div>
</div>
<p>Pay attention to versions during installation. For example, if CUDA 11+ is installed, you can use commands like pip install torch==2.0.1+cu118. After installation, check if it’s installed properly with python -c “import torch; import transformers; print(‘OK’)”, etc.</p>
</li>
<li><p><strong>Mamba Installation and Usage:</strong>
Install <strong>mamba-ssm</strong> package for Mamba architecture experiments. This package includes optional GPU computation optimization, so Linux/NVIDIA environment is required. Installation command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mamba-ssm<span class="o">[</span>causal-conv1d<span class="o">]</span><span class="w">  </span><span class="c1"># (optionally including conv1d acceleration)</span>
</pre></div>
</div>
<p>After installation, let’s run a simple example to execute Mamba block:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mamba_ssm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mamba</span>
<span class="n">mamba_block</span> <span class="o">=</span> <span class="n">Mamba</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">d_state</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_conv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># 32-dimensional model</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># arbitrary input with sequence length 10</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">mamba_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># output (1, 10, 32)</span>
</pre></div>
</div>
<p>This code can simply check if Mamba block works properly.</p>
</li>
<li><p><strong>Transformer vs Mamba Inference Time Comparison Experiment:</strong>
Let’s compare the speed at which Transformer model and Mamba model generate responses for identical input. Here we simply compare performance between GPT-2 (Transformer, 150M parameter level) and the small Mamba block created earlier. Actually, models of equivalent parameter scale are needed, but we demonstrate with a small example for proof of concept. Execute the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mamba_ssm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mamba</span>

<span class="c1"># 1. Set prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;What is artificial intelligence?&quot;</span>

<span class="c1"># 2. Load Transformer model (GPT-2 small)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">transformer_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># 3. Tokenize prompt and inference (Transformer)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output_ids</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">transformer_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformer output:&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformer inference time: </span><span class="si">{</span><span class="n">transformer_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>

<span class="c1"># 4. Create Mamba model (with same hidden size)</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>  <span class="c1"># GPT2 hidden size (768)</span>
<span class="n">mamba_model</span> <span class="o">=</span> <span class="n">Mamba</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_state</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_conv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Mamba is not a trained language model, so it&#39;s in arbitrary weight state. Here we only perform speed comparison.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># arbitrary tensor corresponding to 50 tokens generated by GPT2</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">mamba_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># process 50 tokens with Mamba block</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">mamba_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mamba 50 token processing time: </span><span class="si">{</span><span class="n">mamba_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>

<span class="c1"># 5. Compare results</span>
<span class="n">speedup</span> <span class="o">=</span> <span class="n">transformer_time</span> <span class="o">/</span> <span class="n">mamba_time</span> <span class="k">if</span> <span class="n">mamba_time</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference speed comparison: Mamba block is about </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x faster than Transformer.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This code measures ① time to generate 50 new tokens with GPT-2 and ② time for Mamba block to process 50 tokens sequentially. Finally, it compares the two values and outputs <strong>how much faster Mamba’s inference speed is</strong>. In example environments, Mamba block is a lightweight model, so it will appear several times faster than Transformer.</p>
<p><strong>Caution:</strong> Here we’re not using Mamba as a <strong>trained LM</strong>, so actual generated text comparison is not possible. Instead, we focus on <strong>computation comparison</strong> for tokens of the same length. For more accurate comparison, you need to compare <strong>language models pre-trained with Mamba</strong> (Mamba-3B, etc.) with Transformers of the same scale. If you have secured such models, you can experiment by calling generate with identical prompts and measuring time with time.time().</p>
</li>
<li><p><strong>Application and Extension</strong>:</p>
<ul class="simple">
<li><p><strong>Test Multiple Models</strong>: You can also load and practice generate with models like Llama3, Mixtral, Qwen2 covered above through Hugging Face paths or official distribution paths. For example, for Llama3-70B model, you can load meta-llama/Meta-Llama-3-70B with AutoModelForCausalLM (note that large models require attention to hardware requirements).</p></li>
<li><p><strong>Long Context Experiments</strong>: If you want to test the long context capabilities of models like Jamba or Mamba, you can observe whether normal output comes out and what memory usage is like by giving arbitrary long dummy text (e.g., 100K tokens) as prompt.</p></li>
<li><p><strong>Profiling</strong>: Using Python’s profile module or PyTorch’s profiler, you can measure computation time per layer for each model. This enables deeper analysis of where Mamba saves time and which operations take much time in Transformer.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<section id="major-papers-and-research-materials">
<h3>Major Papers and Research Materials<a class="headerlink" href="#major-papers-and-research-materials" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Vaswani, A., et al. (2017). “Attention is all you need.” Advances in neural information processing systems.</p></li>
<li><p>Gu, A., &amp; Dao, T. (2023). “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” arXiv preprint.</p></li>
<li><p>Peng, B., et al. (2023). “RWKV: Reinventing RNNs for the Transformer Era.” arXiv preprint.</p></li>
<li><p>Lieber, O., et al. (2024). “Jamba: A Hybrid Transformer-Mamba Language Model.” arXiv preprint.</p></li>
<li><p>Meta AI (2024). “Llama 3: Technical Report.” Meta Research.</p></li>
</ul>
</section>
<section id="technical-documents-and-implementations">
<h3>Technical Documents and Implementations<a class="headerlink" href="#technical-documents-and-implementations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hugging Face Transformers Documentation: <a class="reference external" href="https://huggingface.co/docs/transformers">https://huggingface.co/docs/transformers</a></p></li>
<li><p>Mamba GitHub Repository: <a class="github reference external" href="https://github.com/state-spaces/mamba">state-spaces/mamba</a></p></li>
<li><p>RWKV Wiki: <a class="reference external" href="https://wiki.rwkv.com">https://wiki.rwkv.com</a></p></li>
<li><p>AI21 Labs Jamba Documentation</p></li>
<li><p>Mistral AI Mixtral Technical Documentation</p></li>
<li><p>Alibaba Qwen2 Model Card and Documentation</p></li>
</ul>
</section>
<section id="online-resources-and-blogs">
<h3>Online Resources and Blogs<a class="headerlink" href="#online-resources-and-blogs" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“A Visual Guide to Mamba and State Space Models” - Newsletter by Maarten Grootendorst</p></li>
<li><p>“The RWKV language model: An RNN with the advantages of a transformer” - The Good Minima</p></li>
<li><p>“Mamba Explained” - The Gradient</p></li>
<li><p>“Introducing RWKV - An RNN with the advantages of a transformer” - Hugging Face Blog</p></li>
<li><p>“Introducing Jamba: AI21’s Groundbreaking SSM-Transformer Model” - AI21 Blog</p></li>
<li><p>“Takeaways From the Llama 3 Release Paper” - Medium/ailia-ai</p></li>
<li><p>“Qwen2 — Alibaba’s New Powerhouse Multimodal AI” - Research Graph Hub</p></li>
</ul>
</section>
<section id="benchmarks-and-evaluation-materials">
<h3>Benchmarks and Evaluation Materials<a class="headerlink" href="#benchmarks-and-evaluation-materials" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>MLCommons MLPerf Inference Benchmark Results</p></li>
<li><p>“Serving Performances of Mixtral 8x7B” - FriendliAI Blog</p></li>
<li><p>OpenAI Model Comparison Studies</p></li>
<li><p>Various model cards on Hugging Face Model Hub</p></li>
</ul>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week01"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Deep Learning for Natural Language Processing (131307379A)</p>
      </div>
    </a>
    <a class="right-next"
       href="qna.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-structure-of-transformer-architecture">1. Basic Structure of Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-operation-example-code">Self-Attention Operation Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-architecture-selective-state-space-model">2. Mamba Architecture – Selective State Space Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-structure-and-usage-example-code">Mamba Structure and Usage Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv-architecture-efficient-processing-with-rnn-like-structure">3. RWKV Architecture – Efficient Processing with RNN-like Structure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv-model-usage-example-code">RWKV Model Usage Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-architecture-moe-based-transformer-mamba-hybrid">4. Jamba Architecture – MoE-based Transformer+Mamba Hybrid</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-s-model-structure">Jamba’s Model Structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#large-scale-context-window-and-cost-efficiency">Large-Scale Context Window and Cost-Efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe-mixture-of-experts-utilization">MoE (Mixture of Experts) Utilization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#usage">Usage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-model-utilization-example-code">Jamba Model Utilization Example Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison-by-architecture">5. Performance Comparison by Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-latest-open-source-llms-and-characteristics">6. Introduction to Latest Open Source LLMs and Characteristics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">Llama 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixtral-87b">Mixtral 8×7B</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qwen2-72b">Qwen2-72B</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-guidelines">7. Practice Guidelines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#major-papers-and-research-materials">Major Papers and Research Materials</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-documents-and-implementations">Technical Documents and Implementations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-resources-and-blogs">Online Resources and Blogs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarks-and-evaluation-materials">Benchmarks and Evaluation Materials</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
