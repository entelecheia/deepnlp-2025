
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 9: Advanced RAG Architectures &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week09/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 10: Revolutionary Alignment Techniques" href="../week10/index.html" />
    <link rel="prev" title="Week 8: Core Review and Latest Trends" href="../week08/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Deep Learning for Natural Language Processing (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer and Next-Generation Architectures</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x and Latest Deep Learning Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: Advanced Prompting Techniques and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM Evaluation Paradigms and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: Advances in Multimodal NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: Ultra-Long Context Processing and Efficient Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: Core Review and Latest Trends</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 9: Advanced RAG Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week10/index.html">Week 10: Revolutionary Alignment Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workshops/index.html">LLM From Scratch Workshop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workshops/week01.html">Week 1 Workshop: LLM Overview and Development Environment Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">Team Project Guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/en/week09/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek09/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week09/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 9: Advanced RAG Architectures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-need-for-rag-evolution-long-term-memory-and-multi-context-integration">1. The Need for RAG Evolution: Long-term Memory and Multi-context Integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hipporag-biologically-inspired-long-term-memory-architecture">2. HippoRAG: Biologically Inspired Long-term Memory Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag-knowledge-graph-based-retrieval-augmented-generation">3. GraphRAG: Knowledge Graph-based Retrieval-Augmented Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-search-combination-of-keywords-and-embeddings">4. Hybrid Search: Combination of Keywords and Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag-implementation-practice-using-langchain">5. GraphRAG Implementation Practice using LangChain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-rag-application-cases-and-conclusion">6. Advanced RAG Application Cases and Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-9-advanced-rag-architectures">
<h1>Week 9: Advanced RAG Architectures<a class="headerlink" href="#week-9-advanced-rag-architectures" title="Link to this heading">#</a></h1>
<p>This week covers <strong>advanced architectures</strong> for <strong>Retrieval-Augmented Generation</strong> (RAG) techniques. The <strong>vector RAG</strong> approach we’ve learned so far embeds documents as vectors and retrieves similar vectors for queries to pass to LLMs. While this is a powerful method for overcoming long context limitations and utilizing <strong>external knowledge bases</strong>, it still shows limitations in <strong>multi-document knowledge integration</strong> and <strong>relational reasoning</strong>. This lecture explores <strong>next-generation RAG architectures</strong> that have emerged to overcome these limitations, learning their <strong>structural characteristics</strong> and <strong>practical implementation</strong> methods. We focus particularly on <strong>HippoRAG</strong>, <strong>GraphRAG</strong>, and <strong>hybrid search</strong> techniques, naturally connecting them with previously learned concepts like <strong>LangChain</strong> utilization, <strong>vector databases</strong>, <strong>FlashAttention</strong>, and <strong>PEFT</strong>.</p>
<section id="the-need-for-rag-evolution-long-term-memory-and-multi-context-integration">
<h2>1. The Need for RAG Evolution: Long-term Memory and Multi-context Integration<a class="headerlink" href="#the-need-for-rag-evolution-long-term-memory-and-multi-context-integration" title="Link to this heading">#</a></h2>
<p>Traditional RAG systems (vector-based) receive user questions, search for relevant documents using <strong>embedding similarity</strong>, and provide the found documents as context to LLMs for answer generation. While this <strong>vector RAG</strong> approach enabled LLMs to dynamically utilize <strong>external knowledge</strong> not learned during pre-training, it revealed the following limitations:</p>
<ul class="simple">
<li><p><strong>Difficulty in Multi-hop Reasoning</strong>: When information needed for answers is <strong>distributed across multiple documents</strong>, it’s difficult to find all relevant pieces through vector search alone. Previous solutions like <strong>Iterative RAG</strong> (e.g., IRCoT) repeat query-generation multiple times, but this is inefficient and still causes omissions.</p></li>
<li><p><strong>Lack of Relational Context</strong>: Vector embeddings capture semantic similarity well but cannot express <strong>explicit relationships</strong> between documents (e.g., A is part of B, cause-effect relationships). Therefore, it’s difficult to handle cases requiring <strong>knowledge graphs</strong> that express relationships or <strong>keyword-based exact matching</strong>.</p></li>
<li><p><strong>Absence of Long-term Memory Management</strong>: Current RAG systems infinitely accumulate new documents in <strong>existing vector databases</strong>, leading to problems of <strong>unnecessary information accumulation</strong> or <strong>noise increase</strong> over time. Unlike human memory, they lack functions for forgetting old information or consolidation.</p></li>
</ul>
<p>For these reasons, attempts have been made to develop RAG closer to <strong>human brain’s long-term memory systems</strong>. That is, <strong>evolved RAG</strong> architectures with functions like <strong>structured indexing</strong> (e.g., knowledge graphs), <strong>intentional forgetting</strong> (removing old information), and <strong>knowledge integration</strong> (summarization and relationship extraction) have emerged. The techniques we’ll cover in these sections—HippoRAG, GraphRAG, hybrid search—were developed in this context. These advanced RAG structures can work complementarily with efficient Transformer techniques like <strong>FlashAttention</strong> (long context processing) or <strong>PEFT</strong>-based fine-tuning (knowledge injection)—for example, using <strong>RAG</strong> to bring external knowledge while <strong>FlashAttention</strong> efficiently processes long contexts, or <strong>PEFT</strong> fine-tuning LLMs to better utilize RAG-integrated knowledge.</p>
<section id="checkpoint-questions">
<h3>Checkpoint Questions<a class="headerlink" href="#checkpoint-questions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Why do basic vector RAG systems have limitations in <strong>knowledge integration across multiple documents</strong> (multi-hop queries)? What approaches have been attempted to supplement these limitations?</p></li>
<li><p>Compared to human brain’s long-term memory functions, what <strong>memory management functions</strong> are lacking in existing RAG structures? (e.g., forgetting, consolidation)</p></li>
<li><p>How can techniques like FlashAttention or PEFT be combined with RAG? What <strong>benefits</strong> can be gained from such combinations?</p></li>
</ul>
</section>
</section>
<section id="hipporag-biologically-inspired-long-term-memory-architecture">
<h2>2. HippoRAG: Biologically Inspired Long-term Memory Architecture<a class="headerlink" href="#hipporag-biologically-inspired-long-term-memory-architecture" title="Link to this heading">#</a></h2>
<p><strong>HippoRAG</strong> is an advanced RAG framework inspired by <strong>human hippocampus</strong> memory formation theory. This approach was presented at <strong>NeurIPS 2024</strong> and focuses on improving LLMs’ long-term knowledge integration capabilities by mimicking <strong>hippocampus-neocortex interactions</strong>. To understand HippoRAG’s operation, let’s first briefly examine the <strong>hippocampus-based memory indexing theory</strong> that inspired it:</p>
<p><img alt="Hippocampus-based Memory Indexing Theory" src="../_images/hippocampus-memory-theory.jpeg" />
<em>Figure 1: Correspondence between human hippocampus memory formation theory and HippoRAG architecture</em></p>
<ul class="simple">
<li><p><strong>Neocortex</strong>: Stores sensory experiences as <strong>high-dimensional representations</strong> through abstraction—in human brains, actual memory content is distributed and stored across multiple cortical areas.</p></li>
<li><p><strong>Hippocampus</strong>: Acts as <strong>memory index</strong>—separates individual experiences into unique patterns (<strong>pattern separation</strong>) and restores entire memories from partial cues (<strong>pattern completion</strong>). The hippocampus updates its own index when new experiences arrive and enables <strong>continuous learning</strong> by not overwriting existing neocortical memories.</p></li>
<li><p><strong>Para-hippocampal region</strong> (PHR): Connects neocortex and hippocampus—forms <strong>similarity links</strong> when connecting similar concepts to send to hippocampus.</p></li>
</ul>
<p>HippoRAG applies this theory to RAG. HippoRAG operates in two stages: <strong>offline indexing</strong> and <strong>online search</strong>:</p>
<p><img alt="HippoRAG Overall Architecture" src="../_images/hipporag-architecture.jpeg" />
<em>Figure 2: HippoRAG’s overall architecture - offline indexing and online search stages</em></p>
<p><strong>(1) Offline Indexing – Building Artificial Hippocampus Index:</strong> This process converts original documents into <strong>knowledge graph</strong> form to create a kind of <strong>“hippocampus index”</strong> in advance. The specific procedure is as follows:</p>
<ul class="simple">
<li><p><strong>LLM-based OpenIE</strong>: Uses large-scale LLMs (e.g., GPT-4) to extract <strong>key facts</strong> (triples) from each document. For example, from the sentence “Professor Thomas researches Alzheimer’s at Stanford,” it extracts triples like (Thomas, researches, Alzheimer’s), (Stanford, employs, Thomas). This corresponds to the process where human neocortex processes experiences and decomposes them into elements. Through this <strong>Open Information Extraction</strong>, document content is stored as <strong>structured information pieces</strong> rather than dense vectors, so each fact is <strong>separately stored (pattern separation)</strong> without mixing with others.</p></li>
<li><p><strong>Knowledge Graph Construction</strong>: Forms a <strong>schema-free knowledge graph</strong> using <strong>subjects/objects</strong> of all extracted triples as nodes and <strong>relationships</strong> as edges. This graph becomes a <strong>network-form index</strong> that integrates knowledge from all documents.</p></li>
<li><p><strong>Synopsis Link Addition</strong> (PHR role): Corresponds to the PHR part, adding additional edges that connect <strong>synonyms or similar concepts</strong>. For this, <strong>embedding similarity</strong> is calculated for each node (concept), and <strong>synonym edges</strong> are added between node pairs with cosine similarity above threshold. For example, connections are formed between “Alzheimer’s” and “Alzheimer’s disease” nodes so that when one is activated during search, the other is also activated. This is similar to PHR’s function of linking similar memories before sending to hippocampus.</p></li>
<li><p><strong>Node-Document Mapping Storage</strong>: Stores a mapping table recording which original text paragraphs each node (concept) in the graph appeared in. This is information needed to restore search results back to actual original text paragraphs later.</p></li>
</ul>
<p>The constructed <strong>knowledge graph index</strong> corresponds to the <strong>associative memory network</strong> built by human brain’s hippocampus. This index can be more <strong>memory efficient</strong> than traditional vector databases—instead of storing entire documents as vectors, it stores only <strong>key concept nodes</strong>, significantly saving storage space (research reports about <strong>25%</strong> storage space reduction). Also, since relationships between concepts are explicitly connected, it shows strengths in <strong>connection-based reasoning</strong>.</p>
<p><img alt="Knowledge Graph Index Construction" src="../_images/knowledge-graph-index.jpeg" />
<em>Figure 3: Process of building knowledge graph index from documents</em></p>
<p><strong>(2) Online Search – Memory Restoration through Hippocampus Index:</strong> When a user query arrives, HippoRAG finds relevant information by mimicking the process of <strong>restoring entire memories from partial cues</strong>. The steps are as follows:</p>
<ul class="simple">
<li><p><strong>Query Entity Extraction</strong> (Neocortex role): Calls LLM once to extract <strong>important entity names</strong> or <strong>keywords</strong> appearing in user questions. For example, if the question is “Who is the Stanford professor researching Alzheimer’s?”, extraction results in a set like <strong>{Stanford, Alzheimer’s}</strong>.</p></li>
<li><p><strong>Graph Seed Node Selection</strong>: Each extracted query entity is <strong>embedded</strong>, and the most similar nodes within the graph are found. In the above example, the node closest to <strong>Stanford</strong> and <strong>Alzheimer’s</strong> would probably be <strong>Thomas Südhof</strong>. These selected nodes act as <strong>starting points</strong> (partial cues) for hippocampus search.</p></li>
<li><p><strong>Personalized PageRank (PPR) based Graph Traversal</strong>: This is HippoRAG’s core step. <strong>Personalized PageRank</strong> algorithm is performed on the graph starting from selected seed nodes. PPR performs probabilistic “diffusion” from seed nodes to adjacent nodes, with high probability concentrated on nodes near seeds. This corresponds to hippocampus activating entire related memories from partial cues (Pattern Completion). PPR results give <strong>weights</strong> to related nodes in the graph, indicating how deeply related each concept is to <strong>current query</strong>.</p></li>
<li><p><strong>Related Original Text Fragment Restoration</strong>: Top nodes obtained from PPR are mapped back to original text paragraphs using <strong>node-document mapping table</strong>. For each paragraph, PPR scores of nodes appearing in it are summed to determine <strong>final ranking</strong>. The most relevant paragraphs are finally passed to LLM for answer generation.</p></li>
</ul>
<p>HippoRAG achieves <strong>multi-hop information connection in a single search</strong> through this process. For example, in the previous Stanford/Alzheimer’s example, traditional vector RAG would need <strong>two-step search</strong>—searching with <strong>“Stanford”</strong> once and then finding <strong>“Thomas Südhof”</strong> information based on results—but HippoRAG finds <strong>“Stanford→Thomas Südhof←Alzheimer’s”</strong> connection <strong>at once</strong> through the graph. According to experimental results, HippoRAG showed up to <strong>20% higher accuracy</strong> than existing SOTA in QA requiring 2-step or more knowledge integration, and achieved <strong>10-20x cheaper</strong> and <strong>6-13x faster</strong> performance compared to iterative search techniques like <strong>IRCoT</strong>.</p>
<p>To use an analogy, if <strong>vector RAG</strong> views each document as an island and selects a few islands similar to questions, <strong>HippoRAG</strong> pre-places bridges connecting documents and finds destination information by <strong>moving along bridges</strong> starting from islands with question cues. Through this <strong>Neuro-symbolic</strong> combination, HippoRAG gives LLMs the ability to reconstruct related entire contexts from partial information, like humans do.</p>
<section id="id1">
<h3>Checkpoint Questions<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In HippoRAG’s offline indexing stage, why is a <strong>knowledge graph</strong> built instead of <strong>vector embeddings</strong>? What benefits does this provide?</p></li>
<li><p>What role does <strong>Personalized PageRank</strong> (PPR) algorithm play in HippoRAG, and how does this improve multi-document reasoning?</p></li>
<li><p>Compared to existing vector RAG or IRCoT, what improvements in <strong>response accuracy</strong> or <strong>speed</strong> have been reported for HippoRAG?</p></li>
</ul>
</section>
</section>
<section id="graphrag-knowledge-graph-based-retrieval-augmented-generation">
<h2>3. GraphRAG: Knowledge Graph-based Retrieval-Augmented Generation<a class="headerlink" href="#graphrag-knowledge-graph-based-retrieval-augmented-generation" title="Link to this heading">#</a></h2>
<p><strong>GraphRAG</strong> is a <strong>graph-enhanced RAG</strong> architecture proposed by Microsoft and others, systematically utilizing <strong>topic-wise associations</strong> of large-scale documents using <strong>knowledge graphs</strong>. Conceptually similar to HippoRAG we examined earlier in explicitly modeling <strong>inter-document relationships</strong>, it differs in implementation and application aspects. GraphRAG’s goal is to <strong>convert unstructured text into structured knowledge forms</strong> to improve <strong>query-answer accuracy</strong> and <strong>interpretability</strong>.</p>
<p><img alt="GraphRAG Pipeline" src="../_images/graphrag-pipeline.jpeg" />
<em>Figure 4: GraphRAG’s overall pipeline structure</em></p>
<p>GraphRAG’s typical <strong>workflow</strong> is summarized as follows:</p>
<ol class="arabic simple">
<li><p><strong>Document → Text Chunk Segmentation</strong>: Large documents are segmented into fixed lengths. (Too long chunks cause information loss, too short cause search inaccuracy and cost problems, so appropriate <strong>chunk size</strong> must be selected.)</p></li>
<li><p><strong>Entity and Relationship Extraction</strong>: Each chunk is processed using LLM to extract <strong>key entities</strong> (noun phrases), <strong>relationships</strong> (verb/preposition phrases), <strong>facts</strong> (claims, numbers), etc. Brief descriptions are attached to each element extracted by LLM to improve <strong>interpretability</strong>. (e.g., when extracting “NeoChip” company entity, generate descriptions like “public company specializing in low-power processors”.)</p></li>
<li><p><strong>Knowledge Graph Construction</strong>: Creates <strong>weighted knowledge graph</strong> using extracted entities as nodes and relationships as edges. If identical relationships appear in multiple documents, edge weights are increased to indicate <strong>duplicate importance</strong>. Also, duplicate entities or facts are <strong>clustered and summarized</strong> to make the graph concise.</p></li>
<li><p><strong>Graph Community Analysis</strong>: <strong>Community detection algorithms</strong> (e.g., Leiden algorithm) are applied to completed graphs to identify <strong>topic clusters</strong> where nodes are densely connected. Through this <strong>hierarchical clustering</strong>, massive knowledge graphs can be divided into <strong>topic-wise subgraphs</strong>, useful for subsequent <strong>summarization</strong> and <strong>search scope reduction</strong>.</p></li>
<li><p><strong>Community-wise Summary Generation</strong>: For each found community (topic), summaries representing their content are generated. <strong>Central nodes</strong> (important entities with high connectivity) in that topic are prioritized to maximize <strong>semantic density</strong>. When graph hierarchy has multiple levels, <strong>recursive summarization</strong> is performed where upper levels include lower summaries, obtaining <strong>multi-resolution</strong> topic summary information.</p></li>
<li><p><strong>Hierarchical Query Answering</strong>: When user questions arrive, <strong>community summaries</strong> matching questions are first selected (mixing multiple topic summaries if needed), input to LLM to generate <strong>first responses</strong> (Map stage). Then <strong>usefulness scores</strong> of each response are evaluated, and top responses are gathered again to generate <strong>final answers</strong> (Reduce stage). This <strong>map-reduce</strong> approach constructs answers hierarchically even for massive document groups, improving <strong>overall accuracy and consistency</strong>.</p></li>
</ol>
<p>Through the above GraphRAG pipeline, <strong>explainable and topic-structured search</strong> becomes possible compared to simple embedding-based search. For example, processing thousands of internal corporate reports with GraphRAG generates <strong>field-wise knowledge graphs + summaries</strong>, so when users ask questions like “summarize AI research achievements in the last 3 years”, fast and accurate answers can be generated using relevant community summaries. Also, <strong>reliability</strong> can be improved by presenting graph paths or summaries as answer evidence.</p>
<p><img alt="Community Analysis and Summarization" src="../_images/community-analysis.jpeg" />
<em>Figure 5: Graph community analysis and hierarchical summary generation process</em></p>
<p>In Microsoft’s published GraphRAG case, they showed examples of performing <strong>query-answering</strong> by structuring Wikipedia novel “Christmas Carol” through GraphRAG pipeline. This demonstrated <strong>higher response accuracy</strong> (up to 99% precision) than traditional RAG and showed that <strong>understandable reasoning processes</strong> (knowledge graph paths, summary content) can be provided to users even in complex large-scale datasets.</p>
<p>GraphRAG is conceptually similar to HippoRAG but in implementation, it’s sometimes provided in <strong>Knowledge Graph + vector DB combination</strong> form through <strong>open source tools</strong> (e.g., Microsoft’s <strong>GraphRAG library</strong>). Also, GraphRAG has high <strong>pre-construction costs</strong> due to many <strong>data engineering aspects</strong> (graph generation, community analysis, etc.), but once constructed graphs are utilized, <strong>query-answering stage costs are reduced</strong>.</p>
<section id="id2">
<h3>Checkpoint Questions<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What is the purpose of introducing <strong>knowledge graphs</strong> and <strong>community detection</strong> in GraphRAG pipeline? Explain how this structuring contributes to query-answering.</p></li>
<li><p>What is GraphRAG’s <strong>map-reduce query-answering strategy</strong>, and why is this approach effective for large-scale document collections?</p></li>
<li><p>Both HippoRAG and GraphRAG use graphs, but what differences exist in <strong>implementation methods</strong> or <strong>application scope</strong>? For example, compare the two techniques in terms of real-time dynamic updates.</p></li>
</ul>
</section>
</section>
<section id="hybrid-search-combination-of-keywords-and-embeddings">
<h2>4. Hybrid Search: Combination of Keywords and Embeddings<a class="headerlink" href="#hybrid-search-combination-of-keywords-and-embeddings" title="Link to this heading">#</a></h2>
<p><strong>Hybrid search</strong> is an approach that combines traditional <strong>sparse</strong> (Lexical) search and <strong>dense</strong> (Vector) search to utilize advantages of both methods. While HippoRAG or GraphRAG we covered earlier focused on <strong>knowledge structuring</strong>, hybrid search is an improvement at the <strong>search algorithm level</strong>, simultaneously utilizing <strong>accurate keyword matching capabilities</strong> and <strong>semantic similarity reasoning capabilities</strong>.</p>
<p><img alt="Hybrid Search Structure" src="../_images/hybrid-search-architecture.jpeg" />
<em>Figure 6: Hybrid search structure and BM25 + vector search combination method</em></p>
<p>Traditionally, <strong>keyword-based ranking</strong> like <strong>BM25</strong> is strong in <strong>exact term matching</strong> between queries and documents, accurately finding typos, abbreviations, names, etc. However, it easily misses cases where expressions differ even with same meaning and doesn’t reflect contextual meaning. Conversely, <strong>embedding-based vector search</strong> captures <strong>semantic similarity</strong> of sentences well, finding related results even with different expressions, but may miss exact keywords (e.g., code snippets, proper nouns, etc.).</p>
<p>Hybrid search <strong>performs these two in parallel</strong> then <strong>integrates</strong> (score fusion) results. For example, in OpenAI Embedding + BM25 combination hybrid search, <strong>BM25 scores</strong> and <strong>cosine similarity scores</strong> are calculated for one query, then final ranking is determined through <strong>weighted summation or Rank-Fusion</strong> techniques. For simple weighted sum:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">final_score</span> <span class="o">=</span> <span class="n">α</span> <span class="o">*</span> <span class="p">(</span><span class="n">BM25_score_normalized</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">α</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Vector_score</span><span class="p">)</span>
</pre></div>
</div>
<p>This can be adjusted. (α is importance ratio of two searches) Or techniques like <strong>Reciprocal Rank Fusion (RRF)</strong> that sum scores based on <strong>ranks</strong> in each result list are widely used. RRF can mix two results without being affected by score distribution differences of each search method, so it’s frequently used.</p>
<p><strong>LangChain</strong> provides EnsembleRetriever etc. to easily implement such hybrid search. The following is example code for configuring <strong>BM25 + vector embedding</strong> hybrid retriever using LangChain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BM25Retriever</span><span class="p">,</span> <span class="n">EnsembleRetriever</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.embeddings.openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>

<span class="c1"># 1. Document list (example sentences)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The capital of France is Paris.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Machine learning enables computers to learn from data.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The Mona Lisa was painted by Leonardo da Vinci.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Python is a popular programming language for AI.&quot;</span>
<span class="p">]</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">]</span>

<span class="c1"># 2. Create BM25-based Retriever</span>
<span class="n">bm25_retriever</span> <span class="o">=</span> <span class="n">BM25Retriever</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">bm25_retriever</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Use top 2 results</span>

<span class="c1"># 3. Create vector-based Retriever (OpenAI embedding + FAISS)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>  <span class="c1"># Embedding model (e.g., text-embedding-ada-002)</span>
<span class="n">vector_store</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">vector_retriever</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>

<span class="c1"># 4. Create Ensemble (hybrid) Retriever (BM25 40%, vector 60% weight)</span>
<span class="n">hybrid_retriever</span> <span class="o">=</span> <span class="n">EnsembleRetriever</span><span class="p">(</span>
    <span class="n">retrievers</span><span class="o">=</span><span class="p">[</span><span class="n">bm25_retriever</span><span class="p">,</span> <span class="n">vector_retriever</span><span class="p">],</span>
    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># 5. Execute example query</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Who painted the famous portrait of a woman?&quot;</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">hybrid_retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Result </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Execution Result:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Result</span> <span class="mi">1</span><span class="p">:</span> <span class="n">The</span> <span class="n">Mona</span> <span class="n">Lisa</span> <span class="n">was</span> <span class="n">painted</span> <span class="n">by</span> <span class="n">Leonardo</span> <span class="n">da</span> <span class="n">Vinci</span><span class="o">.</span>
<span class="n">Result</span> <span class="mi">2</span><span class="p">:</span> <span class="n">The</span> <span class="n">capital</span> <span class="n">of</span> <span class="n">France</span> <span class="ow">is</span> <span class="n">Paris</span><span class="o">.</span>
</pre></div>
</div>
<p>As seen in the above result, for query “Who painted the famous portrait of a woman?”, hybrid search finds <strong>Mona Lisa related sentence</strong> as 1st priority. Pure vector search alone might miss without exact word “Mona Lisa”, and BM25 alone might be inaccurate depending on keyword matching like “portrait”. Hybrid method reflects both <strong>keyword matching</strong> (Mona Lisa→portrait matching) and <strong>semantic similarity</strong> (famous woman portrait→Mona Lisa painting meaning connection), so optimal results are obtained.</p>
<p><img alt="Hybrid Search Comparison" src="../_images/hybrid-search-comparison.jpeg" />
<em>Figure 7: Comparison of vector search, BM25 search, and hybrid search results</em></p>
<p>Hybrid search is particularly useful in domains where <strong>technical terms, code, or proper nouns</strong> are important. For example, in medical documents, items difficult to catch with vectors like <strong>abbreviations</strong> (“BP” vs “Blood Pressure”), or cases where <strong>literal matching is meaningful</strong> like source code queries, BM25 plays a complementary role. As an actual case, Stack Overflow abandoned existing TF-IDF based search and switched to <strong>embedding + keyword</strong> hybrid, greatly improving <strong>search quality</strong> by not missing keyword matching in code-including questions while finding semantically related answers.</p>
<p>Of course, hybrid search also has <strong>considerations</strong>. Since two searches are performed in parallel, <strong>latency</strong> may increase, and <strong>tuning work</strong> is needed for score combination/normalization. However, most vector databases provide hybrid functions (e.g., Elastic, Pinecone, Weaviate, etc.) or API support by default, so implementation difficulty is decreasing.</p>
<p>In summary, hybrid search balances <strong>“finding exactly what’s asked”</strong> and <strong>“understanding intent and finding”</strong>. This maximizes <strong>Retriever</strong> stage performance of RAG systems, improving final answer quality by providing <strong>better context</strong> to LLMs.</p>
<section id="id3">
<h3>Checkpoint Questions<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In hybrid search, what types of information do <strong>BM25 and embedding search</strong> find well respectively? Why do synergistic effects occur when combining the two methods?</p></li>
<li><p>When implementing hybrid search, different search scores must be combined. How do <strong>normalization</strong> or <strong>Rank Fusion</strong> techniques solve this combination problem?</p></li>
<li><p>If you were to build a RAG system, explain with examples in what situations adopting hybrid search would be advantageous over pure vector search.</p></li>
</ul>
</section>
</section>
<section id="graphrag-implementation-practice-using-langchain">
<h2>5. GraphRAG Implementation Practice using LangChain<a class="headerlink" href="#graphrag-implementation-practice-using-langchain" title="Link to this heading">#</a></h2>
<p>The advanced RAG techniques introduced earlier may seem difficult to implement, but fortunately, frameworks like <strong>LangChain</strong> allow relatively easy practice of some functions. Particularly for <strong>GraphRAG</strong>, LangChain has modules supporting <strong>graph query chains</strong> and <strong>graph state machines</strong>. This section introduces methods for implementing <strong>simple GraphRAG</strong> workflows using LangChain.</p>
<p>First, here’s a basic graph query-answering example using LangChain’s <strong>GraphQAChain</strong>. This chain connects <strong>graph databases</strong> with LLMs, performs queries on graphs, and lets LLMs answer in natural language.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_experimental.graph_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMGraphTransformer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">GraphQAChain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.graphs.networkx_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">NetworkxEntityGraph</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># 1. Example text and LLM preparation</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Marie Curie was a physicist. Marie Curie won a Nobel Prize. Marie Curie worked in Paris.&quot;</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;gpt-3.5-turbo&#39;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># OpenAI LLM example</span>

<span class="c1"># 2. Convert text to graph triples using LLM Graph Transformer</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span>
<span class="n">graph_transformer</span> <span class="o">=</span> <span class="n">LLMGraphTransformer</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span>
<span class="n">graph_docs</span> <span class="o">=</span> <span class="n">graph_transformer</span><span class="o">.</span><span class="n">convert_to_graph_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># 3. Create NetworkX graph object and add nodes/edges</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">NetworkxEntityGraph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph_docs</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">graph_docs</span><span class="o">.</span><span class="n">relationships</span><span class="p">:</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">edge</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">edge</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">relation</span><span class="o">=</span><span class="n">edge</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

<span class="c1"># 4. Create GraphQAChain (connect graph and LLM)</span>
<span class="n">graph_qa_chain</span> <span class="o">=</span> <span class="n">GraphQAChain</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 5. Execute query</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;What did Marie Curie win?&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">graph_qa_chain</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code extracts a simple <strong>knowledge graph</strong> from one paragraph using LLM (convert_to_graph_documents), converts it to NetworkX graph, then performs query-answering through <strong>GraphQAChain</strong>. Execution result for example query “What did Marie Curie win?” could be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Marie</span> <span class="n">Curie</span> <span class="n">won</span> <span class="n">a</span> <span class="n">Nobel</span> <span class="n">Prize</span><span class="o">.</span>
</pre></div>
</div>
<p>LangChain’s GraphQAChain internally goes through steps of <strong>query analysis → Cypher (graph query language) generation → graph DB query → result synthesis → LLM answer</strong>. Thanks to this, users can get answers utilizing graph information with <strong>natural language queries only</strong> without directly writing complex Cypher syntax or graph traversal logic. This can be seen as an example where LangChain abstracts GraphRAG’s <strong>online search stage</strong>.</p>
<p>Also, LangChain allows <strong>graph+vector mixed chain</strong> configuration. For example, <strong>GraphCypherQAChain</strong> is a <strong>hybrid chain</strong> that uses <strong>vector embeddings</strong> to convert queries to graph queries, executes Cypher in <strong>Neo4j</strong> etc., then <strong>combines with additional embedding search</strong>. For complex examples, using Microsoft’s LangGraph extension, <strong>branching workflows</strong> can be built as follows:</p>
<p><img alt="LangChain GraphRAG Workflow" src="../_images/langchain-graphrag-workflow.jpeg" />
<em>Figure 8: LangChain-based GraphRAG chain configuration - workflow that branches graph queries and vector search</em></p>
<p>The above figure is a simplified representation of GraphRAG example workflow published by Neo4j. When questions arrive at <strong>START</strong> node, <strong>route_question</strong> function analyzes question content and branches to <strong>right branch for vector search</strong> if needed, or <strong>left branch for simple graph queries</strong>. Right branch first has <strong>Decomposer</strong> split complex questions into two parts (e.g., “find papers” + “get authors of those papers”). Then <strong>VECTOR_SEARCH</strong> node processes first part to find similar documents, and <strong>PROMPT_TEMPLATE_WITH_CONTEXT</strong> node combines results with second graph query part to generate Cypher query. Finally, <strong>GRAPH_QA_WITH_CONTEXT</strong> node finds answers in graph DB and provides final response through LLM. Left branch performs traditional GraphQA process.</p>
<p>The lesson gained from such LangChain-based implementation is that <strong>advanced RAG architectures can also be sufficiently modularized and configured</strong>. That is, <strong>Retrievers</strong> can be configured in multiple stages or <strong>Chains</strong> can be branched/merged to reproduce human-designed logic in HippoRAG or GraphRAG. For example, to implement HippoRAG with LangChain, instead of document embedding, <strong>LLM triple extraction + graph build</strong> stage and <strong>NER + PPR graph traversal</strong> stage during queries can be made into Python functions and combined into Chain. Actually, HippoRAG’s open code also implements these procedures in Python.</p>
<p>In summary, <strong>LangChain</strong> is an <strong>excellent tool</strong> for easily practicing advanced RAG ideas. Using it, learners can directly create <strong>advanced RAG pipelines</strong>, observe how each stage affects results, and gain deep understanding. Furthermore, when building <strong>their own RAG systems</strong> in enterprise environments, LangChain’s modules can be utilized to quickly create <strong>prototypes</strong>.</p>
<section id="id4">
<h3>Checkpoint Questions<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>How does LangChain’s <strong>GraphQAChain</strong> convert natural language questions to graph queries and generate answers? Reason through internal operation step by step.</p></li>
<li><p>In the example workflow above, what criteria does <strong>question branching</strong> (route) logic use to select graph vs vector paths? For example, what factors determine when vector search is needed for incoming questions?</p></li>
<li><p>Assuming you apply HippoRAG/GraphRAG to your domain data, what modules (chains, retrievers, etc.) should be utilized to implement this with LangChain? (e.g., when applying GraphRAG to research paper data)</p></li>
</ul>
</section>
</section>
<section id="advanced-rag-application-cases-and-conclusion">
<h2>6. Advanced RAG Application Cases and Conclusion<a class="headerlink" href="#advanced-rag-application-cases-and-conclusion" title="Link to this heading">#</a></h2>
<p>Finally, we briefly examine <strong>RAG architecture cases in actual large-scale systems</strong> and conclude. Recently in industry, <strong>ultra-large scale RAG</strong> systems are operating that must process <strong>tens of millions of tokens</strong> daily while maintaining response latency within 100ms. For this, the techniques we covered earlier are comprehensively utilized. For example:</p>
<ul class="simple">
<li><p><strong>Large-scale Vector DB + Hybrid Search</strong>: Store hundreds of millions of documents as <strong>vector embeddings</strong>, but apply <strong>BM25</strong> etc. keyword filters in first-stage candidate search to <strong>reduce search space</strong>, then use embedding similarity search + RRF for precise ranking. This catches both <strong>search accuracy</strong> and <strong>latency</strong>.</p></li>
<li><p><strong>FlashAttention-based Long Context Processing</strong>: When putting retrieved document fragments into LLMs for answers, sometimes tens of thousands of tokens are input in one prompt. In such cases, LLM servers with memory optimization techniques like FlashAttention are used to operate stably even in <strong>large-scale contexts</strong>.</p></li>
<li><p><strong>Continuous Learning and Forgetting</strong>: In situations where new documents are added daily, applying HippoRAG approach, <strong>triples of new documents are periodically extracted</strong> to update knowledge graph index, and <strong>memory management policies</strong> are applied that remove old information from graphs or lower weights of edges with decreased importance. This maintains both <strong>update efficiency</strong> and <strong>search quality</strong>.</p></li>
</ul>
<p>The techniques we examined—<strong>HippoRAG</strong>, <strong>GraphRAG</strong>, <strong>Hybrid Search</strong>—serve as core elements of such large-scale systems, each responsible for <strong>long-term memory</strong>, <strong>relational reasoning</strong>, and <strong>search precision</strong>. Learners can gain deep insights by going beyond theoretical understanding of these architectures to <strong>directly implement and tune</strong> them through practice and projects.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>HippoRAG</p></th>
<th class="head"><p>GraphRAG</p></th>
<th class="head"><p>Hybrid Search</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Core Approach</strong></p></td>
<td><p>Biologically-inspired memory indexing using hippocampus theory</p></td>
<td><p>Knowledge graph construction with community detection</p></td>
<td><p>Combination of sparse (BM25) and dense (vector) search</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Data Structure</strong></p></td>
<td><p>Knowledge graph with synopsis links (PHR connections)</p></td>
<td><p>Hierarchical knowledge graph with community clusters</p></td>
<td><p>Vector embeddings + inverted index</p></td>
</tr>
<tr class="row-even"><td><p><strong>Indexing Method</strong></p></td>
<td><p>LLM-based OpenIE → Graph construction → Synopsis linking</p></td>
<td><p>Entity/relationship extraction → Graph building → Community analysis</p></td>
<td><p>Document embedding + keyword indexing</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Search Strategy</strong></p></td>
<td><p>Query entity extraction → PPR graph traversal → Memory restoration</p></td>
<td><p>Community summary selection → Map-reduce answering</p></td>
<td><p>Parallel BM25 + vector search → Score fusion</p></td>
</tr>
<tr class="row-even"><td><p><strong>Memory Management</strong></p></td>
<td><p>Pattern separation and completion (like hippocampus)</p></td>
<td><p>Hierarchical summarization and clustering</p></td>
<td><p>Static index with periodic updates</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multi-hop Reasoning</strong></p></td>
<td><p>✅ Excellent (single-step PPR traversal)</p></td>
<td><p>✅ Good (community-based reasoning)</p></td>
<td><p>❌ Limited (requires multiple queries)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Exact Match Capability</strong></p></td>
<td><p>⚠️ Moderate (depends on entity extraction)</p></td>
<td><p>⚠️ Moderate (depends on graph structure)</p></td>
<td><p>✅ Excellent (BM25 component)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Semantic Understanding</strong></p></td>
<td><p>✅ Good (through synopsis links and PPR)</p></td>
<td><p>✅ Excellent (LLM-based extraction and summarization)</p></td>
<td><p>✅ Good (vector component)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Storage Efficiency</strong></p></td>
<td><p>✅ High (25% reduction reported)</p></td>
<td><p>❌ Low (requires extensive graph storage)</p></td>
<td><p>⚠️ Moderate (dual indexing overhead)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Query Speed</strong></p></td>
<td><p>✅ Fast (6-13x faster than iterative methods)</p></td>
<td><p>⚠️ Moderate (depends on graph size)</p></td>
<td><p>⚠️ Moderate (parallel search overhead)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Setup Complexity</strong></p></td>
<td><p>⚠️ Moderate (requires graph construction)</p></td>
<td><p>❌ High (complex pipeline with community detection)</p></td>
<td><p>✅ Low (straightforward implementation)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Real-time Updates</strong></p></td>
<td><p>⚠️ Moderate (requires graph updates)</p></td>
<td><p>❌ Difficult (expensive re-clustering)</p></td>
<td><p>✅ Easy (independent index updates)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Interpretability</strong></p></td>
<td><p>✅ Good (graph paths and node activations)</p></td>
<td><p>✅ Excellent (community summaries and graph evidence)</p></td>
<td><p>⚠️ Limited (score combination only)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Best Use Cases</strong></p></td>
<td><p>Multi-document reasoning, long-term memory systems, academic research</p></td>
<td><p>Large-scale document analysis, enterprise knowledge bases, topic discovery</p></td>
<td><p>Technical documentation, code search, mixed content types</p></td>
</tr>
<tr class="row-even"><td><p><strong>Typical Domains</strong></p></td>
<td><p>Scientific literature, legal documents, research papers</p></td>
<td><p>Corporate reports, Wikipedia-scale content, news archives</p></td>
<td><p>Stack Overflow, medical records, software documentation</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Integration with LLMs</strong></p></td>
<td><p>Direct context restoration for answer generation</p></td>
<td><p>Hierarchical prompting with community summaries</p></td>
<td><p>Enhanced retrieval for standard RAG pipelines</p></td>
</tr>
<tr class="row-even"><td><p><strong>Scalability</strong></p></td>
<td><p>✅ Good (efficient graph operations)</p></td>
<td><p>⚠️ Moderate (community detection bottleneck)</p></td>
<td><p>✅ Good (parallel processing)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Accuracy Improvement</strong></p></td>
<td><p>20% higher for multi-hop QA</p></td>
<td><p>Up to 99% precision reported</p></td>
<td><p>Significant improvement over single-method approaches</p></td>
</tr>
</tbody>
</table>
</div>
<p>Completing this week, you will have learned <strong>components and operation principles of latest RAG architectures</strong>, which becomes the foundation for upcoming <strong>LangChain-based RAG practice</strong>. For example, in the next step, you will perform tasks of building GraphRAG for actual <strong>internal wiki documents</strong> and <strong>benchmarking and improving</strong> search performance. Through such practice, I hope you experience how concepts learned in this lecture are applied to real systems.</p>
<section id="id5">
<h3>Checkpoint Questions<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In large-scale enterprise RAG systems, what techniques are combined to simultaneously satisfy <strong>response latency</strong> and <strong>search accuracy</strong>? Give specific examples.</p></li>
<li><p>When applying RAG to continuously updated knowledge bases, how can HippoRAG’s ideas be utilized? (e.g., new information integration, old information processing)</p></li>
<li><p>Using advanced RAG concepts learned in this lecture, if you were to design your own RAG pipeline, what structure would you conceive? List main stages and techniques to use.</p></li>
</ul>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Chen, J., Lin, H., Han, X., &amp; Sun, L. (2024). HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. arXiv preprint arXiv:2405.14831.</p></li>
<li><p>Microsoft Research (2024). GraphRAG: Transforming Unstructured Text into Explainable, Queryable Intelligence using Knowledge Graph-Enhanced RAG.</p></li>
<li><p>Sharma, T. (2024). HippoRAG: Redefining AI Retrieval emulating the Hippocampus. Medium.</p></li>
<li><p>Sharma, T. (2024). Microsoft GraphRAG: Transforming Unstructured Text into Explainable, Queryable Intelligence using Knowledge Graph-Enhanced RAG. Medium.</p></li>
<li><p>Superlinked (2024). Optimizing RAG with Hybrid Search &amp; Reranking. VectorHub.</p></li>
<li><p>Neo4j (2024). Create a Neo4j GraphRAG Workflow Using LangChain and LangGraph. Neo4j Developer Blog.</p></li>
<li><p>Nixie Search (2024). Search Overview Documentation. GitHub.</p></li>
<li><p>S, Suruthi (2024). Exploring HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. Medium.</p></li>
<li><p>Shrsv (2024). About HippoRAG. DEV Community.</p></li>
<li><p>VectorHub (2024). Hybrid Search &amp; Rerank RAG Documentation. GitHub.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week08/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 8: Core Review and Latest Trends</p>
      </div>
    </a>
    <a class="right-next"
       href="../week10/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 10: Revolutionary Alignment Techniques</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-need-for-rag-evolution-long-term-memory-and-multi-context-integration">1. The Need for RAG Evolution: Long-term Memory and Multi-context Integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hipporag-biologically-inspired-long-term-memory-architecture">2. HippoRAG: Biologically Inspired Long-term Memory Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag-knowledge-graph-based-retrieval-augmented-generation">3. GraphRAG: Knowledge Graph-based Retrieval-Augmented Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-search-combination-of-keywords-and-embeddings">4. Hybrid Search: Combination of Keywords and Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag-implementation-practice-using-langchain">5. GraphRAG Implementation Practice using LangChain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-rag-application-cases-and-conclusion">6. Advanced RAG Application Cases and Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
