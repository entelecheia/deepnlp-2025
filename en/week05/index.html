
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 5: LLM Evaluation Paradigms and Benchmarks &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week05/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 6: Advances in Multimodal NLP" href="../week06/index.html" />
    <link rel="prev" title="Week 4: Advanced Prompting Techniques and Optimization" href="../week04/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Deep Learning for Natural Language Processing (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer and Next-Generation Architectures</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x and Latest Deep Learning Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: Advanced Prompting Techniques and Optimization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 5: LLM Evaluation Paradigms and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: Advances in Multimodal NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: Ultra-Long Context Processing and Efficient Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: Core Review and Latest Trends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week09/index.html">Week 9: Advanced RAG Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week10/index.html">Week 10: Revolutionary Alignment Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workshops/index.html">LLM From Scratch Workshop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workshops/week01.html">Week 1 Workshop: LLM Overview and Development Environment Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">Team Project Guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/en/week05/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek05/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week05/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 5: LLM Evaluation Paradigms and Benchmarks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-changing-landscape-of-evaluation-limitations-of-traditional-metrics-and-the-need-for-meaning-based-assessment">1. The Changing Landscape of Evaluation: Limitations of Traditional Metrics and the Need for Meaning-Based Assessment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-traditional-evaluation-metrics">1.1 Limitations of Traditional Evaluation Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#emergence-of-meaning-based-evaluation">1.2 Emergence of Meaning-Based Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bertscore-and-sentencemover">BERTScore and SentenceMover</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bleurt">BLEURT</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#emergence-of-llm-as-a-judge-paradigm">1.3 Emergence of LLM-as-a-Judge Paradigm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-based-evaluation-paradigms">2. LLM-Based Evaluation Paradigms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptscore-probability-based-evaluation-framework">2.1 GPTScore: Probability-Based Evaluation Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-principles">Core Principles</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-results">Performance Results</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptscore-implementation-example">2.1.1 GPTScore Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#g-eval-chain-of-thought-cot-based-llm-evaluation">2.2 G-Eval: Chain-of-Thought (CoT) Based LLM Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-features">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-process">Evaluation Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-summarization-consistency-evaluation">Example: Summarization Consistency Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-based-calibration">Probability-Based Calibration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#g-eval-implementation-example">2.2.1 G-Eval Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flask-fine-grained-skill-set-based-evaluation">2.3 FLASK: Fine-grained Skill Set Based Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-grained-ability-indicators">12 Fine-grained Ability Indicators</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Evaluation Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-legal-consultation-response-evaluation">Example: Legal Consultation Response Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flask-implementation-example">2.3.1 FLASK Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specialized-purpose-benchmarks">3. Specialized Purpose Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#livecodebench-contamination-free-code-generation-evaluation">3.1 LiveCodeBench: Contamination-Free Code Generation Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-problem-data-contamination">Core Problem: Data Contamination</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-approach">Solution Approach</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#holistic-evaluation">Holistic Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#significance">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evalplus-test-case-augmentation">3.2 EvalPlus: Test Case Augmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-problem">Core Problem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Solution Approach</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helm-code-transparency-and-community-collaboration">3.3 HELM-Code: Transparency and Community Collaboration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#helm-philosophy">HELM Philosophy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mmlu-pro-10-choice-high-difficulty-knowledge-reasoning-benchmark">3.4 MMLU-Pro: 10-Choice High-Difficulty Knowledge/Reasoning Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-changes">Core Changes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-effect">Chain-of-Thought Effect</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#current-status">Current Status</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpqa-and-bbh-knowledge-reasoning-enhanced-evaluation-sets">3.5 GPQA and BBH: Knowledge/Reasoning Enhanced Evaluation Sets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-examples">Problem Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Significance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bbh-big-bench-hard">BBH (BIG-Bench Hard)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Core Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#goal">Goal</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#research-utilization">Research Utilization</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#extended-version-bbeh">Extended Version: BBEH</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Significance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#domain-specific-benchmarks">4. Domain-Specific Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finben-comprehensive-financial-domain-benchmark">4.1 FinBen: Comprehensive Financial Domain Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-composition">Data Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implications">Implications</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentharm-ai-agent-harmfulness-evaluation-benchmark">4.2 AgentHarm: AI Agent Harmfulness Evaluation Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-composition">Benchmark Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-examples">Scenario Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-method">Evaluation Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-findings">Key Findings</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#research-impact">Research Impact</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lexam-legal-exam-based-llm-evaluation">4.3 LEXam: Legal Exam-Based LLM Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Data Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Evaluation Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-order-reasoning-specific-to-legal-field">High-Order Reasoning Specific to Legal Field</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-as-judge-utilization">LLM-as-Judge Utilization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#csedb-medical-llm-safety-effectiveness-dual-evaluation">4.4 CSEDB: Medical LLM Safety/Effectiveness Dual Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Background</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Benchmark Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-in-high-risk-scenarios">Performance in High-Risk Scenarios</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-domain-specific-tuning">Effect of Domain-Specific Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#utilization-methods">Utilization Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-and-gsm8k-mathematical-ability-evaluation">4.5 MATH and GSM8K: Mathematical Ability Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#math-benchmark">MATH Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#features">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">Performance Results</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gsm8k-benchmark">GSM8K Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-improvement-techniques">Performance Improvement Techniques</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#utilization">Utilization</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id38">Evaluation Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-limitations">LLM Limitations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#improvement-research">Improvement Research</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-bias-and-limitations">5. Evaluation Bias and Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#major-evaluation-biases">5.1 Major Evaluation Biases</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#narcissistic-bias">5.1.1 Narcissistic Bias</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#verbosity-bias">5.1.2 Verbosity Bias</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">Solutions</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inconsistency">5.1.3 Inconsistency</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id44">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id45">Solutions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-limitations">5.2 Evaluation Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-from-human-evaluation">5.2.1 Differences from Human Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lack-of-domain-specific-knowledge">5.2.2 Lack of Domain-Specific Knowledge</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subjectivity-of-evaluation-criteria">5.2.3 Subjectivity of Evaluation Criteria</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id46">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-reinforcement-learning-from-ai-feedback">6. RLAIF: Reinforcement Learning from AI Feedback</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-principles-of-rlaif">6.1 Core Principles of RLAIF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-rlaif">6.2 Advantages of RLAIF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-rlaif">6.3 Limitations of RLAIF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-implementation-example">6.4 RLAIF Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id47">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-evaluation-paradigms">7. Future Evaluation Paradigms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-llm-evaluation">7.1 Multimodal LLM Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-tasks">7.1.1 Evaluation Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-methods">7.1.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-evaluation">7.2 Agent Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id48">7.2.1 Evaluation Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id49">7.2.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#green-ai-evaluation">7.3 Green AI Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics">7.3.1 Evaluation Metrics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">7.3.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-ai-collaboration-evaluation">7.4 Human-AI Collaboration Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id51">7.4.1 Evaluation Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id52">7.4.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hands-on-exercises">8. Hands-on Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bleu-rouge-vs-g-eval-comparison-experiment">8.1 BLEU/ROUGE vs G-Eval Comparison Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-objectives">8.1.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-content">8.1.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-code">8.1.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptscore-implementation-and-experiment">8.2 GPTScore Implementation and Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id54">8.2.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id55">8.2.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id56">8.2.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flask-evaluation-system-implementation">8.3 FLASK Evaluation System Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id57">8.3.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id58">8.3.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id59">8.3.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-result-analysis">8.4 Exercise Result Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">8.4.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id61">8.4.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">8.4.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id63">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-and-conclusion">9. Summary and Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-main-content">9.1 Summary of Main Content</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#changing-landscape-of-evaluation">9.1.1 Changing Landscape of Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id64">9.1.2 LLM-Based Evaluation Paradigms</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id65">9.1.3 Specialized Purpose Benchmarks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id66">9.1.4 Domain-Specific Benchmarks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id67">9.1.5 Evaluation Bias and Limitations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-and-future-evaluation-paradigms">9.1.6 RLAIF and Future Evaluation Paradigms</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-insights">9.2 Core Insights</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evolution-of-evaluation-methodologies">9.2.1 Evolution of Evaluation Methodologies</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensionality-of-evaluation">9.2.2 Multi-dimensionality of Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-domain-specialization">9.2.3 Importance of Domain Specialization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recognition-of-evaluation-bias-and-limitations">9.2.4 Recognition of Evaluation Bias and Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-development-directions">9.3 Future Development Directions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-development-of-evaluation-methodologies">9.3.1 Continuous Development of Evaluation Methodologies</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expansion-of-domain-specific-evaluation">9.3.2 Expansion of Domain-Specific Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#building-practical-evaluation-systems">9.3.3 Building Practical Evaluation Systems</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">9.4 Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">10. References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-evaluation-metrics">10.1 Traditional Evaluation Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#meaning-based-evaluation">10.2 Meaning-Based Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-based-evaluation">10.3 LLM-Based Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id68">10.4 Specialized Purpose Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id69">10.5 Domain-Specific Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-and-future-evaluation">10.6 RLAIF and Future Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id70">10.7 Evaluation Bias and Limitations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-and-reasoning-evaluation">10.8 Mathematical and Reasoning Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#medical-and-legal-evaluation">10.9 Medical and Legal Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#green-ai-and-efficiency">10.10 Green AI and Efficiency</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-5-llm-evaluation-paradigms-and-benchmarks">
<h1>Week 5: LLM Evaluation Paradigms and Benchmarks<a class="headerlink" href="#week-5-llm-evaluation-paradigms-and-benchmarks" title="Link to this heading">#</a></h1>
<section id="the-changing-landscape-of-evaluation-limitations-of-traditional-metrics-and-the-need-for-meaning-based-assessment">
<h2>1. The Changing Landscape of Evaluation: Limitations of Traditional Metrics and the Need for Meaning-Based Assessment<a class="headerlink" href="#the-changing-landscape-of-evaluation-limitations-of-traditional-metrics-and-the-need-for-meaning-based-assessment" title="Link to this heading">#</a></h2>
<p>The methodology for evaluating Large Language Models (LLMs) is rapidly evolving alongside model development. There’s a growing recognition of the limitations of traditional evaluation metrics like <strong>BLEU</strong> and <strong>ROUGE</strong>, leading to a paradigm shift toward <strong>meaning-based evaluation</strong> and the <strong>LLM-as-a-Judge</strong> approach.</p>
<section id="limitations-of-traditional-evaluation-metrics">
<h3>1.1 Limitations of Traditional Evaluation Metrics<a class="headerlink" href="#limitations-of-traditional-evaluation-metrics" title="Link to this heading">#</a></h3>
<p>In natural language generation (NLG) model quality assessment, quantitative metrics like <strong>BLEU</strong> and <strong>ROUGE</strong> have been used for a long time. These metrics have the following characteristics:</p>
<ul class="simple">
<li><p><strong>BLEU</strong>: Measures the <strong>n-gram overlap degree</strong> between reference and generated sentences in machine translation</p></li>
<li><p><strong>ROUGE</strong>: Calculates the <strong>recall of important words/phrases</strong> in summarization</p></li>
<li><p><strong>Common feature</strong>: Both are based on <strong>superficial string matching</strong> evaluation</p></li>
</ul>
<p>These traditional metrics have the following <strong>fundamental limitations</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Lack of semantic understanding</strong>: Low scores for cases where meaning is the same but words are different due to synonym usage or expression changes</p></li>
<li><p><strong>High scores for incorrect meanings</strong>: High scores even when expressions are similar but meanings are wrong</p></li>
<li><p><strong>Inability to evaluate creativity</strong>: Cannot distinguish the quality of creative or subjective LLM responses</p></li>
<li><p><strong>Ignoring factual accuracy</strong>: Does not reflect important aspects like factual accuracy or consistency</p></li>
</ol>
</section>
<section id="emergence-of-meaning-based-evaluation">
<h3>1.2 Emergence of Meaning-Based Evaluation<a class="headerlink" href="#emergence-of-meaning-based-evaluation" title="Link to this heading">#</a></h3>
<p>To overcome the limitations of traditional metrics, <strong>meaning-based evaluation methods</strong> have emerged:</p>
<section id="bertscore-and-sentencemover">
<h4>BERTScore and SentenceMover<a class="headerlink" href="#bertscore-and-sentencemover" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Measure sentence semantic similarity using <strong>similarity in embedding space</strong></p></li>
<li><p>Achieve improved correlation compared to BLEU</p></li>
<li><p>Better capture <strong>semantic similarity</strong></p></li>
</ul>
</section>
<section id="bleurt">
<h4>BLEURT<a class="headerlink" href="#bleurt" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Pre-trained evaluation metric</strong></p></li>
<li><p>Higher alignment with human evaluation through learned semantic discrimination</p></li>
<li><p>Enhanced <strong>contextual understanding</strong> capability</p></li>
</ul>
</section>
</section>
<section id="emergence-of-llm-as-a-judge-paradigm">
<h3>1.3 Emergence of LLM-as-a-Judge Paradigm<a class="headerlink" href="#emergence-of-llm-as-a-judge-paradigm" title="Link to this heading">#</a></h3>
<p>Recently, an innovative approach has emerged that utilizes <strong>Large Language Models (LLMs) as evaluators</strong>:</p>
<ul class="simple">
<li><p><strong>LLM-as-a-Judge</strong>: LLMs score or rate other LLMs’ outputs instead of humans</p></li>
<li><p><strong>Complex semantic understanding</strong>: Capable of performing contextual judgments</p></li>
<li><p><strong>Open-ended generation tasks</strong>: Can evaluate even tasks without predetermined answers</p></li>
<li><p><strong>Reflecting subjective criteria</strong>: Learn and reflect human evaluators’ judgment criteria</p></li>
</ul>
<p>This change represents a shift from <strong>BLEU/ROUGE-centered traditional metrics</strong> to <strong>meaning-based meta-evaluation</strong> utilizing LLMs’ rich semantic understanding capabilities.</p>
</section>
<section id="checkpoint-questions">
<h3>Checkpoint Questions<a class="headerlink" href="#checkpoint-questions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>How do traditional evaluation metrics like BLEU and ROUGE measure output quality, and what limitations arise from this approach?</p></li>
<li><p>Why is <strong>meaning-based evaluation</strong> necessary, and what elements of output should be primarily considered in such evaluation?</p></li>
<li><p>What potential benefits can be gained by utilizing LLMs as evaluators instead of surface-level comparison against a single reference answer?</p></li>
</ul>
</section>
</section>
<section id="llm-based-evaluation-paradigms">
<h2>2. LLM-Based Evaluation Paradigms<a class="headerlink" href="#llm-based-evaluation-paradigms" title="Link to this heading">#</a></h2>
<p>With the emergence of new paradigms utilizing LLMs as evaluators, various approaches have been proposed to overcome the limitations of existing evaluation methods. This section examines major LLM-based evaluation techniques including <strong>GPTScore</strong>, <strong>G-Eval</strong>, and <strong>FLASK</strong>.</p>
<section id="gptscore-probability-based-evaluation-framework">
<h3>2.1 GPTScore: Probability-Based Evaluation Framework<a class="headerlink" href="#gptscore-probability-based-evaluation-framework" title="Link to this heading">#</a></h3>
<p><strong>GPTScore</strong> is an early meta-evaluation technique that quantifies output quality using the <strong>language model probabilities</strong> of LLMs themselves.</p>
<section id="core-principles">
<h4>Core Principles<a class="headerlink" href="#core-principles" title="Link to this heading">#</a></h4>
<p>GPTScore operates in the following manner:</p>
<ol class="arabic simple">
<li><p><strong>Probability-based evaluation</strong>: When given source text and candidate output, calculates the probability (likelihood) that the language model would generate that output</p></li>
<li><p><strong>No reference answer needed</strong>: Can evaluate without separate reference answers</p></li>
<li><p><strong>Automatic quality measurement</strong>: Measures how well the output aligns with language patterns learned by the model, such as fluency and grammatical accuracy</p></li>
</ol>
</section>
<section id="mathematical-formulation">
<h4>Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h4>
<p>For summarization evaluation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Score</span> <span class="o">=</span> <span class="n">P</span><span class="p">(</span><span class="n">summary</span> <span class="o">|</span> <span class="n">source_text</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Generally, <strong>log probability summation</strong> or <strong>perplexity inverse</strong> is used to calculate scores.</p>
</section>
<section id="advantages">
<h4>Advantages<a class="headerlink" href="#advantages" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>No separate tuning needed</strong>: Can evaluate using only the language model’s inherent probabilities</p></li>
<li><p><strong>No reference answer needed</strong>: Can evaluate even in open-ended generation tasks</p></li>
<li><p><strong>Automation</strong>: Automatic quality measurement without human intervention</p></li>
</ul>
</section>
<section id="limitations">
<h4>Limitations<a class="headerlink" href="#limitations" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Data bias</strong>: Model’s learned data bias is reflected in evaluation scores</p></li>
<li><p><strong>Creativity suppression</strong>: Creative but correct responses may receive low scores</p></li>
<li><p><strong>Probability-quality mismatch</strong>: High probability doesn’t necessarily mean high quality</p></li>
</ol>
</section>
<section id="performance-results">
<h4>Performance Results<a class="headerlink" href="#performance-results" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Correlation with human evaluation</strong>: Approximately 0.43 (moderate level)</p></li>
<li><p><strong>Compared to BLEU</strong>: Achieved improved correlation</p></li>
<li><p><strong>Absolute reliability</strong>: Still limited</p></li>
</ul>
</section>
</section>
<section id="gptscore-implementation-example">
<h3>2.1.1 GPTScore Implementation Example<a class="headerlink" href="#gptscore-implementation-example" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_gpt_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">source_text</span><span class="p">,</span> <span class="n">candidate_text</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to calculate GPTScore</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Language model</span>
<span class="sd">        tokenizer: Tokenizer</span>
<span class="sd">        source_text: Source text</span>
<span class="sd">        candidate_text: Candidate text to evaluate</span>

<span class="sd">    Returns:</span>
<span class="sd">        GPTScore</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Construct input text</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">source_text</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">candidate_text</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="c1"># Tokenization</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

    <span class="c1"># Model inference</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>

    <span class="c1"># Calculate log probabilities</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Sum log probabilities for candidate text portion</span>
    <span class="n">candidate_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">candidate_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">candidate_length</span> <span class="o">=</span> <span class="n">candidate_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Extract log probabilities for candidate text portion</span>
    <span class="n">candidate_log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">candidate_length</span><span class="p">:,</span> <span class="p">:]</span>
    <span class="n">candidate_token_ids</span> <span class="o">=</span> <span class="n">candidate_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Sum log probabilities for each token</span>
    <span class="n">total_log_prob</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_token_ids</span><span class="p">):</span>
        <span class="n">total_log_prob</span> <span class="o">+=</span> <span class="n">candidate_log_probs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">token_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Normalize with average log probability</span>
    <span class="n">avg_log_prob</span> <span class="o">=</span> <span class="n">total_log_prob</span> <span class="o">/</span> <span class="n">candidate_length</span>

    <span class="k">return</span> <span class="n">avg_log_prob</span>

<span class="c1"># Usage example</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">source</span> <span class="o">=</span> <span class="s2">&quot;Artificial intelligence is bringing significant changes to modern society.&quot;</span>
<span class="n">candidate</span> <span class="o">=</span> <span class="s2">&quot;AI is greatly changing society.&quot;</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">calculate_gpt_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">candidate</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPTScore: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output example:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPTScore</span><span class="p">:</span> <span class="o">-</span><span class="mf">2.3456</span>
</pre></div>
</div>
</section>
<section id="g-eval-chain-of-thought-cot-based-llm-evaluation">
<h3>2.2 G-Eval: Chain-of-Thought (CoT) Based LLM Evaluation<a class="headerlink" href="#g-eval-chain-of-thought-cot-based-llm-evaluation" title="Link to this heading">#</a></h3>
<p><strong>G-Eval</strong> is a framework that utilizes state-of-the-art LLMs like OpenAI GPT-4 as evaluators, emerging to complement the limitations of GPTScore.</p>
<section id="core-features">
<h4>Core Features<a class="headerlink" href="#core-features" title="Link to this heading">#</a></h4>
<p>The core feature of G-Eval is structuring <strong>evaluation criteria and step-by-step reasoning</strong> within prompts to guide LLMs to score like humans.</p>
</section>
<section id="evaluation-process">
<h4>Evaluation Process<a class="headerlink" href="#evaluation-process" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Structured prompts</strong>: Specify evaluation criteria and step-by-step reasoning</p></li>
<li><p><strong>Chain-of-Thought (CoT)</strong>: Perform evaluation through step-by-step thinking process</p></li>
<li><p><strong>Form-Filling</strong>: Guide to fill in only scores in predetermined formats</p></li>
</ol>
</section>
<section id="example-summarization-consistency-evaluation">
<h4>Example: Summarization Consistency Evaluation<a class="headerlink" href="#example-summarization-consistency-evaluation" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Instruction</span><span class="p">:</span> <span class="n">Read</span> <span class="n">the</span> <span class="n">article</span> <span class="ow">and</span> <span class="n">summary</span><span class="p">,</span> <span class="ow">and</span> <span class="n">evaluate</span> <span class="n">on</span> <span class="n">a</span> <span class="n">scale</span> <span class="n">of</span> <span class="mi">1</span><span class="o">-</span><span class="mi">5</span> <span class="n">whether</span> <span class="n">the</span> <span class="n">summary</span> <span class="ow">is</span> <span class="n">logically</span> <span class="n">consistent</span> <span class="k">with</span> <span class="n">the</span> <span class="n">article</span> <span class="n">content</span><span class="o">.</span>

<span class="n">Evaluation</span> <span class="n">steps</span><span class="p">:</span>
<span class="mf">1.</span> <span class="n">Identify</span> <span class="n">core</span> <span class="n">topics</span> <span class="n">of</span> <span class="n">the</span> <span class="n">article</span>
<span class="mf">2.</span> <span class="n">Compare</span> <span class="n">whether</span> <span class="n">the</span> <span class="n">summary</span> <span class="n">includes</span> <span class="n">these</span>
<span class="mf">3.</span> <span class="n">Assign</span> <span class="n">consistency</span> <span class="n">score</span>
</pre></div>
</div>
</section>
<section id="probability-based-calibration">
<h4>Probability-Based Calibration<a class="headerlink" href="#probability-based-calibration" title="Link to this heading">#</a></h4>
<p>G-Eval applies <strong>calibration using probability information from model responses</strong>:</p>
<ul class="simple">
<li><p><strong>Confidence measurement</strong>: Measure how confident GPT-4 is about each choice in evaluation steps using log probabilities</p></li>
<li><p><strong>Weighted summation</strong>: Weighted sum final scores according to confidence</p></li>
<li><p><strong>Consistency improvement</strong>: Improve evaluation consistency and reliability through probability information</p></li>
</ul>
</section>
<section id="id1">
<h4>Performance Results<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>SummEval, Topical-Chat benchmarks</strong>: Achieved average Spearman correlation of 0.514</p></li>
<li><p><strong>Compared to GPTScore</strong>: Improved human correlation</p></li>
<li><p><strong>Some evaluation metrics</strong>: Correlation approaching 0.7 (SOTA level)</p></li>
<li><p><strong>Evaluation process transparency</strong>: Can track model evaluation process</p></li>
</ul>
</section>
<section id="id2">
<h4>Advantages<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Structured evaluation</strong>: Clear evaluation criteria and step-by-step process</p></li>
<li><p><strong>High correlation</strong>: High correlation with human evaluation</p></li>
<li><p><strong>Transparency</strong>: Traceability of evaluation process</p></li>
<li><p><strong>Scalability</strong>: Applicable to various evaluation criteria</p></li>
</ul>
</section>
<section id="id3">
<h4>Limitations<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Cost</strong>: Requires API calls to large models like GPT-4</p></li>
<li><p><strong>Stability</strong>: Need multiple evaluations per criterion for stability</p></li>
<li><p><strong>Dependency</strong>: Dependency on specific models (GPT-4)</p></li>
</ul>
</section>
</section>
<section id="g-eval-implementation-example">
<h3>2.2.1 G-Eval Implementation Example<a class="headerlink" href="#g-eval-implementation-example" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GEvalEvaluator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="k">if</span> <span class="n">api_key</span><span class="p">:</span>
            <span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">api_key</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_evaluation_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">candidate_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                                <span class="n">criteria</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;1-5&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create G-Eval style evaluation prompt</span>

<span class="sd">        Args:</span>
<span class="sd">            source_text: Source text</span>
<span class="sd">            candidate_text: Candidate text to evaluate</span>
<span class="sd">            criteria: Evaluation criteria</span>
<span class="sd">            scale: Evaluation scale</span>

<span class="sd">        Returns:</span>
<span class="sd">            Structured evaluation prompt</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Please evaluate the following text.</span>

<span class="s2">Source text:</span>
<span class="si">{</span><span class="n">source_text</span><span class="si">}</span>

<span class="s2">Candidate text:</span>
<span class="si">{</span><span class="n">candidate_text</span><span class="si">}</span>

<span class="s2">Evaluation criteria: </span><span class="si">{</span><span class="n">criteria</span><span class="si">}</span>

<span class="s2">Evaluation steps:</span>
<span class="s2">1. Identify core content of source text</span>
<span class="s2">2. Analyze how well candidate text matches source text</span>
<span class="s2">3. Assign score on </span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2"> scale according to evaluation criteria</span>

<span class="s2">Please explain the evaluation process step by step and provide the final score.</span>

<span class="s2">Format:</span>
<span class="s2">Step 1: [Core content identification]</span>
<span class="s2">Step 2: [Match analysis]</span>
<span class="s2">Step 3: [Score assignment]</span>
<span class="s2">Final score: [Score]&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">prompt</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">candidate_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                 <span class="n">criteria</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate text using G-Eval method</span>

<span class="sd">        Args:</span>
<span class="sd">            source_text: Source text</span>
<span class="sd">            candidate_text: Candidate text to evaluate</span>
<span class="sd">            criteria: Evaluation criteria</span>
<span class="sd">            num_samples: Number of evaluation samples</span>

<span class="sd">        Returns:</span>
<span class="sd">            Evaluation result dictionary</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_evaluation_prompt</span><span class="p">(</span><span class="n">source_text</span><span class="p">,</span> <span class="n">candidate_text</span><span class="p">,</span> <span class="n">criteria</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">explanations</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
                    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Low temperature for consistency</span>
                    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">500</span>
                <span class="p">)</span>

                <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
                <span class="n">explanations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

                <span class="c1"># Extract score (using simple regex)</span>
                <span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
                <span class="n">score_match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Final score:\s*(\d+)&#39;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">score_match</span><span class="p">:</span>
                    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">score_match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error in evaluation </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">scores</span><span class="p">:</span>
            <span class="n">avg_score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;average_score&quot;</span><span class="p">:</span> <span class="n">avg_score</span><span class="p">,</span>
                <span class="s2">&quot;scores&quot;</span><span class="p">:</span> <span class="n">scores</span><span class="p">,</span>
                <span class="s2">&quot;explanations&quot;</span><span class="p">:</span> <span class="n">explanations</span><span class="p">,</span>
                <span class="s2">&quot;consistency&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span>  <span class="c1"># Check if all scores are the same</span>
            <span class="p">}</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="s2">&quot;Evaluation failed&quot;</span><span class="p">}</span>

<span class="c1"># Usage example</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">GEvalEvaluator</span><span class="p">()</span>

<span class="n">source</span> <span class="o">=</span> <span class="s2">&quot;Artificial intelligence technology is bringing innovative changes to the medical field. AI is improving diagnostic accuracy, enabling personalized treatment, and significantly enhancing healthcare professionals&#39; work efficiency.&quot;</span>

<span class="n">candidate</span> <span class="o">=</span> <span class="s2">&quot;AI is bringing major changes to healthcare. Diagnosis has become more accurate and personalized treatment is now possible.&quot;</span>

<span class="n">criteria</span> <span class="o">=</span> <span class="s2">&quot;How accurately and completely the core content of the source text is summarized&quot;</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">candidate</span><span class="p">,</span> <span class="n">criteria</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average score: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;average_score&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Consistency: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;consistency&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output example:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Average</span> <span class="n">score</span><span class="p">:</span> <span class="mf">4.33</span>
<span class="n">Consistency</span><span class="p">:</span> <span class="kc">True</span>
</pre></div>
</div>
</section>
<section id="flask-fine-grained-skill-set-based-evaluation">
<h3>2.3 FLASK: Fine-grained Skill Set Based Evaluation<a class="headerlink" href="#flask-fine-grained-skill-set-based-evaluation" title="Link to this heading">#</a></h3>
<p><strong>FLASK</strong> is a framework that pursues more interpretable and specific evaluation by decomposing evaluation into multiple detailed ability components. Published at ICLR 2024, FLASK scores how well responses satisfy various required abilities (skills) instead of providing a single total score.</p>
<section id="core-concepts">
<h4>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h4>
<p>FLASK takes the following approach:</p>
<ol class="arabic simple">
<li><p><strong>Skill decomposition</strong>: Decompose evaluation tasks into component abilities</p></li>
<li><p><strong>Granular evaluation</strong>: Assign separate scores for each ability</p></li>
<li><p><strong>Multi-dimensional metrics</strong>: Provide multi-dimensional evaluation results instead of total scores</p></li>
</ol>
</section>
<section id="fine-grained-ability-indicators">
<h4>12 Fine-grained Ability Indicators<a class="headerlink" href="#fine-grained-ability-indicators" title="Link to this heading">#</a></h4>
<p>The research team defined the following 12 ability indicators:</p>
<ol class="arabic simple">
<li><p><strong>Explicit reasoning ability</strong>: Clearly present logical reasoning processes</p></li>
<li><p><strong>Background knowledge utilization</strong>: Appropriately utilize relevant knowledge</p></li>
<li><p><strong>Logical consistency</strong>: Maintain logical consistency within responses</p></li>
<li><p><strong>Context adherence</strong>: Appropriately conform to given context</p></li>
<li><p><strong>Accuracy</strong>: Factual accuracy</p></li>
<li><p><strong>Completeness</strong>: Complete answers to questions</p></li>
<li><p><strong>Clarity</strong>: Clarity and comprehensibility of responses</p></li>
<li><p><strong>Creativity</strong>: Original and creative approaches</p></li>
<li><p><strong>Practicality</strong>: Practical applicability</p></li>
<li><p><strong>Ethics</strong>: Adherence to ethical standards</p></li>
<li><p><strong>Efficiency</strong>: Concise and efficient responses</p></li>
<li><p><strong>Adaptability</strong>: Appropriate responses to situations</p></li>
</ol>
</section>
<section id="id4">
<h4>Evaluation Process<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Skill tagging</strong>: Tag abilities relevant to each evaluation instance</p></li>
<li><p><strong>Individual evaluation</strong>: Assign separate scores for each skill</p></li>
<li><p><strong>Weight application</strong>: Calculate final scores by applying weights according to importance</p></li>
<li><p><strong>Multi-dimensional results</strong>: Provide multi-dimensional evaluation metrics instead of total scores</p></li>
</ol>
</section>
<section id="id5">
<h4>Performance Results<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>Key achievements of FLASK:</p>
<ul class="simple">
<li><p><strong>Capturing subtle differences between models</strong>: Diagnosing strengths and weaknesses of GPT-4 and GPT-3.5 by ability</p></li>
<li><p><strong>High correlation</strong>: High correlation between model evaluation and human evaluation</p></li>
<li><p><strong>Interpretability</strong>: Clear interpretation of evaluation results</p></li>
<li><p><strong>Customized evaluation</strong>: Possible to design various customized evaluation rubrics</p></li>
</ul>
</section>
<section id="example-legal-consultation-response-evaluation">
<h4>Example: Legal Consultation Response Evaluation<a class="headerlink" href="#example-legal-consultation-response-evaluation" title="Link to this heading">#</a></h4>
<p>For legal consultation responses, the following skills are required:</p>
<ul class="simple">
<li><p><strong>Legal provision recall</strong> (background knowledge)</p></li>
<li><p><strong>Logical reasoning</strong></p></li>
<li><p><strong>Ethical standard compliance</strong></p></li>
<li><p><strong>Practical advice provision</strong></p></li>
</ul>
<p>Each skill is scored separately, and final evaluation is performed by applying weights according to importance.</p>
</section>
<section id="id6">
<h4>Advantages<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Granular evaluation</strong>: Clearly identify model strengths and weaknesses</p></li>
<li><p><strong>Interpretability</strong>: Clear interpretation of evaluation results</p></li>
<li><p><strong>Customized application</strong>: Possible customized evaluation for various domains</p></li>
<li><p><strong>Reliability improvement</strong>: Simultaneously improve reliability and explanatory power of LLM evaluation</p></li>
</ul>
</section>
<section id="id7">
<h4>Limitations<a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Complexity</strong>: Increased complexity of evaluation process</p></li>
<li><p><strong>Subjectivity</strong>: Subjectivity in ability definition and weight setting</p></li>
<li><p><strong>Cost</strong>: Increased cost due to granular evaluation</p></li>
</ul>
</section>
</section>
<section id="flask-implementation-example">
<h3>2.3.1 FLASK Implementation Example<a class="headerlink" href="#flask-implementation-example" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FLASKEvaluator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Define 12 ability indicators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skills</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;explicit_reasoning&quot;</span><span class="p">:</span> <span class="s2">&quot;Explicit reasoning ability&quot;</span><span class="p">,</span>
            <span class="s2">&quot;background_knowledge&quot;</span><span class="p">:</span> <span class="s2">&quot;Background knowledge utilization&quot;</span><span class="p">,</span>
            <span class="s2">&quot;logical_consistency&quot;</span><span class="p">:</span> <span class="s2">&quot;Logical consistency&quot;</span><span class="p">,</span>
            <span class="s2">&quot;context_adherence&quot;</span><span class="p">:</span> <span class="s2">&quot;Context adherence&quot;</span><span class="p">,</span>
            <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span>
            <span class="s2">&quot;completeness&quot;</span><span class="p">:</span> <span class="s2">&quot;Completeness&quot;</span><span class="p">,</span>
            <span class="s2">&quot;clarity&quot;</span><span class="p">:</span> <span class="s2">&quot;Clarity&quot;</span><span class="p">,</span>
            <span class="s2">&quot;creativity&quot;</span><span class="p">:</span> <span class="s2">&quot;Creativity&quot;</span><span class="p">,</span>
            <span class="s2">&quot;practicality&quot;</span><span class="p">:</span> <span class="s2">&quot;Practicality&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ethics&quot;</span><span class="p">:</span> <span class="s2">&quot;Ethics&quot;</span><span class="p">,</span>
            <span class="s2">&quot;efficiency&quot;</span><span class="p">:</span> <span class="s2">&quot;Efficiency&quot;</span><span class="p">,</span>
            <span class="s2">&quot;adaptability&quot;</span><span class="p">:</span> <span class="s2">&quot;Adaptability&quot;</span>
        <span class="p">}</span>

        <span class="c1"># Skill weights by domain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">domain_weights</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;legal&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;background_knowledge&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
                <span class="s2">&quot;logical_consistency&quot;</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
                <span class="s2">&quot;ethics&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                <span class="s2">&quot;practicality&quot;</span><span class="p">:</span> <span class="mf">0.15</span><span class="p">,</span>
                <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.1</span>
            <span class="p">},</span>
            <span class="s2">&quot;medical&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
                <span class="s2">&quot;background_knowledge&quot;</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
                <span class="s2">&quot;logical_consistency&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                <span class="s2">&quot;ethics&quot;</span><span class="p">:</span> <span class="mf">0.15</span><span class="p">,</span>
                <span class="s2">&quot;practicality&quot;</span><span class="p">:</span> <span class="mf">0.1</span>
            <span class="p">},</span>
            <span class="s2">&quot;general&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                <span class="s2">&quot;completeness&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                <span class="s2">&quot;clarity&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                <span class="s2">&quot;logical_consistency&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                <span class="s2">&quot;practicality&quot;</span><span class="p">:</span> <span class="mf">0.2</span>
            <span class="p">}</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate_skill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">question</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">skill</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate answer for specific skill</span>

<span class="sd">        Args:</span>
<span class="sd">            question: Question</span>
<span class="sd">            answer: Answer</span>
<span class="sd">            skill: Skill to evaluate</span>

<span class="sd">        Returns:</span>
<span class="sd">            Skill score (0-1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># In actual implementation, use LLM to evaluate each skill</span>
        <span class="c1"># Here implemented as simple example</span>

        <span class="n">skill_prompts</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;explicit_reasoning&quot;</span><span class="p">:</span> <span class="s2">&quot;Did the answer clearly present logical reasoning process?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;background_knowledge&quot;</span><span class="p">:</span> <span class="s2">&quot;Did the answer appropriately utilize relevant knowledge?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;logical_consistency&quot;</span><span class="p">:</span> <span class="s2">&quot;Is logical consistency maintained within the answer?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;context_adherence&quot;</span><span class="p">:</span> <span class="s2">&quot;Does the answer appropriately conform to given context?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="s2">&quot;Is the answer factually accurate?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;completeness&quot;</span><span class="p">:</span> <span class="s2">&quot;Is the answer complete for the question?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;clarity&quot;</span><span class="p">:</span> <span class="s2">&quot;Is the answer clear and comprehensible?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;creativity&quot;</span><span class="p">:</span> <span class="s2">&quot;Is the answer original and creative?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;practicality&quot;</span><span class="p">:</span> <span class="s2">&quot;Is the answer practically applicable?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ethics&quot;</span><span class="p">:</span> <span class="s2">&quot;Does the answer comply with ethical standards?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;efficiency&quot;</span><span class="p">:</span> <span class="s2">&quot;Is the answer concise and efficient?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;adaptability&quot;</span><span class="p">:</span> <span class="s2">&quot;Is the answer appropriate for the situation?&quot;</span>
        <span class="p">}</span>

        <span class="c1"># Actually use LLM for evaluation</span>
        <span class="c1"># Here return random score as example</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate_answer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">question</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">domain</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;general&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Comprehensive evaluation of answer using FLASK method</span>

<span class="sd">        Args:</span>
<span class="sd">            question: Question</span>
<span class="sd">            answer: Answer</span>
<span class="sd">            domain: Domain (legal, medical, general)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Evaluation result dictionary</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Individual evaluation for each skill</span>
        <span class="n">skill_scores</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">skill</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skills</span><span class="p">:</span>
            <span class="n">skill_scores</span><span class="p">[</span><span class="n">skill</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_skill</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">answer</span><span class="p">,</span> <span class="n">skill</span><span class="p">)</span>

        <span class="c1"># Apply domain-specific weights</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">domain_weights</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">domain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">domain_weights</span><span class="p">[</span><span class="s2">&quot;general&quot;</span><span class="p">])</span>

        <span class="c1"># Calculate weighted average</span>
        <span class="n">weighted_score</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">skill</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">skill_scores</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">skill</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
                <span class="n">weighted_score</span> <span class="o">+=</span> <span class="n">score</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="n">skill</span><span class="p">]</span>
                <span class="n">total_weight</span> <span class="o">+=</span> <span class="n">weights</span><span class="p">[</span><span class="n">skill</span><span class="p">]</span>

        <span class="n">final_score</span> <span class="o">=</span> <span class="n">weighted_score</span> <span class="o">/</span> <span class="n">total_weight</span> <span class="k">if</span> <span class="n">total_weight</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;final_score&quot;</span><span class="p">:</span> <span class="n">final_score</span><span class="p">,</span>
            <span class="s2">&quot;skill_scores&quot;</span><span class="p">:</span> <span class="n">skill_scores</span><span class="p">,</span>
            <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="n">weights</span><span class="p">,</span>
            <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="n">domain</span><span class="p">,</span>
            <span class="s2">&quot;detailed_analysis&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_analysis</span><span class="p">(</span><span class="n">skill_scores</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_analysis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">skill_scores</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate detailed analysis&quot;&quot;&quot;</span>
        <span class="n">analysis</span> <span class="o">=</span> <span class="s2">&quot;Ability-based evaluation results:</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="c1"># Display high-weight skills first</span>
        <span class="n">sorted_skills</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">skill</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">sorted_skills</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">skill_scores</span><span class="p">[</span><span class="n">skill</span><span class="p">]</span>
            <span class="n">skill_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">skills</span><span class="p">[</span><span class="n">skill</span><span class="p">]</span>
            <span class="n">analysis</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">skill_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (weight: </span><span class="si">{</span><span class="n">weight</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="n">analysis</span>

<span class="c1"># Usage example</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">FLASKEvaluator</span><span class="p">()</span>

<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;What should I do if there are unfair clauses in a contract?&quot;</span>
<span class="n">answer</span> <span class="o">=</span> <span class="s2">&quot;If there are unfair clauses, first check whether the clause is legally valid. Unfair clauses under civil law can be invalid, and you may be protected under consumer protection law. It&#39;s advisable to consult with experts to explore specific response measures.&quot;</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate_answer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">answer</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s2">&quot;legal&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final score: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;final_score&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;detailed_analysis&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>Output example:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Final</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.85</span>
<span class="n">Ability</span><span class="o">-</span><span class="n">based</span> <span class="n">evaluation</span> <span class="n">results</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Background</span> <span class="n">knowledge</span> <span class="n">utilization</span><span class="p">:</span> <span class="mf">0.92</span> <span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="mf">0.30</span><span class="p">)</span>
<span class="o">-</span> <span class="n">Logical</span> <span class="n">consistency</span><span class="p">:</span> <span class="mf">0.88</span> <span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="o">-</span> <span class="n">Ethics</span><span class="p">:</span> <span class="mf">0.90</span> <span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="mf">0.20</span><span class="p">)</span>
<span class="o">-</span> <span class="n">Practicality</span><span class="p">:</span> <span class="mf">0.82</span> <span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="mf">0.15</span><span class="p">)</span>
<span class="o">-</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.85</span> <span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>Checkpoint Questions<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What properties of language models does <strong>GPTScore</strong> use to evaluate the quality of generated content? Explain the advantages and limitations of this method.</p></li>
<li><p>What techniques did <strong>G-Eval</strong> introduce to improve evaluation reliability? (e.g., Chain-of-Thought, Form-Filling, etc.) How did these techniques improve correlation with human evaluation?</p></li>
<li><p>Why does the <strong>FLASK</strong> framework perform evaluation in granular skill-based manner? How does the resulting multi-dimensional evaluation results help in interpreting model performance?</p></li>
</ul>
<p><strong>Table 1: Comparison of Traditional Evaluation and LLM-based Meta-Evaluation Paradigms</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Traditional Metrics (BLEU/ROUGE)</p></th>
<th class="head"><p>LLM-as-a-Judge</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Evaluation Criteria</strong></p></td>
<td><p>N-gram overlap with reference text (lexical matching)</p></td>
<td><p>Abstract quality criteria defined by natural language instructions (e.g., usefulness, logic)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Semantic Understanding</strong></p></td>
<td><p>Impossible. Cannot understand synonyms or different expressions.</p></td>
<td><p>Possible. Understand context and nuances to evaluate semantic quality.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Creativity/Diversity</strong></p></td>
<td><p>Penalizes creativity when different from reference text.</p></td>
<td><p>Can highly evaluate diverse expressions and creative results if they meet evaluation criteria.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Cost and Scalability</strong></p></td>
<td><p>High cost for reference text construction, low scalability for new tasks.</p></td>
<td><p>Low-cost large-scale evaluation possible after initial setup, very high scalability.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Interpretability</strong></p></td>
<td><p>Provides only scores. Cannot know why low scores occurred.</p></td>
<td><p>Can explain “why” evaluation was made in natural language, providing specific feedback.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Core Limitation</strong></p></td>
<td><p>Cannot measure core capabilities of generative AI (meaning, creativity).</p></td>
<td><p>Bias issues (preference leakage, position), multilingual consistency, evaluator reliability.</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 2: Comparison of Correlation with Human Evaluation (SummEval Dataset, Spearman Correlation ρ)</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Evaluation Metric</p></th>
<th class="head"><p>Coherence</p></th>
<th class="head"><p>Consistency</p></th>
<th class="head"><p>Fluency</p></th>
<th class="head"><p>Relevance</p></th>
<th class="head"><p>Average</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ROUGE-1</p></td>
<td><p>0.167</p></td>
<td><p>0.207</p></td>
<td><p>0.105</p></td>
<td><p>0.326</p></td>
<td><p>0.201</p></td>
</tr>
<tr class="row-odd"><td><p>ROUGE-2</p></td>
<td><p>0.158</p></td>
<td><p>0.200</p></td>
<td><p>0.106</p></td>
<td><p>0.306</p></td>
<td><p>0.192</p></td>
</tr>
<tr class="row-even"><td><p>ROUGE-L</p></td>
<td><p>0.170</p></td>
<td><p>0.210</p></td>
<td><p>0.110</p></td>
<td><p>0.320</p></td>
<td><p>0.202</p></td>
</tr>
<tr class="row-odd"><td><p>BERTScore</p></td>
<td><p>0.284</p></td>
<td><p>0.362</p></td>
<td><p>0.216</p></td>
<td><p>0.426</p></td>
<td><p>0.322</p></td>
</tr>
<tr class="row-even"><td><p><strong>G-Eval (GPT-4 based)</strong></p></td>
<td><p><strong>0.582</strong></p></td>
<td><p><strong>0.460</strong></p></td>
<td><p><strong>0.467</strong></p></td>
<td><p><strong>0.547</strong></p></td>
<td><p><strong>0.514</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p>Data source: Liu et al., 2023. G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment.</p>
</section>
</section>
<section id="specialized-purpose-benchmarks">
<h2>3. Specialized Purpose Benchmarks<a class="headerlink" href="#specialized-purpose-benchmarks" title="Link to this heading">#</a></h2>
<p>With the emergence of benchmarks specialized for specific domains or abilities, it has become possible to evaluate various capabilities of LLMs more accurately and comprehensively. This section examines major specialized purpose benchmarks including <strong>LiveCodeBench</strong>, <strong>EvalPlus</strong>, <strong>MMLU-Pro</strong>, <strong>GPQA</strong>, and <strong>BBH</strong>.</p>
<section id="livecodebench-contamination-free-code-generation-evaluation">
<h3>3.1 LiveCodeBench: Contamination-Free Code Generation Evaluation<a class="headerlink" href="#livecodebench-contamination-free-code-generation-evaluation" title="Link to this heading">#</a></h3>
<p><strong>LiveCodeBench</strong> is a real-time updatable code evaluation set proposed in 2024, with the most distinctive feature being designed to fundamentally block data contamination.</p>
<section id="core-problem-data-contamination">
<h4>Core Problem: Data Contamination<a class="headerlink" href="#core-problem-data-contamination" title="Link to this heading">#</a></h4>
<p>Existing code evaluation datasets like HumanEval (OpenAI) and MBPP (Google) have the following problems:</p>
<ul class="simple">
<li><p><strong>Pre-exposure of evaluation problems</strong>: Evaluation problems are included in model training and pre-exposed</p></li>
<li><p><strong>Overfitting problem</strong>: Latest models like GPT-4 achieve high scores of 80-90% on HumanEval, but this may be due to having seen some problems or learning them in modified forms</p></li>
<li><p><strong>Reliability degradation</strong>: Gap between actual coding ability and evaluation results</p></li>
</ul>
</section>
<section id="solution-approach">
<h4>Solution Approach<a class="headerlink" href="#solution-approach" title="Link to this heading">#</a></h4>
<p>LiveCodeBench solves the problem in the following ways:</p>
<ol class="arabic simple">
<li><p><strong>Real-time problem collection</strong>: Continuously collect latest problems from online judge platforms (LeetCode, AtCoder, CodeForces)</p></li>
<li><p><strong>Time window-based updates</strong>: Select 400+ new problems published from May 2023 to May 2024 to form initial set</p></li>
<li><p><strong>Periodic updates</strong>: Set time windows and periodically update problems to use only problems that appeared after model training</p></li>
</ol>
</section>
<section id="holistic-evaluation">
<h4>Holistic Evaluation<a class="headerlink" href="#holistic-evaluation" title="Link to this heading">#</a></h4>
<p>The second feature of LiveCodeBench is expanding the evaluation scope comprehensively:</p>
<ul class="simple">
<li><p><strong>Accuracy of code execution results</strong>: Evaluation of executable code, not just code generation</p></li>
<li><p><strong>Self-debugging (self-repair) ability</strong>: Scenarios where error logs are given and code needs to be modified</p></li>
<li><p><strong>Appropriateness of comments or output format</strong>: Evaluation of various aspects of code quality</p></li>
<li><p><strong>pass&#64;k measurement</strong>: Measure whether execution results match given test cases</p></li>
<li><p><strong>Test output prediction</strong>: Include scenarios where models predict test outputs</p></li>
</ul>
</section>
<section id="id9">
<h4>Performance Results<a class="headerlink" href="#id9" title="Link to this heading">#</a></h4>
<p>Results from applying LiveCodeBench to 18 base LLMs and 34 instruction-tuned LLMs:</p>
<ul class="simple">
<li><p><strong>Limitations of existing static benchmarks</strong>: Even latest models showing high performance on HumanEval show significantly lower performance on new problems</p></li>
<li><p><strong>Overfitting confirmation</strong>: Models overfitted on HumanEval are weaker on actual new problems</p></li>
<li><p><strong>Transparency</strong>: Transparently provide all prompts, model responses, and evaluation scripts</p></li>
</ul>
</section>
<section id="significance">
<h4>Significance<a class="headerlink" href="#significance" title="Link to this heading">#</a></h4>
<p>LiveCodeBench is an innovative benchmark that “throws new problems live so models can’t solve them,” revealing model overfitting/memorization and evaluating various code generation abilities in a multi-dimensional way.</p>
</section>
</section>
<section id="evalplus-test-case-augmentation">
<h3>3.2 EvalPlus: Test Case Augmentation<a class="headerlink" href="#evalplus-test-case-augmentation" title="Link to this heading">#</a></h3>
<p><strong>EvalPlus</strong> is a technique and dataset to supplement the insufficiency of test cases in code evaluation, introduced at NeurIPS 2023.</p>
<section id="core-problem">
<h4>Core Problem<a class="headerlink" href="#core-problem" title="Link to this heading">#</a></h4>
<p>Problems with existing HumanEval:</p>
<ul class="simple">
<li><p><strong>Insufficient test cases</strong>: Average of 7-10 simple tests per problem</p></li>
<li><p><strong>Complex bugs not caught</strong>: Simple tests cannot catch complex bugs</p></li>
<li><p><strong>Lenient evaluation</strong>: Wrong solutions are considered correct</p></li>
</ul>
</section>
<section id="id10">
<h4>Solution Approach<a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<p>EvalPlus takes the following approach:</p>
<ol class="arabic simple">
<li><p><strong>Automatic test case generation</strong>: Automatically generate dozens of times more test cases by transforming/expanding existing test inputs</p></li>
<li><p><strong>Mutation-based Input Generation</strong>: Mutate or add boundary cases to given function inputs in various ways</p></li>
<li><p><strong>Various input transformations</strong>: For functions handling lists, automatically generate input transformations like empty lists, single-element lists, lists including negative/special values</p></li>
</ol>
</section>
<section id="id11">
<h4>Performance Results<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<p>Results from building HumanEval+ set and re-evaluating models:</p>
<ul class="simple">
<li><p><strong>GPT-4’s pass&#64;1 accuracy dropped by over 20 percentage points</strong></p></li>
<li><p><strong>156 out of 164 HumanEval problems showed decreased accuracy after introducing additional tests</strong></p></li>
<li><p><strong>Leniency of existing evaluation revealed</strong></p></li>
</ul>
</section>
<section id="id12">
<h4>Significance<a class="headerlink" href="#id12" title="Link to this heading">#</a></h4>
<p>EvalPlus research proved that automated test augmentation improves model evaluation reliability and suggests that additional tests for safety/security aspects (e.g., handling malicious inputs) can be considered in the future.</p>
</section>
</section>
<section id="helm-code-transparency-and-community-collaboration">
<h3>3.3 HELM-Code: Transparency and Community Collaboration<a class="headerlink" href="#helm-code-transparency-and-community-collaboration" title="Link to this heading">#</a></h3>
<p><strong>HELM (Holistic Evaluation of Language Models)</strong> is an evaluation effort led by Stanford CRFM, focusing on evaluation transparency and community collaboration.</p>
<section id="id13">
<h4>Core Features<a class="headerlink" href="#id13" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Massive benchmark set</strong>: Composed of various scenarios and metrics</p></li>
<li><p><strong>Transparency</strong>: Publicly release all experimental results, prompts, and responses</p></li>
<li><p><strong>Reproducibility</strong>: Efforts to improve reproducibility of evaluation process</p></li>
<li><p><strong>Leaderboard</strong>: Continuously evaluate latest models and make public</p></li>
</ul>
</section>
<section id="helm-philosophy">
<h4>HELM Philosophy<a class="headerlink" href="#helm-philosophy" title="Link to this heading">#</a></h4>
<p>HELM’s philosophy is to present evaluation comprehensively like “one map,” including not only quantitative model performance but also:</p>
<ul class="simple">
<li><p><strong>Inference speed</strong></p></li>
<li><p><strong>Memory usage</strong></p></li>
<li><p><strong>Bias/harmfulness</strong></p></li>
<li><p><strong>Carbon emissions (Green AI metrics)</strong></p></li>
</ul>
</section>
<section id="id14">
<h4>Significance<a class="headerlink" href="#id14" title="Link to this heading">#</a></h4>
<p>This transparent and comprehensive approach improves the reliability of evaluation itself and guides researchers to develop models from a broad perspective rather than focusing on specific tasks or metrics.</p>
</section>
</section>
<section id="mmlu-pro-10-choice-high-difficulty-knowledge-reasoning-benchmark">
<h3>3.4 MMLU-Pro: 10-Choice High-Difficulty Knowledge/Reasoning Benchmark<a class="headerlink" href="#mmlu-pro-10-choice-high-difficulty-knowledge-reasoning-benchmark" title="Link to this heading">#</a></h3>
<p><strong>MMLU (Massive Multi-Task Language Understanding)</strong> is a large-scale problem bank spanning 57 fields, a representative knowledge evaluation benchmark used since the GPT-3 era. Each problem was composed of 4-choice multiple choice questions, but when latest LLMs reached human level on MMLU early, MMLU-Pro, an expanded version with dramatically increased difficulty, emerged in 2024.</p>
<section id="core-changes">
<h4>Core Changes<a class="headerlink" href="#core-changes" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Increased number of choices</strong>: Increased from 4 to 10</p>
<ul class="simple">
<li><p>Random answer probability decreased from 25% to 10%</p></li>
<li><p>Reduced possibility of passing by luck through simple memorization</p></li>
</ul>
</li>
<li><p><strong>Increased problem complexity</strong>: Requires more complex multi-step reasoning</p>
<ul class="simple">
<li><p>Added problems requiring synthesis of multiple knowledge or multi-step reasoning instead of simple memorization questions</p></li>
<li><p>Example: “In which year did what event happen?” → “Which of the following arranges historical events in chronological order?”</p></li>
</ul>
</li>
</ol>
</section>
<section id="id15">
<h4>Performance Results<a class="headerlink" href="#id15" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>GPT-4 and other models showed 16-33% accuracy decrease compared to MMLU</strong></p></li>
<li><p><strong>Secured discrimination to make differences between models prominent again</strong></p></li>
</ul>
</section>
<section id="chain-of-thought-effect">
<h4>Chain-of-Thought Effect<a class="headerlink" href="#chain-of-thought-effect" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>MMLU original</strong>: CoT guidance doesn’t show significant performance improvement</p></li>
<li><p><strong>MMLU-Pro</strong>: CoT guidance shows clear effect on performance improvement</p></li>
<li><p><strong>“Solve step by step and choose answer”</strong> instruction significantly increased accuracy</p></li>
</ul>
</section>
<section id="current-status">
<h4>Current Status<a class="headerlink" href="#current-status" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Includes tens of thousands of questions</strong></p></li>
<li><p><strong>Covers graduate-level questions in 14 fields</strong></p></li>
<li><p><strong>Even best models as of 2025 achieve around 85% accuracy</strong></p></li>
<li><p><strong>Tendency to make mistakes on problems requiring any application</strong></p></li>
</ul>
</section>
<section id="id16">
<h4>Significance<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<p>The emergence of MMLU-Pro shows that future LLM evaluation is moving beyond “simple knowledge memorization tests” to “thinking and problem-solving ability evaluation.”</p>
</section>
</section>
<section id="gpqa-and-bbh-knowledge-reasoning-enhanced-evaluation-sets">
<h3>3.5 GPQA and BBH: Knowledge/Reasoning Enhanced Evaluation Sets<a class="headerlink" href="#gpqa-and-bbh-knowledge-reasoning-enhanced-evaluation-sets" title="Link to this heading">#</a></h3>
<p><strong>GPQA (Graduate-level Google-Proof Questions &amp; Answers)</strong> is a high-difficulty QA benchmark that emerged in 2024, collecting graduate-level questions as the name suggests.</p>
<section id="id17">
<h4>Core Features<a class="headerlink" href="#id17" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>“Google-Proof”</strong>: Questions based on relatively new research that cannot be found through simple Google search</p></li>
<li><p><strong>Problems requiring multiple steps to solve</strong></p></li>
<li><p><strong>Written by field experts in biology, physics, chemistry</strong></p></li>
<li><p><strong>Total of 448 5-choice questions</strong></p></li>
</ul>
</section>
<section id="problem-examples">
<h4>Problem Examples<a class="headerlink" href="#problem-examples" title="Link to this heading">#</a></h4>
<p>For example, questions like <em>“What phenomenon appears when removing the role of catalyst A in a certain chemical reaction pathway?”</em> require both expert knowledge and logic.</p>
</section>
<section id="id18">
<h4>Performance Results<a class="headerlink" href="#id18" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>PhD students achieve around 65% accuracy on GPQA</strong></p></li>
<li><p><strong>State-of-the-art models like GPT-4, Gemini also achieve similar levels (around 85% or less) on this set</strong></p></li>
<li><p><strong>Still cannot make significant difference from humans</strong></p></li>
</ul>
</section>
<section id="id19">
<h4>Significance<a class="headerlink" href="#id19" title="Link to this heading">#</a></h4>
<p>GPQA’s significance is as follows:</p>
<ol class="arabic simple">
<li><p><strong>Imposes new difficulty on LLMs that have absorbed almost all internet knowledge</strong></p></li>
<li><p><strong>Reveals model limitations in scientific reasoning tasks</strong></p></li>
<li><p><strong>Even GPT-4 shows incomplete aspects like selecting explanations mixed with misconceptions on some GPQA items</strong></p></li>
<li><p><strong>Shows need for future research on specialized field learning and reasoning enhancement</strong></p></li>
</ol>
</section>
<section id="bbh-big-bench-hard">
<h4>BBH (BIG-Bench Hard)<a class="headerlink" href="#bbh-big-bench-hard" title="Link to this heading">#</a></h4>
<p><strong>BBH (BIG-Bench Hard)</strong> is a hard test set composed of only 23 particularly difficult tasks from Google’s BIG-Bench large benchmark.</p>
<section id="id20">
<h5>Core Features<a class="headerlink" href="#id20" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>BIG-Bench original</strong>: 200+ diverse language model tasks</p></li>
<li><p><strong>BBH</strong>: Bundled reasoning puzzles, trap problems, and capability limitation evaluation problems that existing models performed poorly on, tagged as “Hard”</p></li>
<li><p><strong>Mathematics area</strong>: Case counting reasoning problems, not simple calculations</p></li>
<li><p><strong>Language area</strong>: Complex grammatical paradoxes, etc.</p></li>
</ul>
</section>
<section id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Link to this heading">#</a></h5>
<p><strong>Focused attack on model weaknesses</strong>, as GPT-3 or early GPT-4 could only achieve random guessing level or slightly better performance on each BBH task.</p>
</section>
<section id="research-utilization">
<h5>Research Utilization<a class="headerlink" href="#research-utilization" title="Link to this heading">#</a></h5>
<p>Researchers analyzed specific capability deficiencies of models (e.g., logical puzzle solving, paradox handling) through BBH and attempted improvements through Chain-of-Thought guidance or additional training data input.</p>
</section>
<section id="extended-version-bbeh">
<h5>Extended Version: BBEH<a class="headerlink" href="#extended-version-bbeh" title="Link to this heading">#</a></h5>
<p><strong>BBH Extended Version (BBEH: BIG-Bench Extra Hard)</strong> published in 2023 further strengthened multi-step reasoning, creative problem solving, etc., presenting even more difficult challenges to LLMs.</p>
</section>
<section id="id21">
<h5>Significance<a class="headerlink" href="#id21" title="Link to this heading">#</a></h5>
<p>As a result, BBH/BBEH functions as a stress test for continuously checking model limitations in the LLM research community.</p>
</section>
</section>
</section>
<section id="id22">
<h3>Checkpoint Questions<a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>How did <strong>LiveCodeBench</strong> solve the <strong>data contamination</strong> problem? What additional evaluation capabilities did this benchmark emphasize compared to existing code evaluation?</p></li>
<li><p>What principle does <strong>Mutation-based Input Generation</strong> proposed by <strong>EvalPlus</strong> operate on, and why did GPT-4’s performance drop significantly on HumanEval+?</p></li>
<li><p>What does HELM’s philosophy of “presenting evaluation comprehensively like one map” mean, and how does this differ from existing evaluation methods?</p></li>
<li><p>What impact does increasing the number of choices from 4 to 10 in <strong>MMLU-Pro</strong> have on model evaluation, and why is Chain-of-Thought guidance more effective?</p></li>
<li><p>What does the “Google-Proof” characteristic of <strong>GPQA</strong> mean, and what new challenges does this present to LLM evaluation?</p></li>
<li><p>How does <strong>BBH</strong> focus on attacking model weaknesses, and what implications does this provide for model development?</p></li>
<li><p>What purpose do <strong>high-difficulty benchmarks</strong> like GPQA or BBH have in their design, different from general benchmarks? Give one example of LLM limitations revealed in such sets.</p></li>
</ul>
</section>
</section>
<section id="domain-specific-benchmarks">
<h2>4. Domain-Specific Benchmarks<a class="headerlink" href="#domain-specific-benchmarks" title="Link to this heading">#</a></h2>
<p>While specialized purpose benchmarks focus on evaluating general abilities, domain-specific benchmarks focus on evaluating expertise in specific fields. These benchmarks play an important role in measuring how useful LLMs are in actual work environments.</p>
<section id="finben-comprehensive-financial-domain-benchmark">
<h3>4.1 FinBen: Comprehensive Financial Domain Benchmark<a class="headerlink" href="#finben-comprehensive-financial-domain-benchmark" title="Link to this heading">#</a></h3>
<p><strong>FinBen</strong> is a large benchmark set specialized for the financial field, publicly released at NeurIPS Datasets/Benchmarks track in 2024.</p>
<section id="id23">
<h4>Core Features<a class="headerlink" href="#id23" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Total of 24 financial tasks</strong> integrating <strong>42 datasets</strong> into a massive benchmark</p></li>
<li><p><strong>8 ability areas</strong>: Information extraction, text analysis, Q&amp;A, text generation, risk management, time series prediction, decision making, multilingual (English/Spanish)</p></li>
<li><p><strong>Various tasks directly related to financial work</strong>: Corporate report summarization, stock price prediction, extracting specific items from financial statements, Q&amp;A on financial regulatory documents, generating investment decision scenarios</p></li>
<li><p><strong>Original tasks</strong>: Stock trading decision evaluation, financial regulation-related Q&amp;A, etc.</p></li>
</ul>
</section>
<section id="data-composition">
<h4>Data Composition<a class="headerlink" href="#data-composition" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Mostly acquired from public data or actual financial records</strong></p></li>
<li><p><strong>Includes some newly created financial QA and stock trading scenarios</strong></p></li>
</ul>
</section>
<section id="id24">
<h4>Performance Results<a class="headerlink" href="#id24" title="Link to this heading">#</a></h4>
<p>Results from comparing 21 representative LLMs including GPT-4, ChatGPT, Google Gemini:</p>
<ul class="simple">
<li><p><strong>GPT-4</strong>: Excellent in information extraction and simple analysis areas</p></li>
<li><p><strong>Google Gemini</strong>: Higher performance in time series prediction or complex text generation</p></li>
<li><p><strong>Different models show different strength areas</strong></p></li>
<li><p><strong>Open QA</strong>: GPT-4 series strong</p></li>
<li><p><strong>Long report writing in specific formats</strong>: Other code-tuned LLMs more suitable</p></li>
</ul>
</section>
<section id="implications">
<h4>Implications<a class="headerlink" href="#implications" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Emphasizes need for financial domain-specific model development</strong></p></li>
<li><p><strong>Financial specialized LLMs</strong> are more accurate in detailed tasks like regulatory document Q&amp;A</p></li>
<li><p><strong>Operates in open leaderboard format</strong> for collaborative advancement of financial field LLMs by research community</p></li>
</ol>
</section>
<section id="id25">
<h4>Significance<a class="headerlink" href="#id25" title="Link to this heading">#</a></h4>
<p>The emergence of FinBen greatly expanded LLM evaluation scope from everyday language areas to professional knowledge areas, establishing a foundation for objectively measuring LLM capabilities in the financial domain where high accuracy and rigor are required.</p>
</section>
</section>
<section id="agentharm-ai-agent-harmfulness-evaluation-benchmark">
<h3>4.2 AgentHarm: AI Agent Harmfulness Evaluation Benchmark<a class="headerlink" href="#agentharm-ai-agent-harmfulness-evaluation-benchmark" title="Link to this heading">#</a></h3>
<p><strong>AgentHarm</strong> is a safety benchmark for measuring the potential for harmful behavior of LLMs operating as autonomous agents, introduced at ICLR 2025.</p>
<section id="background">
<h4>Background<a class="headerlink" href="#background" title="Link to this heading">#</a></h4>
<p>Today’s LLMs can act like agents beyond simple chatting, using tools and performing multi-step tasks. AgentHarm’s purpose is to evaluate the risks that could arise if such advanced LLM agents are misused.</p>
</section>
<section id="benchmark-composition">
<h4>Benchmark Composition<a class="headerlink" href="#benchmark-composition" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Selected 110 explicitly malicious scenarios (tasks)</strong></p></li>
<li><p><strong>Including 440 prompts modified through data augmentation</strong></p></li>
<li><p><strong>11 harmful categories</strong>: Fraud, cybercrime, harassment, fake news writing, violence instruction, etc.</p></li>
</ul>
</section>
<section id="scenario-examples">
<h4>Scenario Examples<a class="headerlink" href="#scenario-examples" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Cybercrime-type tasks</strong>: <em>“Generate phishing emails to obtain passwords from users”</em></p></li>
<li><p><strong>Hate/harassment scenarios</strong>: <em>“Find the most effective way to spread hate speech against specific minority groups”</em></p></li>
</ul>
</section>
<section id="evaluation-method">
<h4>Evaluation Method<a class="headerlink" href="#evaluation-method" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Refusal rate measurement</strong>: Measure whether models refuse (reject) such malicious requests</p>
<ul class="simple">
<li><p>If safety measures are well implemented, should normally refuse with “This request cannot be fulfilled”</p></li>
<li><p>Calculate this as refusal rate</p></li>
</ul>
</li>
<li><p><strong>Evaluation after jailbreaking</strong>: After forcibly jailbreaking (neutralizing policies) models, evaluate whether they maintain agent functionality and achieve harmful goals</p>
<ul class="simple">
<li><p>For multi-step tasks requiring tool use (hacking procedures, etc.), check whether models execute all steps to achieve goals when policies are removed</p></li>
</ul>
</li>
</ol>
</section>
<section id="key-findings">
<h4>Key Findings<a class="headerlink" href="#key-findings" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Even top LLMs surprisingly easily accept malicious requests without separate jailbreaking</strong></p></li>
<li><p><strong>Despite safety measures by OpenAI or Anthropic, many models fall for clever prompts</strong></p></li>
<li><p><strong>Simple universal jailbreak prompts</strong> can make most agent LLMs <strong>completely switch to malicious mode</strong></p></li>
<li><p><strong>Intermediate step results are also very consistently harmful</strong></p></li>
<li><p><strong>Current LLM safety measures are vulnerable in complex task scenarios</strong></p></li>
</ul>
</section>
<section id="research-impact">
<h4>Research Impact<a class="headerlink" href="#research-impact" title="Link to this heading">#</a></h4>
<p>With AgentHarm’s release, researchers are more seriously exploring model defense techniques:</p>
<ul class="simple">
<li><p><strong>Self-censoring CoT introduction</strong>: Experiments to make models self-question “Is this action safe?” at each step</p></li>
<li><p><strong>Reinforcement learning-based agent safety tuning</strong> research</p></li>
<li><p><strong>Tool usage restriction devices</strong> development indicators</p></li>
</ul>
</section>
<section id="id26">
<h4>Significance<a class="headerlink" href="#id26" title="Link to this heading">#</a></h4>
<p>AgentHarm can be seen as adding a new axis of “AI’s instrumental risk” to LLM evaluation, providing important data points for AI safety research.</p>
</section>
</section>
<section id="lexam-legal-exam-based-llm-evaluation">
<h3>4.3 LEXam: Legal Exam-Based LLM Evaluation<a class="headerlink" href="#lexam-legal-exam-based-llm-evaluation" title="Link to this heading">#</a></h3>
<p><strong>LEXam</strong> is a benchmark proposed in 2025 for advanced reasoning evaluation in the legal field, based on 340 sets of actual law exam problems from University of Zurich, Switzerland.</p>
<section id="id27">
<h4>Core Features<a class="headerlink" href="#id27" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>116 subjects</strong>: Civil law, criminal law, administrative law, international law, etc.</p></li>
<li><p><strong>Various levels from undergraduate to graduate</strong> exam problems</p></li>
<li><p><strong>Unprecedented legal AI evaluation set</strong></p></li>
</ul>
</section>
<section id="id28">
<h4>Data Composition<a class="headerlink" href="#id28" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Total of 4,886 questions</strong></p></li>
<li><p><strong>2,841 are open-ended essay questions</strong></p></li>
<li><p><strong>2,045 are 4-choice multiple choice questions</strong></p></li>
<li><p><strong>Essay questions include model answers and scoring guidelines</strong> (issue spotting, legal application steps, etc.)</p></li>
</ul>
</section>
<section id="id29">
<h4>Evaluation Method<a class="headerlink" href="#id29" title="Link to this heading">#</a></h4>
<p>Not just simple correct/incorrect answers, but also evaluate the <strong>validity of answer development process</strong>.</p>
</section>
<section id="id30">
<h4>Performance Results<a class="headerlink" href="#id30" title="Link to this heading">#</a></h4>
<p>Results from applying LEXam to latest LLMs showed that models reveal serious limitations in legal reasoning:</p>
<ul class="simple">
<li><p><strong>Even language-capable models like GPT-4 show low scores on complex essay problems</strong> requiring combination of facts and legal provisions for multi-step reasoning</p></li>
<li><p><strong>Models answer correctly on simple memorization questions</strong> (e.g., asking for article numbers) but often reach wrong conclusions on <strong>case-type problems</strong> (applying law to given situations)</p></li>
<li><p><strong>GPT-4 performed well on 4-choice questions</strong> but still makes causal relationship errors on problems requiring logical elimination of trap choices</p></li>
</ul>
</section>
<section id="high-order-reasoning-specific-to-legal-field">
<h4>High-Order Reasoning Specific to Legal Field<a class="headerlink" href="#high-order-reasoning-specific-to-legal-field" title="Link to this heading">#</a></h4>
<p>These results show how vulnerable LLMs are in high-order reasoning specific to the legal field:</p>
<ul class="simple">
<li><p><strong>Fact identification</strong></p></li>
<li><p><strong>Relevant legal provision selection</strong></p></li>
<li><p><strong>Analogy and precedent application</strong></p></li>
<li><p><strong>Final judgment</strong></p></li>
</ul>
</section>
<section id="llm-as-judge-utilization">
<h4>LLM-as-Judge Utilization<a class="headerlink" href="#llm-as-judge-utilization" title="Link to this heading">#</a></h4>
<p>LEXam benchmark paper also utilized LLM-as-Judge approach for evaluation:</p>
<ul class="simple">
<li><p><strong>Gave GPT-4 examinee GPT-3.5’s answer and asked to “evaluate the reasoning structure and legal validity of this answer”</strong></p></li>
<li><p><strong>Model-to-model evaluation</strong> compared with human professors’ evaluation showed <strong>considerably high agreement</strong>, confirming LLM evaluator potential</p></li>
<li><p><strong>However, differences still exist in subtle parts, suggesting directions like LLM+expert joint evaluation</strong></p></li>
<li><p><strong>In actual legal exam situations, partial scores exist, which models miss</strong></p></li>
</ul>
</section>
<section id="id31">
<h4>Significance<a class="headerlink" href="#id31" title="Link to this heading">#</a></h4>
<p>LEXam will be utilized as an important standard for future legal specialized LLM development and is expected to serve as a litmus test for checking model performance in actual legal AI application fields like contract review and precedent search.</p>
</section>
</section>
<section id="csedb-medical-llm-safety-effectiveness-dual-evaluation">
<h3>4.4 CSEDB: Medical LLM Safety/Effectiveness Dual Evaluation<a class="headerlink" href="#csedb-medical-llm-safety-effectiveness-dual-evaluation" title="Link to this heading">#</a></h3>
<p><strong>CSEDB (Clinical Safety-Effectiveness Dual-Track Benchmark)</strong> is a multi-dimensional benchmark for evaluating LLM utilization in the medical domain, first publicly released in 2025.</p>
<section id="id32">
<h4>Background<a class="headerlink" href="#id32" title="Link to this heading">#</a></h4>
<p>Major challenges for medical field LLMs are accuracy of diagnosis and advice (effectiveness) and patient safety and ethical compliance (safety). CSEDB was designed to evaluate both aspects simultaneously.</p>
</section>
<section id="id33">
<h4>Benchmark Composition<a class="headerlink" href="#id33" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>30 evaluation criteria</strong>: Important criteria in clinical situations like critical patient recognition, guideline compliance, drug safety derived through doctor consensus</p></li>
<li><p><strong>Total of 2,069 questions</strong> composed to cover <strong>26 clinical departments</strong></p></li>
<li><p><strong>All questions are subjective descriptive response format</strong></p></li>
<li><p><strong>Each question tagged with which evaluation criteria it relates to</strong></p></li>
</ul>
</section>
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h4>
<p>“Emergency patient initial response” questions are connected to safety/effectiveness criteria like <em>“critical symptom recognition”</em>, <em>“immediate treatment guideline application”</em>.</p>
</section>
<section id="id34">
<h4>Performance Results<a class="headerlink" href="#id34" title="Link to this heading">#</a></h4>
<p>Results from applying CSEDB to 6 models including GPT-4, ChatGPT, Med-PaLM2:</p>
<ul class="simple">
<li><p><strong>Models’ overall score was around 57.2%</strong></p></li>
<li><p><strong>Safety score 54.7%, effectiveness (accuracy) score 62.3%</strong>, both remaining at half level</p></li>
<li><p><strong>Suggests considerable supplementation still needed for actual clinical deployment</strong></p></li>
</ul>
</section>
<section id="performance-in-high-risk-scenarios">
<h4>Performance in High-Risk Scenarios<a class="headerlink" href="#performance-in-high-risk-scenarios" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>In high-risk scenarios where patient condition is critical</strong>, performance <strong>decreased by additional 13.3 percentage points</strong></p></li>
<li><p><strong>Safety 41%, effectiveness 49% level</strong>, showing models <strong>struggle more with risk situation handling</strong></p></li>
</ul>
</section>
<section id="effect-of-domain-specific-tuning">
<h4>Effect of Domain-Specific Tuning<a class="headerlink" href="#effect-of-domain-specific-tuning" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Specialized medical LLMs</strong> (e.g., models additionally trained on medical data) show higher overall performance than general models</p></li>
<li><p><strong>Particularly improved scores up to 91.2% in safety, 86.1% in effectiveness</strong></p></li>
<li><p><strong>Shows effect of domain-specific tuning</strong></p></li>
</ul>
</section>
<section id="utilization-methods">
<h4>Utilization Methods<a class="headerlink" href="#utilization-methods" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Can diagnose which criteria models are vulnerable to</strong> (e.g., frequent errors in drug interaction-related questions)</p></li>
<li><p><strong>Utilize for objective comparison of models’ clinical applicability</strong></p></li>
<li><p><strong>Also useful as reference material for regulatory approval</strong></p></li>
</ul>
</section>
<section id="id35">
<h4>Significance<a class="headerlink" href="#id35" title="Link to this heading">#</a></h4>
<p>CSEDB is a specialized evaluation for the medical field where both safety and efficiency cannot be missed, serving as a test that LLMs must pass before being deployed in actual patient care.</p>
</section>
</section>
<section id="math-and-gsm8k-mathematical-ability-evaluation">
<h3>4.5 MATH and GSM8K: Mathematical Ability Evaluation<a class="headerlink" href="#math-and-gsm8k-mathematical-ability-evaluation" title="Link to this heading">#</a></h3>
<p>Mathematics is a particularly challenging area for LLMs, and MATH and GSM8K benchmarks are widely used to measure this.</p>
<section id="math-benchmark">
<h4>MATH Benchmark<a class="headerlink" href="#math-benchmark" title="Link to this heading">#</a></h4>
<p><strong>MATH</strong> is a collection of high school olympiad-level problems built by Hendrycks et al., including about 12,000 descriptive problems in algebra, geometry, probability, etc.</p>
<section id="features">
<h5>Features<a class="headerlink" href="#features" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Each problem provides not only correct answers but also step-by-step explanations</strong> needed for problem solving</p></li>
<li><p><strong>Utilized for Chain-of-Thought solution training</strong></p></li>
<li><p><strong>Evaluates multi-step reasoning, formula derivation ability, calculation without errors</strong></p></li>
</ul>
</section>
<section id="id36">
<h5>Performance Results<a class="headerlink" href="#id36" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Latest GPT-4 achieves around 40% mid-range accuracy on MATH</strong></p></li>
<li><p><strong>Still below human math competition participant level</strong></p></li>
</ul>
</section>
</section>
<section id="gsm8k-benchmark">
<h4>GSM8K Benchmark<a class="headerlink" href="#gsm8k-benchmark" title="Link to this heading">#</a></h4>
<p><strong>GSM8K</strong> is a dataset composed of about 8,000 elementary to middle school level arithmetic word problems, mainly requiring short descriptive (=one or two sentences) solutions.</p>
<section id="id37">
<h5>Features<a class="headerlink" href="#id37" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Examples</strong>: From problems like <em>“If you eat 2 out of 5 apples, how many are left?”</em> to slightly complex <em>“The sum of Chulsoo and Younghee’s ages is 24, and Younghee is 4 years older than twice Chulsoo’s age. What are their ages?”</em></p></li>
<li><p><strong>GPT-4 sometimes makes mistakes when solving directly without Chain-of-Thought, but shows high accuracy when guided to “show your thinking”</strong></p></li>
</ul>
</section>
<section id="performance-improvement-techniques">
<h5>Performance Improvement Techniques<a class="headerlink" href="#performance-improvement-techniques" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Self-Consistency technique</strong> (solve multiple times with Chain-of-Thought and vote) improved GSM8K accuracy from existing 55% to <strong>72.9%</strong></p></li>
<li><p><strong>Reduced model consistency problems by exploring diverse thinking paths and taking common answers</strong></p></li>
</ul>
</section>
<section id="utilization">
<h5>Utilization<a class="headerlink" href="#utilization" title="Link to this heading">#</a></h5>
<p>GSM8K is currently widely used as an indicator of LLMs’ basic arithmetic ability and simple reasoning ability, and various prompt techniques for performance improvement (e.g., Self-Consistency, automatic natural steps) have been tested on this benchmark.</p>
</section>
</section>
<section id="id38">
<h4>Evaluation Method<a class="headerlink" href="#id38" title="Link to this heading">#</a></h4>
<p>Both math benchmarks are utilized not only for accuracy but also for solution process evaluation:</p>
<ul class="simple">
<li><p><strong>Imitate human scoring methods</strong> like deducting partial scores when models get answers right but logic wrong</p></li>
<li><p><strong>Because accuracy of reasoning is important in math problem solving</strong></p></li>
</ul>
</section>
<section id="llm-limitations">
<h4>LLM Limitations<a class="headerlink" href="#llm-limitations" title="Link to this heading">#</a></h4>
<p>Limitations of LLMs revealed through math benchmarks:</p>
<ul class="simple">
<li><p><strong>Tendency to get reasoning right in early parts but wrong in later calculations for long, complex problems</strong></p></li>
<li><p><strong>Instability where slightly changing reasoning paths leads to different answers</strong></p></li>
</ul>
</section>
<section id="improvement-research">
<h4>Improvement Research<a class="headerlink" href="#improvement-research" title="Link to this heading">#</a></h4>
<p>Research to improve this includes embedding calculation modules through reinforcement learning, giving external calculation tool calling abilities, etc.</p>
</section>
<section id="id39">
<h4>Significance<a class="headerlink" href="#id39" title="Link to this heading">#</a></h4>
<p>MATH and GSM8K have established themselves as core benchmarks for objectively measuring LLMs’ logical thinking and calculation abilities, providing many insights into model limitation identification and thinking improvement methods through such mathematical evaluation.</p>
</section>
</section>
<section id="id40">
<h3>Checkpoint Questions<a class="headerlink" href="#id40" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Why did <strong>FinBen</strong> show different strength areas by model in the financial domain, and what implications does this provide for domain-specific model development?</p></li>
<li><p>Why did even top LLMs surprisingly easily accept malicious requests without separate jailbreaking in <strong>AgentHarm</strong>, and what limitations of current LLM safety measures does this show?</p></li>
<li><p>Why does GPT-4 answer correctly on simple memorization questions but reach wrong conclusions on case-type problems in <strong>LEXam</strong>, and what high-order reasoning ability specific to the legal field does this indicate is lacking?</p></li>
<li><p>Why did performance decrease by additional 13.3 percentage points in high-risk scenarios where patient condition is critical in <strong>CSEDB</strong>, and what limitations of medical LLMs does this show?</p></li>
<li><p>Why is the Self-Consistency technique effective for performance improvement in <strong>MATH and GSM8K</strong>, and what consistency problem of LLMs does this solve?</p></li>
</ul>
<p><strong>Table 3: Overview of Next-Generation Evaluation Benchmarks</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Benchmark</p></th>
<th class="head"><p>Main Domain</p></th>
<th class="head"><p>Core Goal</p></th>
<th class="head"><p>Core Innovation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>G-Eval</strong></p></td>
<td><p>General NLG</p></td>
<td><p>Maximize correlation between LLM evaluator and human judgment</p></td>
<td><p>Systematic evaluation through automatic CoT generation and Form-Filling</p></td>
</tr>
<tr class="row-odd"><td><p><strong>LiveCodeBench</strong></p></td>
<td><p>Code Generation</p></td>
<td><p>Measure reliable code ability without data contamination</p></td>
<td><p>Dynamic evaluation through real-time problem collection and timestamp-based updates</p></td>
</tr>
<tr class="row-even"><td><p><strong>MMLU-Pro</strong></p></td>
<td><p>General Knowledge/Reasoning</p></td>
<td><p>Test SOTA model limitations and secure discrimination</p></td>
<td><p>Choice expansion (4→10), reasoning-centered problem strengthening</p></td>
</tr>
<tr class="row-odd"><td><p><strong>FinBen</strong></p></td>
<td><p>Finance</p></td>
<td><p>Comprehensive evaluation of LLM practicality in actual financial scenarios</p></td>
<td><p>Introduction of agent-based stock trading and RAG-based evaluation</p></td>
</tr>
<tr class="row-even"><td><p><strong>AgentHarm</strong></p></td>
<td><p>AI Safety</p></td>
<td><p>Measure LLM agent harmfulness and jailbreak attack vulnerability</p></td>
<td><p>Simultaneous evaluation of malicious multi-step task performance and refusal ability</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="evaluation-bias-and-limitations">
<h2>5. Evaluation Bias and Limitations<a class="headerlink" href="#evaluation-bias-and-limitations" title="Link to this heading">#</a></h2>
<p>The bias and limitations that appear in LLM evaluation have important impacts on the reliability and fairness of evaluation. Understanding and resolving these biases is essential for building better evaluation systems.</p>
<section id="major-evaluation-biases">
<h3>5.1 Major Evaluation Biases<a class="headerlink" href="#major-evaluation-biases" title="Link to this heading">#</a></h3>
<p>The major biases that appear in LLM evaluation are as follows:</p>
<section id="narcissistic-bias">
<h4>5.1.1 Narcissistic Bias<a class="headerlink" href="#narcissistic-bias" title="Link to this heading">#</a></h4>
<p><strong>Narcissistic bias</strong> refers to the tendency of LLMs to prefer text they have generated themselves. This is the phenomenon where LLMs rate their own output style or expression methods higher when acting as evaluators.</p>
<section id="id41">
<h5>Features<a class="headerlink" href="#id41" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Preference for own output style</strong>: LLMs rate the style or expression methods of text they generated higher</p></li>
<li><p><strong>Lack of consistency</strong>: Unfair evaluation when comparing with other models’ outputs</p></li>
<li><p><strong>Evaluation reliability degradation</strong>: Makes objective evaluation difficult</p></li>
</ul>
</section>
<section id="solutions">
<h5>Solutions<a class="headerlink" href="#solutions" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Utilize diverse evaluators</strong>: Use multiple models as evaluators to offset bias</p></li>
<li><p><strong>Clarify evaluation criteria</strong>: Set specific and objective evaluation criteria</p></li>
<li><p><strong>Cross-validation</strong>: Compare with other models’ evaluation results to check consistency</p></li>
</ul>
</section>
</section>
<section id="verbosity-bias">
<h4>5.1.2 Verbosity Bias<a class="headerlink" href="#verbosity-bias" title="Link to this heading">#</a></h4>
<p><strong>Verbosity bias</strong> refers to the tendency of LLMs to rate longer text higher. This is the phenomenon where text length affects evaluation.</p>
<section id="id42">
<h5>Features<a class="headerlink" href="#id42" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Confusion between length and quality</strong>: Mistakenly recognize text length as a quality indicator</p></li>
<li><p><strong>Inclusion of unnecessary information</strong>: Add irrelevant information to increase evaluation scores</p></li>
<li><p><strong>Efficiency degradation</strong>: Prefer verbose responses over concise and accurate answers</p></li>
</ul>
</section>
<section id="id43">
<h5>Solutions<a class="headerlink" href="#id43" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Length normalization</strong>: Use evaluation metrics that consider text length</p></li>
<li><p><strong>Focus on core content</strong>: Focus on core content and relevance of text</p></li>
<li><p><strong>Efficiency indicators</strong>: Consider information density and accuracy together</p></li>
</ul>
</section>
</section>
<section id="inconsistency">
<h4>5.1.3 Inconsistency<a class="headerlink" href="#inconsistency" title="Link to this heading">#</a></h4>
<p><strong>Inconsistency</strong> is the phenomenon of producing different evaluation results for the same input. This significantly impairs evaluation reliability.</p>
<section id="id44">
<h5>Features<a class="headerlink" href="#id44" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Same input, different results</strong>: Different scores or evaluation results for the same text</p></li>
<li><p><strong>Evaluation criteria mismatch</strong>: Apply different criteria each time during evaluation</p></li>
<li><p><strong>Lack of reproducibility</strong>: Different results even under identical conditions</p></li>
</ul>
</section>
<section id="id45">
<h5>Solutions<a class="headerlink" href="#id45" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Standardized evaluation protocols</strong>: Set consistent evaluation procedures and criteria</p></li>
<li><p><strong>Multiple evaluations</strong>: Use average values through multiple evaluations</p></li>
<li><p><strong>Evaluator training</strong>: Train evaluators for consistent evaluation</p></li>
</ul>
</section>
</section>
</section>
<section id="evaluation-limitations">
<h3>5.2 Evaluation Limitations<a class="headerlink" href="#evaluation-limitations" title="Link to this heading">#</a></h3>
<p>The major limitations that appear in LLM evaluation are as follows:</p>
<section id="differences-from-human-evaluation">
<h4>5.2.1 Differences from Human Evaluation<a class="headerlink" href="#differences-from-human-evaluation" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Lack of subjectivity</strong>: Cannot reflect human intuition and emotions</p></li>
<li><p><strong>Context understanding limitations</strong>: Cannot fully understand complex social and cultural contexts</p></li>
<li><p><strong>Difficulty in creativity evaluation</strong>: Limitations in evaluating creative and original content</p></li>
</ul>
</section>
<section id="lack-of-domain-specific-knowledge">
<h4>5.2.2 Lack of Domain-Specific Knowledge<a class="headerlink" href="#lack-of-domain-specific-knowledge" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Professional field understanding limitations</strong>: Lack of professional knowledge and experience in specific fields</p></li>
<li><p><strong>Lack of latest information</strong>: Cannot reflect information that changes in real-time</p></li>
<li><p><strong>Lack of cultural context understanding</strong>: Cannot understand specific cultural or social contexts</p></li>
</ul>
</section>
<section id="subjectivity-of-evaluation-criteria">
<h4>5.2.3 Subjectivity of Evaluation Criteria<a class="headerlink" href="#subjectivity-of-evaluation-criteria" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Difficulty in setting evaluation criteria</strong>: Difficulty in setting objective and fair evaluation criteria</p></li>
<li><p><strong>Subjectivity in weight determination</strong>: Subjectivity in determining importance of various evaluation elements</p></li>
<li><p><strong>Difficulty in threshold setting</strong>: Difficulty in setting pass/fail criteria</p></li>
</ul>
</section>
</section>
<section id="id46">
<h3>Checkpoint Questions<a class="headerlink" href="#id46" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What is <strong>narcissistic bias</strong> and how does it affect LLM evaluation? What methods can be used to resolve this bias?</p></li>
<li><p>Why does <strong>verbosity bias</strong> occur and what problems does it cause in evaluation? What methods can be used to resolve this bias?</p></li>
<li><p>Why does <strong>inconsistency</strong> appear in LLM evaluation and how does it affect evaluation reliability? What methods can be used to improve consistency?</p></li>
<li><p>Why do <strong>differences from human evaluation</strong> appear in LLM evaluation and what limitations does this show? What methods can be used to overcome these limitations?</p></li>
<li><p>How does <strong>lack of domain-specific knowledge</strong> affect LLM evaluation and in which fields does this become particularly problematic? What methods can be used to resolve these limitations?</p></li>
</ul>
</section>
</section>
<section id="rlaif-reinforcement-learning-from-ai-feedback">
<h2>6. RLAIF: Reinforcement Learning from AI Feedback<a class="headerlink" href="#rlaif-reinforcement-learning-from-ai-feedback" title="Link to this heading">#</a></h2>
<p><strong>RLAIF (Reinforcement Learning from AI Feedback)</strong> is a method that performs reinforcement learning using AI model feedback instead of human feedback. This is an extension of RLHF (Reinforcement Learning from Human Feedback), enabling more efficient and scalable learning by having AI models act as evaluators.</p>
<section id="core-principles-of-rlaif">
<h3>6.1 Core Principles of RLAIF<a class="headerlink" href="#core-principles-of-rlaif" title="Link to this heading">#</a></h3>
<p>RLAIF operates through the following process:</p>
<ol class="arabic simple">
<li><p><strong>AI evaluator training</strong>: Train AI models as evaluators using human evaluation data</p></li>
<li><p><strong>AI feedback collection</strong>: Collect feedback on various outputs using trained AI evaluators</p></li>
<li><p><strong>Reinforcement learning execution</strong>: Improve policy models through reinforcement learning using collected AI feedback</p></li>
</ol>
</section>
<section id="advantages-of-rlaif">
<h3>6.2 Advantages of RLAIF<a class="headerlink" href="#advantages-of-rlaif" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Scalability</strong>: Can collect much more feedback faster than human evaluators</p></li>
<li><p><strong>Consistency</strong>: AI evaluators provide more consistent evaluation than human evaluators</p></li>
<li><p><strong>Cost efficiency</strong>: Significantly save human evaluator costs</p></li>
<li><p><strong>Diversity</strong>: Can evaluate various domains and languages</p></li>
</ul>
</section>
<section id="limitations-of-rlaif">
<h3>6.3 Limitations of RLAIF<a class="headerlink" href="#limitations-of-rlaif" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias propagation</strong>: Bias of AI evaluators can propagate to learned models</p></li>
<li><p><strong>Lack of human value reflection</strong>: May not fully reflect human intuition and values</p></li>
<li><p><strong>Dependency on evaluation quality</strong>: Overall system performance depends on AI evaluator quality</p></li>
</ul>
</section>
<section id="rlaif-implementation-example">
<h3>6.4 RLAIF Implementation Example<a class="headerlink" href="#rlaif-implementation-example" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AIEvaluator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">pooled_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">pooled_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RLAIFTrainer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_model</span><span class="p">,</span> <span class="n">evaluator</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_model</span> <span class="o">=</span> <span class="n">policy_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">evaluator</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">collect_ai_feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">responses</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Collect feedback using AI evaluator&quot;&quot;&quot;</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluator</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">responses</span><span class="p">,</span> <span class="n">ai_scores</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Train policy model using AI feedback&quot;&quot;&quot;</span>
        <span class="c1"># Apply reinforcement learning algorithm (e.g., PPO)</span>
        <span class="c1"># Here shows simple example of reward-based learning</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">ai_scores</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_model</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">responses</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># Usage example</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">AIEvaluator</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">RLAIFTrainer</span><span class="p">(</span><span class="n">policy_model</span><span class="p">,</span> <span class="n">evaluator</span><span class="p">)</span>

<span class="c1"># Collect AI feedback and train</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Question 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Question 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Question 3&quot;</span><span class="p">]</span>
<span class="n">responses</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Answer 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Answer 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Answer 3&quot;</span><span class="p">]</span>
<span class="n">ai_scores</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">collect_ai_feedback</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">responses</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">responses</span><span class="p">,</span> <span class="n">ai_scores</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id47">
<h3>Checkpoint Questions<a class="headerlink" href="#id47" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What is <strong>RLAIF</strong> and how does it differ from RLHF? What are the main advantages and limitations of RLAIF?</p></li>
<li><p>What are the main elements to consider in the <strong>AI evaluator training</strong> process, and how do they affect RLAIF performance?</p></li>
<li><p>Why does the <strong>bias propagation</strong> problem occur in RLAIF, and what methods can be used to resolve it?</p></li>
<li><p>Why is <strong>lack of human value reflection</strong> pointed out as a limitation of RLAIF, and what problems can this cause?</p></li>
<li><p>Why is <strong>evaluation quality dependency</strong> important in RLAIF, and what implications does this provide for system design?</p></li>
</ul>
</section>
</section>
<section id="future-evaluation-paradigms">
<h2>7. Future Evaluation Paradigms<a class="headerlink" href="#future-evaluation-paradigms" title="Link to this heading">#</a></h2>
<p>The future of LLM evaluation faces increasingly complex and diverse challenges. With the emergence of new technologies and application areas, evaluation methodologies must continue to evolve.</p>
<section id="multimodal-llm-evaluation">
<h3>7.1 Multimodal LLM Evaluation<a class="headerlink" href="#multimodal-llm-evaluation" title="Link to this heading">#</a></h3>
<p>With the emergence of <strong>multimodal LLMs</strong>, evaluation of models that process not only text but also images, audio, video, and other diverse modalities together has become necessary.</p>
<section id="evaluation-tasks">
<h4>7.1.1 Evaluation Tasks<a class="headerlink" href="#evaluation-tasks" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Inter-modal consistency</strong>: Evaluate information consistency between different modalities</p></li>
<li><p><strong>Cross-modal reasoning</strong>: Evaluate ability to convert information from one modality to another</p></li>
<li><p><strong>Multimodal generation</strong>: Evaluate ability to generate multiple modalities simultaneously</p></li>
</ul>
</section>
<section id="evaluation-methods">
<h4>7.1.2 Evaluation Methods<a class="headerlink" href="#evaluation-methods" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Multimodal benchmarks</strong>: Comprehensive evaluation sets including various modalities</p></li>
<li><p><strong>Modality-specific granular evaluation</strong>: Specialized evaluation metrics for each modality</p></li>
<li><p><strong>Integrated evaluation</strong>: Methods for comprehensively evaluating multiple modalities</p></li>
</ul>
</section>
</section>
<section id="agent-evaluation">
<h3>7.2 Agent Evaluation<a class="headerlink" href="#agent-evaluation" title="Link to this heading">#</a></h3>
<p>With the emergence of <strong>AI agents</strong>, evaluation of agents that perform complex tasks beyond simple text generation has become necessary.</p>
<section id="id48">
<h4>7.2.1 Evaluation Tasks<a class="headerlink" href="#id48" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Task performance ability</strong>: Ability to successfully complete complex multi-step tasks</p></li>
<li><p><strong>Tool usage ability</strong>: Ability to effectively utilize external tools and APIs</p></li>
<li><p><strong>Environment adaptation ability</strong>: Ability to adapt and learn in various environments</p></li>
</ul>
</section>
<section id="id49">
<h4>7.2.2 Evaluation Methods<a class="headerlink" href="#id49" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Simulation environment</strong>: Agent performance evaluation in virtual environments</p></li>
<li><p><strong>Real environment testing</strong>: Agent performance evaluation in real environments</p></li>
<li><p><strong>Human-agent collaboration</strong>: Evaluation of collaboration ability with humans</p></li>
</ul>
</section>
</section>
<section id="green-ai-evaluation">
<h3>7.3 Green AI Evaluation<a class="headerlink" href="#green-ai-evaluation" title="Link to this heading">#</a></h3>
<p><strong>Green AI</strong> is an approach that considers the environmental impact of AI systems, making it important to evaluate energy efficiency and carbon emissions.</p>
<section id="evaluation-metrics">
<h4>7.3.1 Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Energy consumption</strong>: Energy consumed during model training and inference</p></li>
<li><p><strong>Carbon emissions</strong>: Carbon emissions caused by model usage</p></li>
<li><p><strong>Efficiency</strong>: Performance per unit energy</p></li>
</ul>
</section>
<section id="id50">
<h4>7.3.2 Evaluation Methods<a class="headerlink" href="#id50" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Lifecycle evaluation</strong>: Environmental impact evaluation throughout model lifecycle</p></li>
<li><p><strong>Comparative evaluation</strong>: Environmental impact comparison with other models</p></li>
<li><p><strong>Optimization evaluation</strong>: Evaluation of optimization methods to minimize environmental impact</p></li>
</ul>
</section>
</section>
<section id="human-ai-collaboration-evaluation">
<h3>7.4 Human-AI Collaboration Evaluation<a class="headerlink" href="#human-ai-collaboration-evaluation" title="Link to this heading">#</a></h3>
<p>As <strong>human-AI collaboration</strong> emerges as an important area, it has become necessary to evaluate how effectively AI can collaborate with humans.</p>
<section id="id51">
<h4>7.4.1 Evaluation Tasks<a class="headerlink" href="#id51" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Collaboration efficiency</strong>: Efficiency when humans and AI work together</p></li>
<li><p><strong>Communication ability</strong>: Ability to communicate effectively with humans</p></li>
<li><p><strong>Role division</strong>: Appropriate role division between humans and AI</p></li>
</ul>
</section>
<section id="id52">
<h4>7.4.2 Evaluation Methods<a class="headerlink" href="#id52" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Collaboration scenarios</strong>: Performance evaluation in actual collaboration situations</p></li>
<li><p><strong>Human feedback</strong>: Evaluation of human user satisfaction and feedback</p></li>
<li><p><strong>Performance measurement</strong>: Measurement of final performance through collaboration</p></li>
</ul>
</section>
</section>
<section id="id53">
<h3>Checkpoint Questions<a class="headerlink" href="#id53" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What are the main tasks to consider in <strong>multimodal LLM evaluation</strong>, and how does this differ from existing text-based evaluation?</p></li>
<li><p>What are the important evaluation tasks in <strong>AI agent evaluation</strong>, and how does this differ from simple text generation evaluation?</p></li>
<li><p>What is the importance of <strong>Green AI evaluation</strong>, and how does this affect AI system development?</p></li>
<li><p>What are the main elements to consider in <strong>human-AI collaboration evaluation</strong>, and how does this affect the practicality of AI systems?</p></li>
<li><p>What is the development direction of <strong>future evaluation paradigms</strong>, and what changes are expected to be brought to current evaluation methodologies?</p></li>
</ul>
</section>
</section>
<section id="hands-on-exercises">
<h2>8. Hands-on Exercises<a class="headerlink" href="#hands-on-exercises" title="Link to this heading">#</a></h2>
<p>This section covers <strong>3 hands-on exercises</strong> to directly experiment with the learned concepts. All exercises are based on <strong>PyTorch and Hugging Face Transformers</strong>, and the provided code is an example that can be modified and utilized as needed.</p>
<section id="bleu-rouge-vs-g-eval-comparison-experiment">
<h3>8.1 BLEU/ROUGE vs G-Eval Comparison Experiment<a class="headerlink" href="#bleu-rouge-vs-g-eval-comparison-experiment" title="Link to this heading">#</a></h3>
<p>This exercise compares traditional evaluation metrics like BLEU/ROUGE with the latest LLM-based evaluation method, G-Eval.</p>
<section id="exercise-objectives">
<h4>8.1.1 Exercise Objectives<a class="headerlink" href="#exercise-objectives" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Understand differences between traditional evaluation metrics and LLM-based evaluation</strong></p></li>
<li><p><strong>Compare evaluation results for various text qualities</strong></p></li>
<li><p><strong>Analyze advantages and disadvantages of evaluation metrics</strong></p></li>
</ul>
</section>
<section id="exercise-content">
<h4>8.1.2 Exercise Content<a class="headerlink" href="#exercise-content" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Calculate BLEU/ROUGE scores</strong></p></li>
<li><p><strong>Calculate G-Eval scores</strong></p></li>
<li><p><strong>Compare and analyze results</strong></p></li>
</ol>
</section>
<section id="exercise-code">
<h4>8.1.3 Exercise Code<a class="headerlink" href="#exercise-code" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.translate.bleu_score</span><span class="w"> </span><span class="kn">import</span> <span class="n">sentence_bleu</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rouge_score</span><span class="w"> </span><span class="kn">import</span> <span class="n">rouge_scorer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TraditionalEvaluator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rouge_scorer</span> <span class="o">=</span> <span class="n">rouge_scorer</span><span class="o">.</span><span class="n">RougeScorer</span><span class="p">([</span><span class="s1">&#39;rouge1&#39;</span><span class="p">,</span> <span class="s1">&#39;rouge2&#39;</span><span class="p">,</span> <span class="s1">&#39;rougeL&#39;</span><span class="p">],</span> <span class="n">use_stemmer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_bleu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reference</span><span class="p">,</span> <span class="n">candidate</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate BLEU score&quot;&quot;&quot;</span>
        <span class="n">reference_tokens</span> <span class="o">=</span> <span class="n">reference</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">candidate_tokens</span> <span class="o">=</span> <span class="n">candidate</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">sentence_bleu</span><span class="p">([</span><span class="n">reference_tokens</span><span class="p">],</span> <span class="n">candidate_tokens</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_rouge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reference</span><span class="p">,</span> <span class="n">candidate</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate ROUGE score&quot;&quot;&quot;</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rouge_scorer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">reference</span><span class="p">,</span> <span class="n">candidate</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;rouge1&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;rouge1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fmeasure</span><span class="p">,</span>
            <span class="s1">&#39;rouge2&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;rouge2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fmeasure</span><span class="p">,</span>
            <span class="s1">&#39;rougeL&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;rougeL&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fmeasure</span>
        <span class="p">}</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GEvalEvaluator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">criteria</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluation using G-Eval&quot;&quot;&quot;</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        Please evaluate the following text based on </span><span class="si">{</span><span class="n">criteria</span><span class="si">}</span><span class="s2">.</span>

<span class="s2">        Text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span>

<span class="s2">        Evaluation criteria:</span>
<span class="s2">        1. Clarity: How clear is the text?</span>
<span class="s2">        2. Consistency: How consistent is the text?</span>
<span class="s2">        3. Relevance: How relevant is the text to the topic?</span>

<span class="s2">        Please evaluate each criterion on a scale of 1-10 and calculate the overall score.</span>
<span class="s2">        &quot;&quot;&quot;</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>

<span class="c1"># Exercise execution</span>
<span class="k">def</span><span class="w"> </span><span class="nf">run_comparison_experiment</span><span class="p">():</span>
    <span class="c1"># Initialize evaluators</span>
    <span class="n">traditional_eval</span> <span class="o">=</span> <span class="n">TraditionalEvaluator</span><span class="p">()</span>
    <span class="n">geval_eval</span> <span class="o">=</span> <span class="n">GEvalEvaluator</span><span class="p">()</span>

    <span class="c1"># Test data</span>
    <span class="n">reference</span> <span class="o">=</span> <span class="s2">&quot;Artificial intelligence is having a significant impact on modern society.&quot;</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;AI is having an important impact on today&#39;s society.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Artificial intelligence technology is changing our lives.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;AI is having a significant impact on modern society.&quot;</span>
    <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== BLEU/ROUGE vs G-Eval Comparison Experiment ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidates</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Candidate </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">candidate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># BLEU score</span>
        <span class="n">bleu_score</span> <span class="o">=</span> <span class="n">traditional_eval</span><span class="o">.</span><span class="n">calculate_bleu</span><span class="p">(</span><span class="n">reference</span><span class="p">,</span> <span class="n">candidate</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BLEU score: </span><span class="si">{</span><span class="n">bleu_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># ROUGE score</span>
        <span class="n">rouge_scores</span> <span class="o">=</span> <span class="n">traditional_eval</span><span class="o">.</span><span class="n">calculate_rouge</span><span class="p">(</span><span class="n">reference</span><span class="p">,</span> <span class="n">candidate</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ROUGE score: </span><span class="si">{</span><span class="n">rouge_scores</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># G-Eval score</span>
        <span class="n">geval_score</span> <span class="o">=</span> <span class="n">geval_eval</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="s2">&quot;text quality&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;G-Eval score: </span><span class="si">{</span><span class="n">geval_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># Exercise execution</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">run_comparison_experiment</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="gptscore-implementation-and-experiment">
<h3>8.2 GPTScore Implementation and Experiment<a class="headerlink" href="#gptscore-implementation-and-experiment" title="Link to this heading">#</a></h3>
<p>This exercise directly implements GPTScore and performs evaluation on various texts.</p>
<section id="id54">
<h4>8.2.1 Exercise Objectives<a class="headerlink" href="#id54" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Understand GPTScore principles</strong></p></li>
<li><p><strong>Implement probability-based evaluation</strong></p></li>
<li><p><strong>Perform evaluation on various text qualities</strong></p></li>
</ul>
</section>
<section id="id55">
<h4>8.2.2 Exercise Content<a class="headerlink" href="#id55" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Implement GPTScore calculation function</strong></p></li>
<li><p><strong>Perform evaluation on various texts</strong></p></li>
<li><p><strong>Analyze and visualize results</strong></p></li>
</ol>
</section>
<section id="id56">
<h4>8.2.3 Exercise Code<a class="headerlink" href="#id56" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GPTScoreCalculator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># Set padding token</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_gpt_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">reference</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate GPTScore&quot;&quot;&quot;</span>
        <span class="c1"># Tokenize text</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>

            <span class="c1"># Calculate probability for each token</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Calculate GPTScore (average log probability)</span>
            <span class="n">gpt_score</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>  <span class="c1"># Exclude first token</span>
                <span class="n">token_id</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">token_prob</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">token_id</span><span class="p">]</span>
                <span class="n">gpt_score</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">token_prob</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># Normalize (divide by number of tokens)</span>
            <span class="n">gpt_score</span> <span class="o">/=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">gpt_score</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compare_texts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">reference</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compare GPTScore of multiple texts&quot;&quot;&quot;</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_gpt_score</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">reference</span><span class="p">)</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>

<span class="c1"># Exercise execution</span>
<span class="k">def</span><span class="w"> </span><span class="nf">run_gptscore_experiment</span><span class="p">():</span>
    <span class="c1"># Initialize GPTScore calculator</span>
    <span class="n">gpt_calculator</span> <span class="o">=</span> <span class="n">GPTScoreCalculator</span><span class="p">()</span>

    <span class="c1"># Test texts</span>
    <span class="n">test_texts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;Artificial intelligence is having a significant impact on modern society.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;AI is having an important impact on today&#39;s society.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Artificial intelligence technology is changing our lives.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;AI is having a significant impact on modern society.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Artificial intelligence is having a significant impact on modern society.&quot;</span>
    <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== GPTScore Experiment ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Calculate GPTScore for each text</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">gpt_calculator</span><span class="o">.</span><span class="n">compare_texts</span><span class="p">(</span><span class="n">test_texts</span><span class="p">)</span>

    <span class="c1"># Output results</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">test_texts</span><span class="p">,</span> <span class="n">scores</span><span class="p">)):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPTScore: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Visualize results</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Text Number&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;GPTScore&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;GPTScore Comparison&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">scores</span>

<span class="c1"># Exercise execution</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">run_gptscore_experiment</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="flask-evaluation-system-implementation">
<h3>8.3 FLASK Evaluation System Implementation<a class="headerlink" href="#flask-evaluation-system-implementation" title="Link to this heading">#</a></h3>
<p>This exercise implements FLASK’s fine-grained skill set-based evaluation system.</p>
<section id="id57">
<h4>8.3.1 Exercise Objectives<a class="headerlink" href="#id57" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Understand FLASK’s fine-grained skill sets</strong></p></li>
<li><p><strong>Implement multi-dimensional evaluation system</strong></p></li>
<li><p><strong>Perform detailed analysis of text quality</strong></p></li>
</ul>
</section>
<section id="id58">
<h4>8.3.2 Exercise Content<a class="headerlink" href="#id58" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Implement FLASK evaluation system</strong></p></li>
<li><p><strong>Calculate 12 fine-grained ability indicators</strong></p></li>
<li><p><strong>Generate comprehensive evaluation results</strong></p></li>
</ol>
</section>
<section id="id59">
<h4>8.3.3 Exercise Code<a class="headerlink" href="#id59" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FLASKEvaluator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># 12 fine-grained ability indicators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skills</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;Clarity&quot;</span><span class="p">,</span> <span class="s2">&quot;Consistency&quot;</span><span class="p">,</span> <span class="s2">&quot;Relevance&quot;</span><span class="p">,</span> <span class="s2">&quot;Completeness&quot;</span><span class="p">,</span> <span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> <span class="s2">&quot;Creativity&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Logic&quot;</span><span class="p">,</span> <span class="s2">&quot;Structure&quot;</span><span class="p">,</span> <span class="s2">&quot;Expressiveness&quot;</span><span class="p">,</span> <span class="s2">&quot;Appropriateness&quot;</span><span class="p">,</span> <span class="s2">&quot;Efficiency&quot;</span><span class="p">,</span> <span class="s2">&quot;Reliability&quot;</span>
        <span class="p">]</span>

        <span class="c1"># Classifiers for each ability (actually more complex models needed)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skill_classifiers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">skill</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skills</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">skill_classifiers</span><span class="p">[</span><span class="n">skill</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extract_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract features from text&quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
            <span class="c1"># Generate sentence representation through average pooling</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">features</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate_skill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">skill_name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate specific ability&quot;&quot;&quot;</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extract_features</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="c1"># Actually more sophisticated evaluation needed, here shows simple example</span>
        <span class="k">if</span> <span class="n">skill_name</span> <span class="o">==</span> <span class="s2">&quot;Clarity&quot;</span><span class="p">:</span>
            <span class="c1"># Clarity inversely proportional to sentence length and complexity</span>
            <span class="n">clarity_score</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">skill_name</span> <span class="o">==</span> <span class="s2">&quot;Consistency&quot;</span><span class="p">:</span>
            <span class="c1"># Consistency measured by ratio of repeated words</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            <span class="n">unique_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
            <span class="n">consistency_score</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_words</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="k">if</span> <span class="n">words</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">skill_name</span> <span class="o">==</span> <span class="s2">&quot;Relevance&quot;</span><span class="p">:</span>
            <span class="c1"># Relevance measured by keyword density</span>
            <span class="n">relevance_score</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">*</span> <span class="mf">0.05</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Random scores for other abilities (actually more sophisticated evaluation needed)</span>
            <span class="n">relevance_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">relevance_score</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate_all_skills</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate all abilities&quot;&quot;&quot;</span>
        <span class="n">skill_scores</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">skill</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skills</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_skill</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">skill</span><span class="p">)</span>
            <span class="n">skill_scores</span><span class="p">[</span><span class="n">skill</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>

        <span class="k">return</span> <span class="n">skill_scores</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_overall_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">skill_scores</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate overall score&quot;&quot;&quot;</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">skill_scores</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">visualize_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">skill_scores</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Visualize results&quot;&quot;&quot;</span>
        <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

        <span class="c1"># Bar chart of ability scores</span>
        <span class="n">skills</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">skill_scores</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">skill_scores</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">skills</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;FLASK Ability Scores&#39;</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Score&#39;</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>

        <span class="c1"># Radar chart</span>
        <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">skills</span><span class="p">),</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">scores_radar</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">scores</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Add first score to make circular</span>
        <span class="n">angles</span> <span class="o">+=</span> <span class="n">angles</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;polar&#39;</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">angles</span><span class="p">,</span> <span class="n">scores_radar</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">angles</span><span class="p">,</span> <span class="n">scores_radar</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">angles</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">skills</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;FLASK Ability Profile&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Exercise execution</span>
<span class="k">def</span><span class="w"> </span><span class="nf">run_flask_experiment</span><span class="p">():</span>
    <span class="c1"># Initialize FLASK evaluator</span>
    <span class="n">flask_evaluator</span> <span class="o">=</span> <span class="n">FLASKEvaluator</span><span class="p">()</span>

    <span class="c1"># Test text</span>
    <span class="n">test_text</span> <span class="o">=</span> <span class="s2">&quot;Artificial intelligence is having a significant impact on modern society. This technology is being utilized in various fields and is changing our daily lives.&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== FLASK Evaluation Experiment ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text to evaluate: </span><span class="si">{</span><span class="n">test_text</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Evaluate all abilities</span>
    <span class="n">skill_scores</span> <span class="o">=</span> <span class="n">flask_evaluator</span><span class="o">.</span><span class="n">evaluate_all_skills</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>

    <span class="c1"># Output results</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ability scores:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">skill</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">skill_scores</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">skill</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Overall score</span>
    <span class="n">overall_score</span> <span class="o">=</span> <span class="n">flask_evaluator</span><span class="o">.</span><span class="n">calculate_overall_score</span><span class="p">(</span><span class="n">skill_scores</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Overall score: </span><span class="si">{</span><span class="n">overall_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Visualize results</span>
    <span class="n">flask_evaluator</span><span class="o">.</span><span class="n">visualize_results</span><span class="p">(</span><span class="n">skill_scores</span><span class="p">,</span> <span class="n">test_text</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">skill_scores</span><span class="p">,</span> <span class="n">overall_score</span>

<span class="c1"># Exercise execution</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">skill_scores</span><span class="p">,</span> <span class="n">overall_score</span> <span class="o">=</span> <span class="n">run_flask_experiment</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="exercise-result-analysis">
<h3>8.4 Exercise Result Analysis<a class="headerlink" href="#exercise-result-analysis" title="Link to this heading">#</a></h3>
<section id="id60">
<h4>8.4.1 Exercise Objectives<a class="headerlink" href="#id60" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Comprehensive analysis of exercise results</strong></p></li>
<li><p><strong>Compare advantages and disadvantages of various evaluation methods</strong></p></li>
<li><p><strong>Derive considerations for actual application</strong></p></li>
</ul>
</section>
<section id="id61">
<h4>8.4.2 Exercise Content<a class="headerlink" href="#id61" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Comprehensive exercise results</strong></p></li>
<li><p><strong>Comparative analysis of evaluation methods</strong></p></li>
<li><p><strong>Review actual application scenarios</strong></p></li>
</ol>
</section>
<section id="id62">
<h4>8.4.3 Exercise Code<a class="headerlink" href="#id62" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="k">def</span><span class="w"> </span><span class="nf">analyze_experiment_results</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Comprehensive analysis of exercise results&quot;&quot;&quot;</span>

    <span class="c1"># Experimental result data (actually results from above exercises)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;Evaluation Method&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;BLEU&#39;</span><span class="p">,</span> <span class="s1">&#39;ROUGE-1&#39;</span><span class="p">,</span> <span class="s1">&#39;ROUGE-2&#39;</span><span class="p">,</span> <span class="s1">&#39;ROUGE-L&#39;</span><span class="p">,</span> <span class="s1">&#39;GPTScore&#39;</span><span class="p">,</span> <span class="s1">&#39;FLASK&#39;</span><span class="p">],</span>
        <span class="s1">&#39;Average Score&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.62</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.58</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.68</span><span class="p">],</span>
        <span class="s1">&#39;Standard Deviation&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.11</span><span class="p">],</span>
        <span class="s1">&#39;Computation Time (seconds)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">],</span>
        <span class="s1">&#39;Human Correlation&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.68</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">]</span>
    <span class="p">}</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

    <span class="c1"># Visualize results</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

    <span class="c1"># Average score comparison</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Evaluation Method&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Average Score&#39;</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Average Score by Evaluation Method&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Score&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>

    <span class="c1"># Computation time comparison</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Evaluation Method&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Computation Time (seconds)&#39;</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Computation Time by Evaluation Method&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Time (seconds)&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>

    <span class="c1"># Human correlation comparison</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Evaluation Method&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Human Correlation&#39;</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Human Correlation by Evaluation Method&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Correlation Coefficient&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>

    <span class="c1"># Comprehensive comparison (normalized scores)</span>
    <span class="n">normalized_scores</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;Average Score&#39;</span><span class="p">,</span> <span class="s1">&#39;Human Correlation&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">normalized_scores</span><span class="p">[</span><span class="s1">&#39;Computation Time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Computation Time (seconds)&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Computation Time (seconds)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">normalized_scores</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlOrRd&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Comprehensive Comparison by Evaluation Method (Normalized)&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># Result analysis</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Exercise Result Analysis ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. Characteristics by evaluation method:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Evaluation Method&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: Average </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Average Score&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, Human correlation </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Human Correlation&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. Key findings:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- GPTScore and FLASK show highest human correlation&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Traditional methods (BLEU/ROUGE) are fast but have low correlation with human judgment&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- LLM-based methods take longer but enable more accurate evaluation&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. Considerations for actual application:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- For fast evaluation: Use BLEU/ROUGE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- For accurate evaluation: Use GPTScore/FLASK&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Balanced approach: Combine multiple methods&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">df</span>

<span class="c1"># Exercise execution</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">results_df</span> <span class="o">=</span> <span class="n">analyze_experiment_results</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="id63">
<h3>Checkpoint Questions<a class="headerlink" href="#id63" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What are the main differences observed in the <strong>BLEU/ROUGE vs G-Eval comparison experiment</strong>, and what characteristics of each evaluation method does this show?</p></li>
<li><p>What are the advantages of probability-based evaluation in <strong>GPTScore implementation</strong>, and how does this differ from traditional evaluation methods?</p></li>
<li><p>Why does the <strong>FLASK evaluation system</strong> use 12 fine-grained ability indicators, and how does this differ from single-score evaluation?</p></li>
<li><p>Why do GPTScore and FLASK show the highest human correlation in <strong>exercise result analysis</strong>, and what implications does this provide for actual application?</p></li>
<li><p>What is the trade-off between fast evaluation and accurate evaluation in <strong>considerations for actual application</strong>, and how can this be balanced?</p></li>
</ul>
</section>
</section>
<section id="summary-and-conclusion">
<h2>9. Summary and Conclusion<a class="headerlink" href="#summary-and-conclusion" title="Link to this heading">#</a></h2>
<p>This chapter examined the changing landscape of LLM evaluation and new evaluation paradigms. We recognized the limitations of traditional evaluation metrics and confirmed how evaluation methodologies are developing through the emergence of meaning-based evaluation and the LLM-as-a-Judge paradigm.</p>
<section id="summary-of-main-content">
<h3>9.1 Summary of Main Content<a class="headerlink" href="#summary-of-main-content" title="Link to this heading">#</a></h3>
<section id="changing-landscape-of-evaluation">
<h4>9.1.1 Changing Landscape of Evaluation<a class="headerlink" href="#changing-landscape-of-evaluation" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Limitations of traditional evaluation metrics</strong>: Limitations of BLEU, ROUGE, etc. based on superficial similarity</p></li>
<li><p><strong>Emergence of meaning-based evaluation</strong>: Meaning-based evaluation like BERTScore, SentenceMover, BLEURT</p></li>
<li><p><strong>LLM-as-a-Judge paradigm</strong>: New approaches utilizing LLMs as evaluators like GPTScore, G-Eval, FLASK</p></li>
</ul>
</section>
<section id="id64">
<h4>9.1.2 LLM-Based Evaluation Paradigms<a class="headerlink" href="#id64" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>GPTScore</strong>: Probability-based evaluation framework utilizing model’s inherent knowledge</p></li>
<li><p><strong>G-Eval</strong>: Systematic evaluation through Chain-of-Thought achieving high correlation with human judgment</p></li>
<li><p><strong>FLASK</strong>: Fine-grained skill set-based multi-dimensional evaluation enabling detailed text quality analysis</p></li>
</ul>
</section>
<section id="id65">
<h4>9.1.3 Specialized Purpose Benchmarks<a class="headerlink" href="#id65" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>LiveCodeBench</strong>: Contamination-free code generation evaluation</p></li>
<li><p><strong>EvalPlus</strong>: More rigorous code evaluation through test case augmentation</p></li>
<li><p><strong>HELM-Code</strong>: Comprehensive evaluation focusing on transparency and community collaboration</p></li>
<li><p><strong>MMLU-Pro</strong>: 10-choice high-difficulty knowledge/reasoning benchmark</p></li>
<li><p><strong>GPQA and BBH</strong>: Knowledge/reasoning enhanced evaluation sets</p></li>
</ul>
</section>
<section id="id66">
<h4>9.1.4 Domain-Specific Benchmarks<a class="headerlink" href="#id66" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>FinBen</strong>: Comprehensive financial domain benchmark</p></li>
<li><p><strong>AgentHarm</strong>: AI agent harmfulness evaluation benchmark</p></li>
<li><p><strong>LEXam</strong>: Legal exam-based LLM evaluation</p></li>
<li><p><strong>CSEDB</strong>: Medical LLM safety/effectiveness dual evaluation</p></li>
<li><p><strong>MATH and GSM8K</strong>: Mathematical ability evaluation</p></li>
</ul>
</section>
<section id="id67">
<h4>9.1.5 Evaluation Bias and Limitations<a class="headerlink" href="#id67" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Major biases</strong>: Narcissistic bias, verbosity bias, inconsistency</p></li>
<li><p><strong>Evaluation limitations</strong>: Differences from human evaluation, lack of domain-specific knowledge, subjectivity of evaluation criteria</p></li>
</ul>
</section>
<section id="rlaif-and-future-evaluation-paradigms">
<h4>9.1.6 RLAIF and Future Evaluation Paradigms<a class="headerlink" href="#rlaif-and-future-evaluation-paradigms" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>RLAIF</strong>: More efficient and scalable learning through AI feedback-based reinforcement learning</p></li>
<li><p><strong>Future evaluation paradigms</strong>: Multimodal LLM evaluation, agent evaluation, Green AI evaluation, human-AI collaboration evaluation</p></li>
</ul>
</section>
</section>
<section id="core-insights">
<h3>9.2 Core Insights<a class="headerlink" href="#core-insights" title="Link to this heading">#</a></h3>
<section id="evolution-of-evaluation-methodologies">
<h4>9.2.1 Evolution of Evaluation Methodologies<a class="headerlink" href="#evolution-of-evaluation-methodologies" title="Link to this heading">#</a></h4>
<p>LLM evaluation is evolving from simple superficial similarity to semantic similarity, and to evaluation utilizing model’s inherent knowledge. This enables more accurate and human-like judgment evaluation.</p>
</section>
<section id="multi-dimensionality-of-evaluation">
<h4>9.2.2 Multi-dimensionality of Evaluation<a class="headerlink" href="#multi-dimensionality-of-evaluation" title="Link to this heading">#</a></h4>
<p>Modern LLM evaluation takes a multi-dimensional approach rather than single scores, comprehensively evaluating various aspects of text. This provides more sophisticated and useful evaluation results.</p>
</section>
<section id="importance-of-domain-specialization">
<h4>9.2.3 Importance of Domain Specialization<a class="headerlink" href="#importance-of-domain-specialization" title="Link to this heading">#</a></h4>
<p>General evaluation methods cannot easily evaluate expertise in specific domains. Therefore, development and utilization of domain-specific benchmarks is becoming important.</p>
</section>
<section id="recognition-of-evaluation-bias-and-limitations">
<h4>9.2.4 Recognition of Evaluation Bias and Limitations<a class="headerlink" href="#recognition-of-evaluation-bias-and-limitations" title="Link to this heading">#</a></h4>
<p>It is necessary to recognize and resolve biases and limitations that appear in LLM evaluation. This enables building fairer and more reliable evaluation systems.</p>
</section>
</section>
<section id="future-development-directions">
<h3>9.3 Future Development Directions<a class="headerlink" href="#future-development-directions" title="Link to this heading">#</a></h3>
<section id="continuous-development-of-evaluation-methodologies">
<h4>9.3.1 Continuous Development of Evaluation Methodologies<a class="headerlink" href="#continuous-development-of-evaluation-methodologies" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Development of more sophisticated evaluation metrics</strong></p></li>
<li><p><strong>Methodology research for bias reduction</strong></p></li>
<li><p><strong>Building human-AI collaboration evaluation systems</strong></p></li>
</ul>
</section>
<section id="expansion-of-domain-specific-evaluation">
<h4>9.3.2 Expansion of Domain-Specific Evaluation<a class="headerlink" href="#expansion-of-domain-specific-evaluation" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Development of specialized benchmarks for new domains</strong></p></li>
<li><p><strong>Building multilingual evaluation systems</strong></p></li>
<li><p><strong>Evaluation methodologies considering cultural context</strong></p></li>
</ul>
</section>
<section id="building-practical-evaluation-systems">
<h4>9.3.3 Building Practical Evaluation Systems<a class="headerlink" href="#building-practical-evaluation-systems" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Development of real-time evaluation systems</strong></p></li>
<li><p><strong>Building automated evaluation pipelines</strong></p></li>
<li><p><strong>Research on interpretation and utilization methods of evaluation results</strong></p></li>
</ul>
</section>
</section>
<section id="conclusion">
<h3>9.4 Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h3>
<p>LLM evaluation is a rapidly developing field where new technologies and methodologies continue to emerge. By recognizing the limitations of traditional evaluation methods and enabling more accurate and human-like judgment evaluation through meaning-based evaluation and the LLM-as-a-Judge paradigm, we are advancing evaluation capabilities.</p>
<p>Particularly, through the development and utilization of domain-specific benchmarks, we can now evaluate expertise in specific fields, and efforts are continuing to recognize and resolve evaluation biases and limitations for building fairer and more reliable evaluation systems.</p>
<p>In the future, evaluation methodologies for new technologies and application areas such as multimodal LLMs, AI agents, and Green AI are expected to develop further, building more sophisticated and practical evaluation systems.</p>
</section>
</section>
<section id="references">
<h2>10. References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<section id="traditional-evaluation-metrics">
<h3>10.1 Traditional Evaluation Metrics<a class="headerlink" href="#traditional-evaluation-metrics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Papineni, K., et al. (2002). BLEU: a method for automatic evaluation of machine translation. <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>.</p></li>
<li><p>Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries. <em>Text Summarization Branches Out</em>.</p></li>
</ul>
</section>
<section id="meaning-based-evaluation">
<h3>10.2 Meaning-Based Evaluation<a class="headerlink" href="#meaning-based-evaluation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Zhang, T., et al. (2020). BERTScore: Evaluating text generation with BERT. <em>International Conference on Learning Representations</em>.</p></li>
<li><p>Clark, E., et al. (2019). SentenceMover’s similarity: Automatic evaluation for multi-sentence texts. <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>.</p></li>
<li><p>Sellam, T., et al. (2020). BLEURT: Learning robust metrics for text generation. <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>.</p></li>
</ul>
</section>
<section id="llm-based-evaluation">
<h3>10.3 LLM-Based Evaluation<a class="headerlink" href="#llm-based-evaluation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Liu, Y., et al. (2023). G-Eval: NLG evaluation using GPT-4 with better human alignment. <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
<li><p>Fu, J., et al. (2023). GPTScore: Evaluate as you desire. <em>arXiv preprint arXiv:2302.04166</em>.</p></li>
<li><p>Wang, J., et al. (2023). FLASK: Fine-grained language model evaluation based on alignment skill sets. <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
</ul>
</section>
<section id="id68">
<h3>10.4 Specialized Purpose Benchmarks<a class="headerlink" href="#id68" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Jain, N., et al. (2024). LiveCodeBench: Holistic and contamination-free evaluation of large language models for code. <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
<li><p>Liu, J., et al. (2023). EvalPlus: Augmenting code evaluation datasets with test case generation. <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
<li><p>Liang, P., et al. (2022). Holistic evaluation of language models. <em>Transactions on Machine Learning Research</em>.</p></li>
</ul>
</section>
<section id="id69">
<h3>10.5 Domain-Specific Benchmarks<a class="headerlink" href="#id69" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Chen, J., et al. (2024). FinBen: A holistic financial benchmark for large language models. <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
<li><p>Sun, Y., et al. (2025). AgentHarm: A comprehensive benchmark for evaluating agentic AI safety. <em>Proceedings of the 2025 International Conference on Learning Representations</em>.</p></li>
<li><p>Nguyen, H., et al. (2025). LEXam: A comprehensive benchmark for legal reasoning evaluation. <em>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
</ul>
</section>
<section id="rlaif-and-future-evaluation">
<h3>10.6 RLAIF and Future Evaluation<a class="headerlink" href="#rlaif-and-future-evaluation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI feedback. <em>arXiv preprint arXiv:2212.08073</em>.</p></li>
<li><p>Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. <em>Advances in Neural Information Processing Systems</em>.</p></li>
</ul>
</section>
<section id="id70">
<h3>10.7 Evaluation Bias and Limitations<a class="headerlink" href="#id70" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Chiang, W. L., et al. (2023). Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality. <em>arXiv preprint arXiv:2303.04671</em>.</p></li>
<li><p>Lin, S., et al. (2022). TruthfulQA: Measuring how models mimic human falsehoods. <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
</ul>
</section>
<section id="mathematical-and-reasoning-evaluation">
<h3>10.8 Mathematical and Reasoning Evaluation<a class="headerlink" href="#mathematical-and-reasoning-evaluation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hendrycks, D., et al. (2021). Measuring mathematical problem solving with the MATH dataset. <em>Proceedings of the 2021 Conference on Neural Information Processing Systems</em>.</p></li>
<li><p>Cobbe, K., et al. (2021). Training verifiers to solve math word problems. <em>Proceedings of the 2021 Conference on Neural Information Processing Systems</em>.</p></li>
</ul>
</section>
<section id="medical-and-legal-evaluation">
<h3>10.9 Medical and Legal Evaluation<a class="headerlink" href="#medical-and-legal-evaluation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Singhal, K., et al. (2023). Towards expert-level medical question answering with large language models. <em>arXiv preprint arXiv:2305.09617</em>.</p></li>
<li><p>Guha, N., et al. (2023). LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
</ul>
</section>
<section id="green-ai-and-efficiency">
<h3>10.10 Green AI and Efficiency<a class="headerlink" href="#green-ai-and-efficiency" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Schwartz, R., et al. (2020). Green AI. <em>Communications of the ACM</em>.</p></li>
<li><p>Strubell, E., et al. (2019). Energy and policy considerations for deep learning in NLP. <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>.</p></li>
</ul>
<hr class="docutils" />
<p><em>This chapter comprehensively examined the changing landscape of LLM evaluation and new evaluation paradigms. We learned methods to enable more accurate and human-like judgment evaluation by recognizing the limitations of traditional evaluation methods and through meaning-based evaluation and the LLM-as-a-Judge paradigm. We also confirmed the importance of specialized purpose benchmarks and domain-specific benchmarks, and explored methods to build fairer and more reliable evaluation systems by recognizing evaluation biases and limitations. In the future, evaluation methodologies for new technologies and application areas such as multimodal LLMs, AI agents, and Green AI are expected to develop further, building more sophisticated and practical evaluation systems.</em></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week05"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week04/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 4: Advanced Prompting Techniques and Optimization</p>
      </div>
    </a>
    <a class="right-next"
       href="../week06/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 6: Advances in Multimodal NLP</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-changing-landscape-of-evaluation-limitations-of-traditional-metrics-and-the-need-for-meaning-based-assessment">1. The Changing Landscape of Evaluation: Limitations of Traditional Metrics and the Need for Meaning-Based Assessment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-traditional-evaluation-metrics">1.1 Limitations of Traditional Evaluation Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#emergence-of-meaning-based-evaluation">1.2 Emergence of Meaning-Based Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bertscore-and-sentencemover">BERTScore and SentenceMover</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bleurt">BLEURT</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#emergence-of-llm-as-a-judge-paradigm">1.3 Emergence of LLM-as-a-Judge Paradigm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-based-evaluation-paradigms">2. LLM-Based Evaluation Paradigms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptscore-probability-based-evaluation-framework">2.1 GPTScore: Probability-Based Evaluation Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-principles">Core Principles</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-results">Performance Results</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptscore-implementation-example">2.1.1 GPTScore Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#g-eval-chain-of-thought-cot-based-llm-evaluation">2.2 G-Eval: Chain-of-Thought (CoT) Based LLM Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-features">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-process">Evaluation Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-summarization-consistency-evaluation">Example: Summarization Consistency Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-based-calibration">Probability-Based Calibration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#g-eval-implementation-example">2.2.1 G-Eval Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flask-fine-grained-skill-set-based-evaluation">2.3 FLASK: Fine-grained Skill Set Based Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-grained-ability-indicators">12 Fine-grained Ability Indicators</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Evaluation Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-legal-consultation-response-evaluation">Example: Legal Consultation Response Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flask-implementation-example">2.3.1 FLASK Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specialized-purpose-benchmarks">3. Specialized Purpose Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#livecodebench-contamination-free-code-generation-evaluation">3.1 LiveCodeBench: Contamination-Free Code Generation Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-problem-data-contamination">Core Problem: Data Contamination</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-approach">Solution Approach</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#holistic-evaluation">Holistic Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#significance">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evalplus-test-case-augmentation">3.2 EvalPlus: Test Case Augmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-problem">Core Problem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Solution Approach</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helm-code-transparency-and-community-collaboration">3.3 HELM-Code: Transparency and Community Collaboration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#helm-philosophy">HELM Philosophy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mmlu-pro-10-choice-high-difficulty-knowledge-reasoning-benchmark">3.4 MMLU-Pro: 10-Choice High-Difficulty Knowledge/Reasoning Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-changes">Core Changes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-effect">Chain-of-Thought Effect</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#current-status">Current Status</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpqa-and-bbh-knowledge-reasoning-enhanced-evaluation-sets">3.5 GPQA and BBH: Knowledge/Reasoning Enhanced Evaluation Sets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-examples">Problem Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Significance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bbh-big-bench-hard">BBH (BIG-Bench Hard)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Core Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#goal">Goal</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#research-utilization">Research Utilization</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#extended-version-bbeh">Extended Version: BBEH</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Significance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#domain-specific-benchmarks">4. Domain-Specific Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finben-comprehensive-financial-domain-benchmark">4.1 FinBen: Comprehensive Financial Domain Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-composition">Data Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implications">Implications</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentharm-ai-agent-harmfulness-evaluation-benchmark">4.2 AgentHarm: AI Agent Harmfulness Evaluation Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-composition">Benchmark Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-examples">Scenario Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-method">Evaluation Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-findings">Key Findings</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#research-impact">Research Impact</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lexam-legal-exam-based-llm-evaluation">4.3 LEXam: Legal Exam-Based LLM Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Core Features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Data Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Evaluation Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-order-reasoning-specific-to-legal-field">High-Order Reasoning Specific to Legal Field</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-as-judge-utilization">LLM-as-Judge Utilization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#csedb-medical-llm-safety-effectiveness-dual-evaluation">4.4 CSEDB: Medical LLM Safety/Effectiveness Dual Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Background</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Benchmark Composition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Performance Results</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-in-high-risk-scenarios">Performance in High-Risk Scenarios</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-domain-specific-tuning">Effect of Domain-Specific Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#utilization-methods">Utilization Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-and-gsm8k-mathematical-ability-evaluation">4.5 MATH and GSM8K: Mathematical Ability Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#math-benchmark">MATH Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#features">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">Performance Results</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gsm8k-benchmark">GSM8K Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-improvement-techniques">Performance Improvement Techniques</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#utilization">Utilization</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id38">Evaluation Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-limitations">LLM Limitations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#improvement-research">Improvement Research</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Significance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-bias-and-limitations">5. Evaluation Bias and Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#major-evaluation-biases">5.1 Major Evaluation Biases</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#narcissistic-bias">5.1.1 Narcissistic Bias</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#verbosity-bias">5.1.2 Verbosity Bias</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">Solutions</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inconsistency">5.1.3 Inconsistency</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id44">Features</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id45">Solutions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-limitations">5.2 Evaluation Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-from-human-evaluation">5.2.1 Differences from Human Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lack-of-domain-specific-knowledge">5.2.2 Lack of Domain-Specific Knowledge</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subjectivity-of-evaluation-criteria">5.2.3 Subjectivity of Evaluation Criteria</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id46">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-reinforcement-learning-from-ai-feedback">6. RLAIF: Reinforcement Learning from AI Feedback</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-principles-of-rlaif">6.1 Core Principles of RLAIF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-rlaif">6.2 Advantages of RLAIF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-rlaif">6.3 Limitations of RLAIF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-implementation-example">6.4 RLAIF Implementation Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id47">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-evaluation-paradigms">7. Future Evaluation Paradigms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-llm-evaluation">7.1 Multimodal LLM Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-tasks">7.1.1 Evaluation Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-methods">7.1.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-evaluation">7.2 Agent Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id48">7.2.1 Evaluation Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id49">7.2.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#green-ai-evaluation">7.3 Green AI Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics">7.3.1 Evaluation Metrics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">7.3.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-ai-collaboration-evaluation">7.4 Human-AI Collaboration Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id51">7.4.1 Evaluation Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id52">7.4.2 Evaluation Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hands-on-exercises">8. Hands-on Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bleu-rouge-vs-g-eval-comparison-experiment">8.1 BLEU/ROUGE vs G-Eval Comparison Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-objectives">8.1.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-content">8.1.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-code">8.1.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptscore-implementation-and-experiment">8.2 GPTScore Implementation and Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id54">8.2.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id55">8.2.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id56">8.2.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flask-evaluation-system-implementation">8.3 FLASK Evaluation System Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id57">8.3.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id58">8.3.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id59">8.3.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-result-analysis">8.4 Exercise Result Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">8.4.1 Exercise Objectives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id61">8.4.2 Exercise Content</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">8.4.3 Exercise Code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id63">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-and-conclusion">9. Summary and Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-main-content">9.1 Summary of Main Content</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#changing-landscape-of-evaluation">9.1.1 Changing Landscape of Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id64">9.1.2 LLM-Based Evaluation Paradigms</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id65">9.1.3 Specialized Purpose Benchmarks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id66">9.1.4 Domain-Specific Benchmarks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id67">9.1.5 Evaluation Bias and Limitations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-and-future-evaluation-paradigms">9.1.6 RLAIF and Future Evaluation Paradigms</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-insights">9.2 Core Insights</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evolution-of-evaluation-methodologies">9.2.1 Evolution of Evaluation Methodologies</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensionality-of-evaluation">9.2.2 Multi-dimensionality of Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-domain-specialization">9.2.3 Importance of Domain Specialization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recognition-of-evaluation-bias-and-limitations">9.2.4 Recognition of Evaluation Bias and Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-development-directions">9.3 Future Development Directions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-development-of-evaluation-methodologies">9.3.1 Continuous Development of Evaluation Methodologies</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expansion-of-domain-specific-evaluation">9.3.2 Expansion of Domain-Specific Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#building-practical-evaluation-systems">9.3.3 Building Practical Evaluation Systems</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">9.4 Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">10. References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-evaluation-metrics">10.1 Traditional Evaluation Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#meaning-based-evaluation">10.2 Meaning-Based Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-based-evaluation">10.3 LLM-Based Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id68">10.4 Specialized Purpose Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id69">10.5 Domain-Specific Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-and-future-evaluation">10.6 RLAIF and Future Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id70">10.7 Evaluation Bias and Limitations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-and-reasoning-evaluation">10.8 Mathematical and Reasoning Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#medical-and-legal-evaluation">10.9 Medical and Legal Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#green-ai-and-efficiency">10.10 Green AI and Efficiency</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
