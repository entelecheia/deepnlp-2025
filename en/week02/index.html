
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 2 - Tool Learning: PyTorch and Latest Frameworks &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week02/index';</script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 3: Efficient Fine-Tuning with Modern PEFT Techniques" href="../week03/index.html" />
    <link rel="prev" title="Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A" href="../week01/qna.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Deep Learning for Natural Language Processing (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1 - Transformer and Next-Generation Architectures</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 2 - Tool Learning: PyTorch and Latest Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: Advanced Prompting Techniques and Optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/en/week02/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek02/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week02/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 2 - Tool Learning: PyTorch and Latest Frameworks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-basics-tensors-and-autograd">1. PyTorch Basics: Tensors and Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flashattention-3-fast-attention-implementation">2. FlashAttention-3: Fast Attention Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-transformers-practice">3. Hugging Face Transformers Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-pre-trained-models-and-tokenizers">3.1 Loading Pre-trained Models and Tokenizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-encoding-using-tokenizer">3.2 Input Encoding Using Tokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-using-classification-pipeline">3.3 Prediction Using Classification Pipeline</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-latest-nlp-frameworks">4. Introduction to Latest NLP Frameworks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dspy-declarative-prompt-programming">4.1 DSPy: Declarative Prompt Programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#haystack-document-based-search-and-reasoning">4.2 Haystack: Document-based Search and Reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#crewai-role-based-multi-agent-framework">4.3 CrewAI: Role-based Multi-Agent Framework</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-bert-vs-mamba-model-comparison-experiment">5. Practice: BERT vs Mamba Model Comparison Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-preparation-and-inference-code">5.1 Model Preparation and Inference Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#result-comparison-accuracy-speed-memory">5.2 Result Comparison: Accuracy, Speed, Memory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-summary-and-implications">6. Experiment Summary and Implications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-2-tool-learning-pytorch-and-latest-frameworks">
<h1>Week 2 - Tool Learning: PyTorch and Latest Frameworks<a class="headerlink" href="#week-2-tool-learning-pytorch-and-latest-frameworks" title="Link to this heading">#</a></h1>
<section id="pytorch-basics-tensors-and-autograd">
<h2>1. PyTorch Basics: Tensors and Autograd<a class="headerlink" href="#pytorch-basics-tensors-and-autograd" title="Link to this heading">#</a></h2>
<p>PyTorch provides <strong>tensor</strong> data structures and automatic differentiation (<strong>Autograd</strong>) functionality to simplify deep learning model implementation. Tensors are multi-dimensional arrays similar to NumPy arrays but support GPU acceleration and automatic differentiation. For example, you can create tensors with torch.tensor() and perform <strong>tensor operations</strong> like addition, multiplication, etc. Tensor operations provide various functions needed for scientific computation such as broadcasting, view transformation, matrix multiplication, etc.</p>
<p>PyTorch’s <strong>Autograd principle</strong> is based on <strong>computational graphs</strong>. When tensor operations are performed, PyTorch dynamically generates <strong>graph nodes</strong> for operators and records functions (grad_fn) for backpropagation at each node. The generated <strong>computation graph</strong> is in the form of a directed acyclic graph (DAG), where leaf nodes are input tensors and root nodes are output (loss) tensors. According to the <strong>chain rule</strong>, when .backward() is called, differentiation is automatically calculated from the graph’s root to leaf. The Autograd engine calls grad_fn defining differentiation formulas for each operation in sequence to propagate <strong>gradients</strong>. As a result, the value of d(Output)/d(Input) is stored in the grad attribute of input tensors.</p>
<p>For example, here’s code that automatically calculates the derivative of a simple first-order function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="c1"># Set requires_grad=True to activate gradient tracking</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>            <span class="c1"># y = 3x^2 + 5x + 1</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                   <span class="c1"># Differentiate y with respect to x (automatic backpropagation)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>                  <span class="c1"># Output: tensor(17.)</span>
</pre></div>
</div>
<p>In the above code, since y=3x²+5x+1, the derivative result is dy/dx=6x+5, and when x=2, 6*2+5=17 is stored in x.grad. Like this, PyTorch Autograd automatically calculates derivatives following the <strong>dynamic computation graph</strong> without users having to calculate differentiation formulas manually. Also, thanks to the dynamic graph characteristics, complex models with <strong>branches or loops</strong> can flexibly construct graphs and process them at each iteration.</p>
<p><em>Figure 1: Example of computation graph constructed by Autograd. Blue nodes are leaf tensors (inputs), and operation nodes represent backward functions of corresponding operations. When .backward() is executed, grad_fn of each node is called following this graph, and finally, derivative values with respect to loss are stored in grad of input tensors.</em></p>
<p>Furthermore, in PyTorch, model parameters can be updated using <strong>gradient descent</strong> with .grad values. Using Optimizer (torch.optim.SGD, etc.), parameters are updated based on gradients calculated by .backward(). At this time, it’s important to initialize existing gradients to 0 with .zero_grad() before new iteration to prevent accumulation of gradient residues from previous steps. Through these procedures, neural networks are trained, and Autograd helps efficient learning by reconstructing graphs and performing backpropagation at each step. Also, if needed, you can inherit torch.autograd.Function for custom operations to implement custom backward functions, making it possible to extend the Autograd engine.</p>
</section>
<section id="flashattention-3-fast-attention-implementation">
<h2>2. FlashAttention-3: Fast Attention Implementation<a class="headerlink" href="#flashattention-3-fast-attention-implementation" title="Link to this heading">#</a></h2>
<p><strong>Attention mechanism</strong> is the core of Transformer models, but has limitations where <strong>computational complexity</strong> increases quadratically with input sequence length n as O(n²). For example, when sequence length increases (performing computation for all query-key pairs), computational cost and memory usage increase exponentially, causing <strong>bottlenecks</strong>. Especially in self-attention, n×n sized score matrices are created for each layer and go through Softmax, so for very long inputs, computation time slows down and GPU memory is heavily required, acting as practical limitations (e.g., BERT’s maximum input 512 token limit). One technique that emerged to reduce such bottlenecks is <strong>FlashAttention</strong>, and <strong>FlashAttention-1</strong> proposed by Tri Dao et al. in 2022 optimized attention operations through <strong>memory access minimization</strong>. Specifically, instead of generating large attention matrices at once, it processes partial blocks repeatedly using <strong>tiling techniques</strong>, and recomputes necessary intermediate values to <strong>reduce GPU global memory I/O</strong>, dramatically improving time and memory usage. Later, <strong>FlashAttention-2</strong> improved GPU utilization by parallelizing work up to sequence dimensions and processing keys/values in block units, but even this only achieved about 35% utilization of theoretical performance on latest GPUs like <strong>H100</strong>. This was due to limitations like not fully utilizing the <strong>asynchronous computation</strong> capabilities of latest hardware.</p>
<p><strong>FlashAttention-3</strong> is a latest technique that further boosts attention speed by utilizing new features of Hopper architecture (GPU). The core ideas are summarized in three points:</p>
<ul class="simple">
<li><p><strong>Warp-specialization based asynchronous execution</strong>: Attention operations are subdivided so that some warps perform matrix multiplication (GEMM) using <strong>Tensor Core</strong> while other warps handle memory load/store with <strong>Tensor Memory Accelerator (TMA)</strong>. This enables <strong>overlapping computation and data transfer</strong> to utilize GPU resources without gaps. <strong>Pipelining</strong> (ping-pong scheduling) is implemented where one warp group calculates Softmax while another warp group performs matmul of the next block, making <strong>entire computation flow without stopping</strong>.</p></li>
<li><p><strong>MatMul-Softmax interleaved parallelization</strong>: Traditional attention applies Softmax after completing all query-key multiplications, but FlashAttention-3 performs multiplication and Softmax <strong>interleaved in block units</strong>. By repeating processing of small blocks and immediately calculating partial Softmax results to overlap with next operations, <strong>waiting time is reduced</strong>. This enables Tensor Core operations and memory access to occur simultaneously as much as possible on H100, improving GPU utilization.</p></li>
<li><p><strong>FP8 low-precision support</strong>: Attention operations are performed in low precision using <strong>FP8</strong> format supported by Hopper GPU. Processing per operation increases 2x compared to FP16, but FlashAttention-3 suppresses precision loss through <strong>block-wise scaling and correction techniques</strong>. According to the paper, when using FP8, it maintained accuracy with <strong>2.6x smaller error</strong> compared to existing FP8 attention while maximizing computational performance.</p></li>
</ul>
<p>As a result of these optimizations, FlashAttention-3 achieved <strong>1.5–2.0x speed improvement compared to existing methods on H100 GPU</strong>. For example, in FP16 settings, it achieves effective performance reaching about <strong>75%</strong> of H100’s theoretical maximum 740 TFLOP/s, which is nearly double improvement compared to FlashAttention-2. When using FP8, it achieves speeds approaching <strong>1.2 PFLOP/s</strong>. The figure below shows speed comparison results between FlashAttention-3 and existing implementations, where its superiority becomes more prominent as sequence length increases.</p>
<p><em>Figure 2: Forward operation speed comparison of FlashAttention-3 on H100 (FP16, seq length↑). Blue line is existing FlashAttention-2, orange line is FlashAttention-3, and FlashAttention-3’s performance improvement becomes more prominent as sequence length increases.</em></p>
<p>FlashAttention-3 is currently publicly available as <strong>beta version</strong> on GitHub and operates on H100 GPU in PyTorch 2.2+ and CUDA 12.3+ environments. For actual usage example, after installing the flash_attn library, you can call it as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flash_attn.flash_attn_interface</span><span class="w"> </span><span class="kn">import</span> <span class="n">flash_attn_func</span>
<span class="c1"># q, k, v: (batch, seq, head, dim) tensors, sm_scale: scaling</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">flash_attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">sm_scale</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>This function returns FlashAttention-optimized attention output for given query, key, value. PyTorch basic API also provides torch.nn.functional.scaled_dot_product_attention function from version 2.0, which can use optimization paths similar to FlashAttention internally depending on GPU environment. In summary, FlashAttention-3 is a technology that <strong>restructures attention operations to be hardware-friendly</strong>, greatly alleviating Transformer’s speed bottlenecks, playing an important role in <strong>context length expansion</strong> and <strong>training speed improvement</strong> in latest large-scale models.</p>
</section>
<section id="hugging-face-transformers-practice">
<h2>3. Hugging Face Transformers Practice<a class="headerlink" href="#hugging-face-transformers-practice" title="Link to this heading">#</a></h2>
<p><strong>Hugging Face Transformers</strong> library is a Python toolkit that allows easy loading and utilization of various pre-trained NLP models (BERT, GPT, T5, etc.). In this section, we practice Korean document classification examples using Hugging Face. We examine <strong>model loading</strong>, <strong>tokenizer usage</strong>, and <strong>pipeline API</strong> utilization methods step by step.</p>
<section id="loading-pre-trained-models-and-tokenizers">
<h3>3.1 Loading Pre-trained Models and Tokenizers<a class="headerlink" href="#loading-pre-trained-models-and-tokenizers" title="Link to this heading">#</a></h3>
<p>Hugging Face Hub has AutoModel and AutoTokenizer classes, so you can easily load pre-trained models and corresponding tokenizers just by model name. For example, here’s code to load a BERT model fine-tuned for NSMC dataset often used in Korean movie review sentiment analysis (NSMC: Naver Sentiment Movie Corpus, positive/negative label data for movie reviews):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;snoop2head/bert-base-nsmc&quot;</span>  <span class="c1"># Example BERT fine-tuned for NSMC sentiment analysis</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above code, the model and tokenizer at the path specified by model_name are downloaded from the internet. AutoTokenizer loads tokenizers specialized for that model (like BERT’s WordPiece, etc.), and AutoModelForSequenceClassification loads BERT model weights fine-tuned for classification. With the loaded tokenizer and model, you can immediately try inference.</p>
</section>
<section id="input-encoding-using-tokenizer">
<h3>3.2 Input Encoding Using Tokenizer<a class="headerlink" href="#input-encoding-using-tokenizer" title="Link to this heading">#</a></h3>
<p>To put natural language text into a model, it must be converted to a sequence of numeric indices through a <strong>tokenizer</strong>. The tokenizer splits sentences into units like WordPiece or SentencePiece and assigns IDs, creating input tensors that the model can understand. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The movie was really fun!&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>The tokenizer splits the sentence “The movie was really fun!” into subword units and encodes each with IDs. Giving the return_tensors=’pt’ option returns it in PyTorch tensor form, and the output inputs is a dictionary with keys like input_ids, attention_mask, etc. The input_ids tensor is the token sequence that the model understands, and attention_mask indicates parts to ignore like padding.</p>
</section>
<section id="prediction-using-classification-pipeline">
<h3>3.3 Prediction Using Classification Pipeline<a class="headerlink" href="#prediction-using-classification-pipeline" title="Link to this heading">#</a></h3>
<p>Using Hugging Face’s <strong>pipeline API</strong>, you can perform the entire inference process combining tokenizer and model at once. For classification, it’s convenient to use the “sentiment-analysis” pipeline:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;sentiment-analysis&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;This movie is really the best work.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above code, the classifier pipeline takes a string as input and immediately outputs sentiment analysis results. For example, if the sentence has positive content, you can get a dictionary like {‘label’: ‘POSITIVE’, ‘score’: 0.98} as a result. Internally, the pipeline performs a series of processes: converting input sentences to input_ids with the tokenizer, applying Softmax to the model’s prediction results (logits), and selecting the label with the highest probability.</p>
<p>If you want to perform batch prediction on multiple sentences, you can give a list as input. Also, you can apply the same pattern to other datasets like <strong>KorNLI</strong> (Korean Natural Language Inference) by extending the example of Korean NSMC dataset. Using Hugging Face datasets library:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">nsmc</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;nsmc&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nsmc</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>You can also load and examine the <strong>NSMC dataset</strong> like above (consisting of movie reviews and labels 0/1). With prepared datasets like this, you can batch convert them with tokenizers and then use Trainer API, etc., to fine-tune models. Detailed fine-tuning methods beyond this course’s scope will be covered later, but the fact that you can easily try simple <strong>inference practice</strong> with just the combination of <strong>pre-trained model + tokenizer + pipeline</strong> is a major advantage of the Hugging Face ecosystem.</p>
</section>
</section>
<section id="introduction-to-latest-nlp-frameworks">
<h2>4. Introduction to Latest NLP Frameworks<a class="headerlink" href="#introduction-to-latest-nlp-frameworks" title="Link to this heading">#</a></h2>
<p>Recently, new <strong>NLP frameworks</strong> that help with large language model (LLM) utilization and specialized applications are emerging one after another. In this section, we examine several rising tools among them: <strong>DSPy</strong>, <strong>Haystack</strong>, <strong>CrewAI</strong>. Each has different purposes and functions, but they are all tools that help developers build <strong>powerful NLP pipelines or agent systems with minimal effort</strong>.</p>
<section id="dspy-declarative-prompt-programming">
<h3>4.1 DSPy: Declarative Prompt Programming<a class="headerlink" href="#dspy-declarative-prompt-programming" title="Link to this heading">#</a></h3>
<p>DSPy stands for <strong>Declarative Self-Improving Python</strong> and is a <strong>declarative prompt programming</strong> framework released by Databricks. It reduces the complexity of managing <strong>long prompt strings</strong> that arise when directly handling LLMs, and allows you to create AI programs with modular composition as if <strong>writing code</strong>. In short, it’s designed with the philosophy of “don’t hardcode prompts, write them <strong>like programming</strong>.”</p>
<p>DSPy’s core concepts are divided into three: <strong>LM, Signature, Module</strong>:</p>
<ul class="simple">
<li><p><strong>LM</strong>: Specifies the language model to use. For example, if you set desired models like OpenAI API’s GPT-4, HuggingFace’s Llama2, etc. with dspy.LM(…) and dspy.configure(lm=…), then all subsequent modules generate results through this LM.</p></li>
<li><p><strong>Signature</strong>: Like specifying input and output types of functions, it declares the <strong>input and output format</strong> of prompt programs. For example, if you define signature like “question -&gt; answer: int”, DSPy automatically generates prompts in a structure that takes question(str) and outputs answer(int). Signatures describe the structure of prompts given to models and expected output forms (e.g., JSON format, etc.).</p></li>
<li><p><strong>Module</strong>: Encapsulates <strong>prompt techniques</strong> themselves for solving problems as modules. For example, simple Q&amp;A can be expressed as dspy.Predict, complex thinking cases as dspy.ChainOfThought (chain of thought), tool-using agents as dspy.ReAct modules. Modules have logic implemented internally for how to compose prompts according to the corresponding techniques.</p></li>
</ul>
<p>Users combine these three to create <strong>AI programs</strong>, then can optimize by automatically improving module prompts or adding few-shot examples through <strong>Optimizer</strong> built into DSPy. For example, you can make simple combinations like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">dspy</span>
<span class="c1"># 1) LM setup (local Llama2 model example)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">LM</span><span class="p">(</span><span class="s1">&#39;ollama/llama2&#39;</span><span class="p">,</span> <span class="n">api_base</span><span class="o">=</span><span class="s1">&#39;http://localhost:11434&#39;</span><span class="p">)</span>
<span class="n">dspy</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span>
<span class="c1"># 2) Signature declaration: question -&gt; answer(int)</span>
<span class="n">simple_sig</span> <span class="o">=</span> <span class="s2">&quot;question -&gt; answer: int&quot;</span>
<span class="c1"># 3) Module selection: Predict (basic Q&amp;A)</span>
<span class="n">simple_model</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">simple_sig</span><span class="p">)</span>
<span class="c1"># 4) Execute</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">simple_model</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">&quot;How many hours does it take from Seoul to Busan by KTX?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code creates a module called simple_model that defines the task of “output integer answers when receiving questions”. Internally, DSPy generates optimal prompts matching these requirements and passes them to the LM. If the initially obtained answer is inaccurate, you can apply Optimizers like <strong>BootstrapFewShot</strong> to automatically add few-shot examples, or instruct continuous answer improvement with <strong>Refine</strong> modules. In this way, DSPy enables composition and optimization of complex LLM pipelines (e.g., <strong>RAG</strong> systems, multi-stage chains, agent loops, etc.) in module units.</p>
<p>DSPy’s advantage is <strong>improved productivity in prompt engineering</strong>. Since LLM calls are designed within structured frameworks like code, it reduces the time people spend writing long prompt sentences manually and going through trial and error. Also, you can maintain the same module interface while switching various <strong>models/techniques</strong>, enabling <strong>flexible experiments</strong> like testing the same Chain-of-Thought module on both GPT-4 and Llama2 to compare performance. Thanks to the declarative approach, even <strong>changing only part of the program</strong> easily reflects in the entire LLM pipeline, making maintenance easy. Although it’s still an early-stage framework, it’s gaining attention for presenting the paradigm of <strong>“handling LLMs like programming”</strong>.</p>
</section>
<section id="haystack-document-based-search-and-reasoning">
<h3>4.2 Haystack: Document-based Search and Reasoning<a class="headerlink" href="#haystack-document-based-search-and-reasoning" title="Link to this heading">#</a></h3>
<p><strong>Haystack</strong> is an <strong>open-source NLP framework</strong> developed by Deepset in Germany, mainly used for building <strong>knowledge-based question answering</strong> systems. Haystack’s strength lies in <strong>flexible pipeline composition</strong>. Users can easily create <strong>end-to-end NLP systems</strong> that return answers when questions are input by linking a series of stages from databases (document stores) to search engines, reader (Reader) or generator (Generator) models into one Pipeline. For example, <strong>Retrieval QA</strong> like “find answers to questions from given document sets” or Wikipedia-based chatbots can be implemented with Haystack.</p>
<p>Haystack’s main components are as follows:</p>
<ul class="simple">
<li><p><strong>DocumentStore</strong>: Literally a database for storing documents. It supports backends like In-Memory, Elasticsearch, FAISS, etc., and stores document text, metadata, embeddings, etc.</p></li>
<li><p><strong>Retriever</strong>: Plays the role of <strong>searching</strong> for relevant documents regarding user questions (Query). It’s diversely implemented from traditional keyword-based methods like BM25 to <strong>Dense Passage Retrieval</strong> models like SBERT, DPR, etc. Retriever finds <strong>top k</strong> relevant documents from DocumentStore.</p></li>
<li><p><strong>Reader</strong> or <strong>Generator</strong>: Takes searched documents as input to generate final <strong>answers</strong>. <strong>Reader</strong> usually uses Extractive QA models (BERT-based, etc.) to extract correct answer spans from the documents, and <strong>Generator</strong> can generate answers using generative models like GPT. Both can be plugged in as nodes (Node) in Haystack.</p></li>
<li><p><strong>Pipeline</strong>: Structure that defines <strong>query-&gt;response flow</strong> by combining the above elements. There are simple ExtractiveQAPipeline that puts Retriever results into Reader, and GenerativeQAPipeline that creates answers generatively. You can also connect <strong>Retriever + Large LM</strong> like Retrieval-Augmented Generation, or implement multi-stage conditional flows.</p></li>
</ul>
<p>Let’s look at a <strong>simple practice example</strong> using Haystack. For example, if you want to create a QA system that answers questions using FAQ document collections:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">haystack.document_stores</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryDocumentStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">haystack.nodes</span><span class="w"> </span><span class="kn">import</span> <span class="n">BM25Retriever</span><span class="p">,</span> <span class="n">FARMReader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">haystack</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># 1) Create document store and write documents</span>
<span class="n">document_store</span> <span class="o">=</span> <span class="n">InMemoryDocumentStore</span><span class="p">()</span>
<span class="n">docs</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Drama **Squid Game** is a Korean survival drama...&quot;</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;Wikipedia&quot;</span><span class="p">}}]</span>
<span class="n">document_store</span><span class="o">.</span><span class="n">write_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="c1"># 2) Configure Retriever and Reader</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">BM25Retriever</span><span class="p">(</span><span class="n">document_store</span><span class="o">=</span><span class="n">document_store</span><span class="p">)</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">FARMReader</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;monologg/koelectra-base-v3-finetuned-korquad&quot;</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 3) Build pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">component</span><span class="o">=</span><span class="n">retriever</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Retriever&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Query&quot;</span><span class="p">])</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">component</span><span class="o">=</span><span class="n">reader</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Reader&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Retriever&quot;</span><span class="p">])</span>

<span class="c1"># 4) Execute QA</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Who is the director of Squid Game?&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Retriever&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="s2">&quot;Reader&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;answers&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above code, we simply put one document in an in-memory document store and built a pipeline combining BM25-based Retriever and Electra Reader trained on Korean KorQuAD data. When you put a query in pipeline.run(), Retriever finds the top 5 documents, and Reader extracts and returns the answer from among them. As a result, you can get an answer like “Hwang Dong-hyuk”.</p>
<p>Haystack’s powerful point is that <strong>components can be easily replaced or extended</strong> like this. You can switch to Dense Retriever, or attach generative models like GPT-3 as Generator instead of Reader. It also supports complex reasoning scenarios by sequentially/parallelly configuring multiple nodes in the middle like multi-hop QA.</p>
<p>In industrial settings, there are many cases of using Haystack to configure <strong>domain document search</strong> + <strong>QA</strong> services or RAG pipelines that inject external knowledge into <strong>chatbots</strong>. In summary, Haystack is a <strong>framework that ties search engines and NLP models together</strong>, a tool that enables building powerful document-based QA systems with relatively little code.</p>
</section>
<section id="crewai-role-based-multi-agent-framework">
<h3>4.3 CrewAI: Role-based Multi-Agent Framework<a class="headerlink" href="#crewai-role-based-multi-agent-framework" title="Link to this heading">#</a></h3>
<p><strong>CrewAI</strong> is one of the recently spotlighted <strong>AI agent</strong> frameworks, a platform that organizes multiple LLM agents in <strong>team (crew)</strong> form to perform <strong>collaborative work</strong>. While existing frameworks like LangChain were centered on single agents or chains, CrewAI specializes in <strong>role-based multi-agents</strong>. For example, to solve one problem, you can divide roles like <strong>Researcher, Analyst, Writer</strong>, etc., and configure each agent to act autonomously with their own tools and goals while collaborating overall to produce final results.</p>
<p>CrewAI’s concepts can be organized by main components as follows:</p>
<ul class="simple">
<li><p><strong>Crew (Team)</strong>: Organization or environment of all agents. Crew objects contain multiple agents and oversee their <strong>collaboration process</strong>. One Crew corresponds to one agent team for achieving specific goals.</p></li>
<li><p><strong>Agent</strong>: Independent <strong>autonomous AI</strong>, each with defined <strong>role</strong>, <strong>tools</strong>, and <strong>goals</strong>. For example, a “literature researcher” agent uses web search tools to collect information, and a “report writer” agent writes final reports with writing tools and appropriate style. Agents can delegate work to other agents or request results when needed (like people collaborating in teams).</p></li>
<li><p><strong>Process</strong>: Defines <strong>interaction rules</strong> or <strong>workflows</strong> of agents within Crew. For example, you can set up flows like “Step 1: Researcher collects materials -&gt; Step 2: Analyst summarizes -&gt; Step 3: Writer organizes” as processes. In CrewAI, such processes are also extended with the concept of <strong>Flow</strong>, and agent execution can be controlled according to events or conditions.</p></li>
</ul>
<p>Using CrewAI, developers can define each agent’s role and tools, create and execute Crews to <strong>automate complex tasks</strong>. Let’s look at a simple usage example. For instance, an agent team that finds materials and writes summary reports on given topics:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">crewai</span><span class="w"> </span><span class="kn">import</span> <span class="n">Crew</span><span class="p">,</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">tool</span>

<span class="c1"># Agent definition: searcher and writer</span>
<span class="n">searcher</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Researcher&quot;</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="s2">&quot;Information Collection&quot;</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">tool</span><span class="p">(</span><span class="s2">&quot;wiki_browser&quot;</span><span class="p">)])</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Writer&quot;</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="s2">&quot;Report Writing&quot;</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">tool</span><span class="p">(</span><span class="s2">&quot;text_editor&quot;</span><span class="p">)])</span>

<span class="c1"># Create Crew and add agents</span>
<span class="n">crew</span> <span class="o">=</span> <span class="n">Crew</span><span class="p">(</span><span class="n">agents</span><span class="o">=</span><span class="p">[</span><span class="n">searcher</span><span class="p">,</span> <span class="n">writer</span><span class="p">],</span> <span class="n">goal</span><span class="o">=</span><span class="s2">&quot;Write a 1-page summary report on the given topic&quot;</span><span class="p">)</span>
<span class="n">crew</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;Investigate and summarize traditional Korean food.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The above example is conceptual code, but it describes the flow of assigning roles and tools (e.g., wiki browser, text editor functions) to Agents, registering them in Crew, and then executing. During execution, the Researcher agent first searches Wikipedia to gather information, then passes the results to the Writer agent. The Writer organizes the received information and writes a summary report to produce the final answer. All these processes occur automatically without human intervention, and the CrewAI framework manages <strong>execution of each step and message exchange between agents</strong>.</p>
<p>CrewAI’s characteristics are <strong>high flexibility and control</strong>. Rather than simply running multiple agents independently, developers can design <strong>collaboration patterns</strong> as desired. Additionally, by finely configuring prompt rules, response formats, etc. for individual agents, <strong>specialized AIs</strong> can be built within teams. In practice, it can be applied to <strong>automated customer support</strong> (e.g., one agent understanding user intent, another agent searching FAQs, another agent generating responses) or <strong>research assistants</strong> (dividing roles to organize literature).</p>
<p>CrewAI is designed to be <strong>compatible with LangChain and others</strong> rather than being a completely new framework, allowing reuse of existing tool chains. However, due to the nature of multi-agent systems, <strong>safety mechanism design</strong> to prevent unexpected interactions or infinite loops is also important. CrewAI recommends setting <strong>restrictions and policies</strong> by role so agents only act within defined boundaries.</p>
<p>In summary, CrewAI is a framework that <strong>systematizes collaboration of role-based autonomous agents</strong>, helping multiple specialized LLMs perform more complex tasks through <strong>division of labor and cooperation</strong> instead of one giant LLM doing everything. This enables approaching multi-agent AI system development in an easy and standardized way.</p>
</section>
</section>
<section id="practice-bert-vs-mamba-model-comparison-experiment">
<h2>5. Practice: BERT vs Mamba Model Comparison Experiment<a class="headerlink" href="#practice-bert-vs-mamba-model-comparison-experiment" title="Link to this heading">#</a></h2>
<p>Having studied the theory and tools of Transformer-based models and the latest SSM (State Space Model) architecture Mamba, let’s perform a <strong>small experiment comparing the two models directly</strong>. The task is as follows:</p>
<ul class="simple">
<li><p><strong>Task</strong>: Korean sentence <strong>sentiment analysis</strong> (positive/negative classification). For example, we’ll classify sentences from the NSMC movie review dataset.</p></li>
<li><p><strong>Models</strong>: ① Transformer-based <strong>BERT</strong> (multilingual BERT or KoBERT, etc.), ② SSM-based <strong>Mamba</strong> (e.g., Mamba-130M level model).</p></li>
<li><p><strong>Comparison Items</strong>: Measure and compare <strong>classification accuracy</strong>, <strong>inference speed</strong>, and <strong>GPU memory usage</strong> of both models.</p></li>
<li><p><strong>Environment</strong>: Conduct under identical experimental conditions. (e.g., single RTX 3090 GPU, batch size 32, sequence length 128, etc.)</p></li>
</ul>
<p>The experiment consists of <strong>model preparation</strong>, <strong>inference and metric measurement</strong>, and <strong>result analysis</strong> stages.</p>
<section id="model-preparation-and-inference-code">
<h3>5.1 Model Preparation and Inference Code<a class="headerlink" href="#model-preparation-and-inference-code" title="Link to this heading">#</a></h3>
<p>First, we assume loading BERT and Mamba models fine-tuned on NSMC data through Hugging Face. (Currently, Mamba doesn’t have as many fine-tuning examples as Transformers, but we assume they’re prepared for this experiment.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="c1"># Load BERT model (example: koBERT NSMC fine-tuned model)</span>
<span class="n">bert_name</span> <span class="o">=</span> <span class="s2">&quot;skt/kobert-base-v1-nsmc&quot;</span>
<span class="n">tokenizer_bert</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_name</span><span class="p">)</span>
<span class="n">model_bert</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_name</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># Load Mamba model (example: Mamba 130M NSMC fine-tuned model)</span>
<span class="n">mamba_name</span> <span class="o">=</span> <span class="s2">&quot;kuotient/mamba-ko-130m-nsmc&quot;</span>  <span class="c1"># assumed path</span>
<span class="n">tokenizer_mamba</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">mamba_name</span><span class="p">)</span>
<span class="n">model_mamba</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">mamba_name</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
<p>In the above code, we loaded both models into GPU memory. Next, we write a prediction function. We measure <strong>inference speed</strong> and <strong>memory usage</strong> by inputting test set batches into the model at once:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">texts</span><span class="p">):</span>
    <span class="c1"># Encoding</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># Inference</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># Results and elapsed time</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
    <span class="k">return</span> <span class="n">preds</span><span class="p">,</span> <span class="n">elapsed</span>

<span class="c1"># Example data (batch of 64 sentences)</span>
<span class="n">batch_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This movie was really the best.&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">64</span>  <span class="c1"># 64 example sentences (should be diverse in practice)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">time_bert</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">model_bert</span><span class="p">,</span> <span class="n">tokenizer_bert</span><span class="p">,</span> <span class="n">batch_texts</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">time_mamba</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">model_mamba</span><span class="p">,</span> <span class="n">tokenizer_mamba</span><span class="p">,</span> <span class="n">batch_texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BERT processing time: </span><span class="si">{</span><span class="n">time_bert</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s, Mamba processing time: </span><span class="si">{</span><span class="n">time_mamba</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The evaluate_model function above takes a batch of 64 sentences, tokenizes them, performs model inference, and measures the elapsed time. Here, torch.cuda.synchronize() is used to accurately measure the end time of GPU operations. The output shows <strong>batch inference time</strong> for BERT and Mamba respectively.</p>
<p>Accuracy is measured through a pre-prepared <strong>validation dataset</strong>. We calculated accuracy by obtaining model predictions on the NSMC validation set (about 50,000 sentences) and comparing with correct answers. Additionally, GPU <strong>memory usage</strong> was checked using PyTorch’s torch.cuda.max_memory_allocated() to see peak usage.</p>
</section>
<section id="result-comparison-accuracy-speed-memory">
<h3>5.2 Result Comparison: Accuracy, Speed, Memory<a class="headerlink" href="#result-comparison-accuracy-speed-memory" title="Link to this heading">#</a></h3>
<p>The measurement results of the experiment are summarized below (hypothetical numerical examples):</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-left"><p>Validation Accuracy</p></th>
<th class="head text-left"><p>Inference Speed<sup>*1</sup></p></th>
<th class="head text-left"><p>GPU Memory Usage<sup>*2</sup></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>BERT-base</strong> (110M)</p></td>
<td class="text-left"><p>88.0%</p></td>
<td class="text-left"><p><strong>120</strong> samples/sec</p></td>
<td class="text-left"><p>800 MB</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Mamba-small</strong> (130M)</p></td>
<td class="text-left"><p>85.5%</p></td>
<td class="text-left"><p>100 samples/sec</p></td>
<td class="text-left"><p><strong>600 MB</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p><small><sup>*1</sup>Inference speed is samples processed per second (based on batch size=64, seq length=128)</small><br />
<small><sup>*2</sup>GPU memory usage is approximate peak value of model+utilization memory during inference</small></p>
<p>Comparing the three metrics in a graph:</p>
<p><em>Figure 3: Performance comparison of BERT and Mamba models. BERT slightly leads in sentiment classification accuracy and inference speed, but Mamba has superior GPU memory efficiency (dark blue: BERT, orange: Mamba).</em></p>
<p>As can be seen from the table and figure, in terms of <strong>classification accuracy</strong>, BERT-base shows about 88% accuracy while Mamba (similar scale model) records about 85%, <strong>slightly behind</strong>. This may be because the Mamba architecture is not yet as specialized for Korean data as Transformers, or pre-training is insufficient. On the other hand, <strong>inference speed</strong> shows BERT being slightly faster under these experimental conditions. Since Mamba’s linear time advantage is not prominent up to sequence length 128, the analysis shows that BERT, with fewer parameters and mature optimization, shows slightly higher <strong>throughput</strong>.</p>
<p><strong>GPU memory usage</strong> is lower for the Mamba model. With the same batch and sequence length, BERT’s memory occupancy increases due to intermediate outputs like attention matrices, while Mamba’s memory requirements are relatively gentle due to <strong>linear increase with sequence length characteristic of state space models</strong>. In the above experiment, BERT used about 0.8GB and Mamba about 0.6GB of GPU memory. If sequence length or batch size is greatly increased, this difference can widen further (BERT’s memory usage increases as O(n²), quickly reaching memory limits with large inputs, while Mamba increases as O(n), making it much more <strong>memory efficient</strong>).</p>
<p>Another major difference is <strong>maximum processable context length</strong>. BERT series are generally limited to <strong>512 token input length</strong>, but Mamba models can process <strong>thousands to tens of thousands of tokens</strong> by design. The actual Mamba-2.8B model supports up to 8,000 tokens, and research versions aim for over 1 million tokens. Therefore, SSM models like Mamba have great advantages in tasks requiring long document analysis.</p>
</section>
</section>
<section id="experiment-summary-and-implications">
<h2>6. Experiment Summary and Implications<a class="headerlink" href="#experiment-summary-and-implications" title="Link to this heading">#</a></h2>
<p>Through the <strong>BERT vs Mamba comparison experiment</strong>, we examined the characteristics and pros/cons of both models. In summary, <strong>existing BERT (Transformer)</strong> models still show high accuracy and stable speed for medium-length inputs and are <strong>still efficient in short input environments</strong>. On the other hand, <strong>Mamba (SSM)</strong> models show potential for ultra-long context processing and <strong>efficiency without performance degradation</strong> as input length increases. However, at the current point, Transformer series are validated in terms of model completeness and optimization, while Mamba is in the research stage, so <strong>Transformers have some advantage in general tasks</strong> (e.g., accuracy comparison in this experiment).</p>
<p><strong>Which model is suitable for which situation?</strong> First, <strong>input sequence length</strong> is the determining factor. For <strong>short sentence-level tasks</strong> (e.g., sentence classification, short-answer QA, etc.), using Transformers like BERT is advantageous in terms of implementation ease and performance. Rich pre-training and tuning techniques are accumulated, making it easy to achieve high accuracy with short inference latency. For <strong>long context or document-level tasks</strong> (e.g., summarizing documents of thousands of words, sentiment analysis of long texts, etc.), linear architectures like Mamba may be advantageous. This is because Mamba can efficiently process input lengths that are impossible or would consume many resources with Transformers. In fact, Mamba shows the ability to process up to <strong>1 million tokens</strong>, suggesting the possibility of opening the era of ultra-long context LLMs.</p>
<p>From an <strong>inference speed</strong> perspective, judgment should also be based on context length. With short inputs, the two models may have similar speeds or Transformers may be faster, but as input length increases, Transformers <strong>slow down dramatically</strong> as O(n²), so reports suggest that Mamba will show <strong>up to 5x faster inference</strong> in sufficiently long contexts. Additionally, Mamba has strengths in time series data and continuous stream processing due to the nature of state space models, and also has generality that can be widely applied to <strong>speech and sequence data processing beyond language</strong>.</p>
<p><strong>Service/Production Application Implications:</strong> Currently in production environments, Transformer series (e.g., BERT, GPT) models are mature and widely used in terms of performance and tooling. Mamba is a very promising technology but <strong>library support, community, and pre-trained model pools</strong> are not as rich as Transformers yet. Therefore, more stability validation may be needed to immediately introduce Mamba as a replacement in industry. However, for services that have had difficulty with ultra-long context processing due to <strong>memory capacity limits or latency issues</strong>, introducing models like Mamba in the future could provide a breakthrough. For example, in <strong>long legal document analysis services</strong> or <strong>chatbots that need to maintain long-term conversation history</strong>, Mamba architecture has the potential to be a game changer.</p>
<p>Additionally, attention should be paid to future hybrid models (e.g., <strong>Jamba: Transformer+Mamba mixed experts</strong>) and competition with other linear sequence models. Currently, it can be viewed as <strong>Transformer’s universality vs. Mamba’s specificity</strong>, and in actual production, approaches to <strong>mutually complementary utilization</strong> of both methods are also considered. For example, a system that processes general conversations with Transformers and switches to Mamba mode when ultra-long context processing is needed for specific requests would be possible.</p>
<p>In summary, <strong>BERT</strong> and <strong>Mamba</strong> each have their strengths and different use cases. <strong>Mature BERT series</strong> are suitable for <strong>short inputs/existing tasks</strong>, while <strong>Mamba</strong> shows potential for <strong>long inputs/new expansion tasks</strong>. If research and technological development continue, it is expected that cases where SSMs like Mamba complement or replace Transformer limitations will gradually increase. When applying to actual services, current model stability, support tools, licenses, etc. should be comprehensively considered, but from a <strong>future-oriented perspective, architectural innovation for ultra-long context and high-efficiency inference is being realized</strong>, and this comparison experiment of the two models provides meaningful insights.</p>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>PyTorch Autograd Official Documentation – <em>“Autograd: Automatic Differentiation”</em></p></li>
<li><p>Tri Dao Blog – <em>“FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision”</em></p></li>
<li><p>Hugging Face Transformers Documentation &amp; Tutorials</p></li>
<li><p>Databricks DSPy Introduction – <em>Programming, not prompting</em></p></li>
<li><p>Deepset Haystack Documentation – <em>Flexible Open Source QA Framework</em></p></li>
<li><p>CrewAI Docs – <em>Role-based Autonomous Agent Teams</em></p></li>
<li><p>Mamba Architecture Paper – <em>Mamba: A Linear-Time State Space Model for Long-Range Sequences</em></p></li>
<li><p><em>“Mamba Explained”</em> - The Gradient</p></li>
<li><p><em>“Improving VTE Identification through Language Models from Radiology Reports: A Comparative Study of Mamba, Phi-3 Mini, and BERT”</em></p></li>
<li><p><em>“FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision”</em> - arXiv</p></li>
<li><p>GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention</p></li>
<li><p><em>“Programming, Not Prompting: A Hands-on Guide to DSPy”</em> - Medium</p></li>
<li><p>DSPy Official Documentation</p></li>
<li><p>Haystack - GeeksforGeeks</p></li>
<li><p><em>“Forget ChatGPT. CrewAI is the Future of AI Automation and Multi-Agent Systems”</em> - Reddit</p></li>
<li><p>Introduction - CrewAI Documentation</p></li>
<li><p><em>“Building a multi agent system using CrewAI”</em> - Medium</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week01/qna.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</p>
      </div>
    </a>
    <a class="right-next"
       href="../week03/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-basics-tensors-and-autograd">1. PyTorch Basics: Tensors and Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flashattention-3-fast-attention-implementation">2. FlashAttention-3: Fast Attention Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-transformers-practice">3. Hugging Face Transformers Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-pre-trained-models-and-tokenizers">3.1 Loading Pre-trained Models and Tokenizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-encoding-using-tokenizer">3.2 Input Encoding Using Tokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-using-classification-pipeline">3.3 Prediction Using Classification Pipeline</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-latest-nlp-frameworks">4. Introduction to Latest NLP Frameworks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dspy-declarative-prompt-programming">4.1 DSPy: Declarative Prompt Programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#haystack-document-based-search-and-reasoning">4.2 Haystack: Document-based Search and Reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#crewai-role-based-multi-agent-framework">4.3 CrewAI: Role-based Multi-Agent Framework</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-bert-vs-mamba-model-comparison-experiment">5. Practice: BERT vs Mamba Model Comparison Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-preparation-and-inference-code">5.1 Model Preparation and Inference Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#result-comparison-accuracy-speed-memory">5.2 Result Comparison: Accuracy, Speed, Memory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-summary-and-implications">6. Experiment Summary and Implications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
