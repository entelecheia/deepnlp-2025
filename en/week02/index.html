
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 2: PyTorch 2.x and Latest Deep Learning Frameworks &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week02/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 3: Efficient Fine-Tuning with Modern PEFT Techniques" href="../week03/index.html" />
    <link rel="prev" title="Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A" href="../week01/qna.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Deep Learning for Natural Language Processing (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer and Next-Generation Architectures</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 2: PyTorch 2.x and Latest Deep Learning Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: Advanced Prompting Techniques and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM Evaluation Paradigms and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: Advances in Multimodal NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: Ultra-Long Context Processing and Efficient Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workshops/index.html">LLM From Scratch Workshop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workshops/week01.html">Week 1 Workshop: LLM Overview and Development Environment Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">Team Project Guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/en/week02/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek02/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week02/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 2: PyTorch 2.x and Latest Deep Learning Frameworks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-2-x-and-torch-compile-the-compiler-revolution">1. PyTorch 2.x and torch.compile: The Compiler Revolution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-torch-compile-works">1.1 How torch.compile Works</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-acquisition-torchdynamo">1. Graph Acquisition (TorchDynamo)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ahead-of-time-automatic-differentiation-aotautograd">2. Ahead-of-Time Automatic Differentiation (AOTAutograd)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-lowering-primtorch">3. Graph Lowering (PrimTorch)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-compilation-torchinductor">4. Graph Compilation (TorchInductor)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-improving-model-inference-speed-with-torch-compile">1.2 Practice: Improving Model Inference Speed with torch.compile</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flashattention-3-attention-optimization-through-hardware-acceleration">2. FlashAttention-3: Attention Optimization through Hardware Acceleration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-principles-of-flashattention">2.1 Core Principles of FlashAttention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-acceleration-of-flashattention-3">2.2 Hardware Acceleration of FlashAttention-3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-enabling-flashattention-in-hugging-face-transformers">2.3 Practice: Enabling FlashAttention in Hugging Face Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Checkpoint Questions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-practice-direct-use-of-pytorch-scaled-dot-product-attention">2.4 Additional Practice: Direct Use of PyTorch scaled_dot_product_attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-transformers-ecosystem-latest-trends-and-practice">3. Hugging Face Transformers Ecosystem: Latest Trends and Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latest-trends">3.1 Latest Trends</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-korean-sentiment-analysis-using-pipeline-api">3.2 Practice: Korean Sentiment Analysis Using Pipeline API</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-agent-frameworks-the-era-of-automation-and-collaboration">4. AI Agent Frameworks: The Era of Automation and Collaboration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-major-ai-agent-frameworks">4.1 Comparison of Major AI Agent Frameworks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dspy-declarative-prompt-programming">4.2 DSPy: Declarative Prompt Programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#haystack-document-based-search-and-reasoning">4.3 Haystack: Document-based Search and Reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#crewai-role-based-multi-agent-framework">4.4 CrewAI: Role-based Multi-Agent Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Checkpoint Questions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph-state-based-multi-agent-orchestration">4.5 LangGraph: State-based Multi-Agent Orchestration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-bert-vs-mamba-model-comparison-experiment">5. Practice: BERT vs Mamba Model Comparison Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">5.1 Environment Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-loading-imdb">5.2 Dataset Loading (IMDB)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-tokenizer-loading">5.3 Model and Tokenizer Loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-function-accuracy-speed-memory">5.4 Evaluation Function (Accuracy, Speed, Memory)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-results-and-interpretation">5.5 Example Results and Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-summary-and-implications">6. Experiment Summary and Implications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#major-papers-and-research-materials">Major Papers and Research Materials</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-documentation-and-implementations">Technical Documentation and Implementations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-resources-and-blogs">Online Resources and Blogs</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-2-pytorch-2-x-and-latest-deep-learning-frameworks">
<h1>Week 2: PyTorch 2.x and Latest Deep Learning Frameworks<a class="headerlink" href="#week-2-pytorch-2-x-and-latest-deep-learning-frameworks" title="Link to this heading">#</a></h1>
<section id="pytorch-2-x-and-torch-compile-the-compiler-revolution">
<h2>1. PyTorch 2.x and torch.compile: The Compiler Revolution<a class="headerlink" href="#pytorch-2-x-and-torch-compile-the-compiler-revolution" title="Link to this heading">#</a></h2>
<p>PyTorch 2.x is an innovative update that has changed the paradigm of deep learning frameworks. While maintaining the flexibility and Pythonic development experience provided by the existing <strong>Eager Mode</strong> approach, it has added a powerful feature that maximizes model execution performance with just a single line of code: <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>. This is evaluated as an innovation that “simultaneously satisfies the flexibility of research and the speed of production.”</p>
<section id="how-torch-compile-works">
<h3>1.1 How torch.compile Works<a class="headerlink" href="#how-torch-compile-works" title="Link to this heading">#</a></h3>
<p>The performance improvement of <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> is achieved through the organic collaboration of four core technologies: <strong>TorchDynamo</strong>, <strong>AOTAutograd</strong>, <strong>PrimTorch</strong>, and <strong>TorchInductor</strong>.</p>
<section id="graph-acquisition-torchdynamo">
<h4>1. Graph Acquisition (TorchDynamo)<a class="headerlink" href="#graph-acquisition-torchdynamo" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Role</strong>: Analyzes Python bytecode to safely capture PyTorch operations as FX graphs</p></li>
<li><p><strong>Core Technology</strong>: “Guard” mechanism that perfectly supports dynamic Python characteristics (conditionals, loops)</p></li>
<li><p><strong>Advantage</strong>: When code paths change, only the relevant parts are processed in eager mode while the rest runs compiled code</p></li>
</ul>
</section>
<section id="ahead-of-time-automatic-differentiation-aotautograd">
<h4>2. Ahead-of-Time Automatic Differentiation (AOTAutograd)<a class="headerlink" href="#ahead-of-time-automatic-differentiation-aotautograd" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Role</strong>: Generates pre-optimized backward graphs based on forward graphs</p></li>
<li><p><strong>Advantage</strong>: Pre-analyzes the entire computation graph to optimize gradient calculation processes and reduce memory usage</p></li>
</ul>
</section>
<section id="graph-lowering-primtorch">
<h4>3. Graph Lowering (PrimTorch)<a class="headerlink" href="#graph-lowering-primtorch" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Role</strong>: Standardizes over 2,000 PyTorch operators into 250 core primitive operators</p></li>
<li><p><strong>Advantage</strong>: Improves compatibility and portability across various hardware backends (GPU, CPU, custom accelerators)</p></li>
</ul>
</section>
<section id="graph-compilation-torchinductor">
<h4>4. Graph Compilation (TorchInductor)<a class="headerlink" href="#graph-compilation-torchinductor" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Role</strong>: Converts primitive operator graphs into hardware-optimized machine code</p></li>
<li><p><strong>Core Technology</strong>: Dynamic generation of high-performance CUDA kernels using Triton compiler on GPU, C++/OpenMP on CPU</p></li>
</ul>
<p>Through these multi-stage optimizations, <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> achieved an <strong>average 51%</strong> training speed improvement across 163 model benchmarks.</p>
</section>
</section>
<section id="practice-improving-model-inference-speed-with-torch-compile">
<h3>1.2 Practice: Improving Model Inference Speed with torch.compile<a class="headerlink" href="#practice-improving-model-inference-speed-with-torch-compile" title="Link to this heading">#</a></h3>
<p>Let’s directly compare the performance of <strong>Eager Mode</strong> and <strong>Compiled Mode</strong> by applying <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> to a simple <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>. While compilation incurs overhead from graph capture and code generation on the first run, subsequent repeated calls show significantly faster speeds that more than offset this cost.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="c1"># Check GPU availability</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Define a simple neural network model</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SimpleNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 1. Eager mode performance measurement</span>
<span class="c1"># Warmup: to exclude potential overhead from initial execution</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">eager_duration</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Eager mode (100 runs): </span><span class="si">{</span><span class="n">eager_duration</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>

<span class="c1"># 2. Apply torch.compile (compiled mode)</span>
<span class="c1"># mode=&quot;reduce-overhead&quot; reduces framework overhead, beneficial for small model calls</span>
<span class="n">compiled_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;reduce-overhead&quot;</span><span class="p">)</span>

<span class="c1"># Warmup and first compilation run (compilation overhead occurs)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">compiled_duration</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compiled mode (100 runs): </span><span class="si">{</span><span class="n">compiled_duration</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>

<span class="c1"># 3. Calculate performance improvement</span>
<span class="n">speedup</span> <span class="o">=</span> <span class="n">eager_duration</span> <span class="o">/</span> <span class="n">compiled_duration</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Speedup with torch.compile: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Example execution results:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">device</span><span class="p">:</span> <span class="n">cuda</span>
<span class="n">Eager</span> <span class="n">mode</span> <span class="p">(</span><span class="mi">100</span> <span class="n">runs</span><span class="p">):</span> <span class="mf">0.0481</span> <span class="n">seconds</span>
<span class="n">Compiled</span> <span class="n">mode</span> <span class="p">(</span><span class="mi">100</span> <span class="n">runs</span><span class="p">):</span> <span class="mf">0.0215</span> <span class="n">seconds</span>
<span class="n">Speedup</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">:</span> <span class="mf">2.24</span><span class="n">x</span>
</pre></div>
</div>
</section>
<section id="checkpoint-questions">
<h3>Checkpoint Questions<a class="headerlink" href="#checkpoint-questions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What roles do the four core technologies of <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> (TorchDynamo, AOTAutograd, PrimTorch, TorchInductor) play?</p></li>
<li><p>Why was the <code class="docutils literal notranslate"><span class="pre">mode=&quot;reduce-overhead&quot;</span></code> option used in the above example? What difference would there be with the default mode for small models?</p></li>
<li><p>What are the main advantages and limitations of <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> compared to the existing eager execution mode?</p></li>
</ul>
</section>
</section>
<section id="flashattention-3-attention-optimization-through-hardware-acceleration">
<h2>2. FlashAttention-3: Attention Optimization through Hardware Acceleration<a class="headerlink" href="#flashattention-3-attention-optimization-through-hardware-acceleration" title="Link to this heading">#</a></h2>
<p>The <strong>attention mechanism</strong>, which is the core of the Transformer architecture and was the main performance bottleneck, has achieved revolutionary progress with the advent of FlashAttention. Traditional attention had limitations in processing long sequences due to O(N²) memory and computational complexity for sequence length N. This was because it had to store and read back massive N × N attention score matrices in GPU’s HBM (High Bandwidth Memory).</p>
<section id="core-principles-of-flashattention">
<h3>2.1 Core Principles of FlashAttention<a class="headerlink" href="#core-principles-of-flashattention" title="Link to this heading">#</a></h3>
<p><strong>FlashAttention</strong> utilizes <strong>tiling</strong> techniques and GPU’s very fast internal SRAM (Static RAM) to solve memory I/O bottlenecks. Instead of computing entire matrices at once, it divides inputs into small blocks (tiles) and performs attention computation in SRAM, storing only intermediate results in HBM, dramatically reducing the number of data exchanges with HBM.</p>
</section>
<section id="hardware-acceleration-of-flashattention-3">
<h3>2.2 Hardware Acceleration of FlashAttention-3<a class="headerlink" href="#hardware-acceleration-of-flashattention-3" title="Link to this heading">#</a></h3>
<p><strong>FlashAttention-3</strong> maximizes the utilization of hardware acceleration features built into NVIDIA’s latest Hopper architecture (H100/H200 GPUs, etc.):</p>
<ul class="simple">
<li><p><strong>TMA (Tensor Memory Accelerator)</strong>: Dedicated hardware that asynchronously accelerates tensor data movement between HBM and SRAM</p></li>
<li><p><strong>WGMMA (Warpgroup Matrix Multiply-Accumulate)</strong>: Hardware unit that processes matrix multiplication-accumulation operations more efficiently</p></li>
<li><p><strong>FP8 Support</strong>: Supports 8-bit floating-point (FP8) low-precision format, nearly doubling memory usage and throughput while minimizing precision loss</p></li>
</ul>
<p>As a result, FlashAttention-3 achieved <strong>1.5x to 2.0x</strong> faster speed compared to FlashAttention-2 on H100 GPU.</p>
</section>
<section id="practice-enabling-flashattention-in-hugging-face-transformers">
<h3>2.3 Practice: Enabling FlashAttention in Hugging Face Transformers<a class="headerlink" href="#practice-enabling-flashattention-in-hugging-face-transformers" title="Link to this heading">#</a></h3>
<p>Hugging Face’s 🤗 Transformers library tightly integrates FlashAttention, allowing it to be easily activated with just the <code class="docutils literal notranslate"><span class="pre">attn_implementation</span></code> argument when loading models. This enables <strong>significant inference speed and memory efficiency improvements</strong> with minimal changes to existing code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Assuming GPU is Hopper architecture or above and flash-attn library is installed</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;openai/gpt-oss-20b&quot;</span>  <span class="c1"># Example model ID supporting FlashAttention-3</span>

<span class="c1"># 1. Load model with standard attention implementation (Eager)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">model_eager</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model with standard attention loaded.&quot;</span><span class="p">)</span>

<span class="c1"># 2. Load model with FlashAttention-3 implementation</span>
<span class="c1"># This model can internally use vLLM&#39;s FlashAttention-3 kernels,</span>
<span class="c1"># which are automatically downloaded from the hub through the &#39;kernels&#39; package.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">model_flash</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">model_id</span><span class="p">,</span>
        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;kernels-community/vllm-flash-attn3&quot;</span>  <span class="c1"># Enable FlashAttention-3</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model with FlashAttention-3 loaded successfully.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Note: This requires a compatible GPU (e.g., NVIDIA Hopper series).&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FlashAttention is not installed or the environment does not support it.&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred while loading with FlashAttention: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Example execution results:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span> <span class="k">with</span> <span class="n">standard</span> <span class="n">attention</span> <span class="n">loaded</span><span class="o">.</span>
<span class="n">Model</span> <span class="k">with</span> <span class="n">FlashAttention</span><span class="o">-</span><span class="mi">3</span> <span class="n">loaded</span> <span class="n">successfully</span><span class="o">.</span>
<span class="n">Note</span><span class="p">:</span> <span class="n">This</span> <span class="n">requires</span> <span class="n">a</span> <span class="n">compatible</span> <span class="n">GPU</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">NVIDIA</span> <span class="n">Hopper</span> <span class="n">series</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>Checkpoint Questions<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What problems does FlashAttention solve in the existing attention mechanism? How does the tiling technique work?</p></li>
<li><p>What are the main hardware acceleration features of NVIDIA Hopper architecture utilized in FlashAttention-3?</p></li>
<li><p>What conditions are required when using the <code class="docutils literal notranslate"><span class="pre">attn_implementation=&quot;kernels-community/vllm-flash-attn3&quot;</span></code> option, and what happens when these conditions are not met?</p></li>
</ul>
</section>
<section id="additional-practice-direct-use-of-pytorch-scaled-dot-product-attention">
<h3>2.4 Additional Practice: Direct Use of PyTorch scaled_dot_product_attention<a class="headerlink" href="#additional-practice-direct-use-of-pytorch-scaled-dot-product-attention" title="Link to this heading">#</a></h3>
<p>From PyTorch 2.0, the core API includes the built-in <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.scaled_dot_product_attention</span></code> (SDPA) function. This function automatically detects GPU architecture and input tensor properties (dtype, mask presence, etc.) in the backend to select the most efficient attention implementation. That is, under conditions compatible with Ampere architecture or above GPUs, it <strong>automatically calls FlashAttention kernels or memory-efficient kernels</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="c1"># Use GPU and half-precision (FP16/BF16) to use FlashAttention path</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>  <span class="c1"># or torch.bfloat16</span>

<span class="c1"># Generate random tensors with batch=1, heads=8, seq_len=1024, embed=64</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Force use of specific kernel only to verify operation (for debugging/testing)</span>
<span class="c1"># enable_flash=True: Attempt to use FlashAttention kernel</span>
<span class="c1"># enable_math=False: Disable pure PyTorch math implementation</span>
<span class="c1"># enable_mem_efficient=False: Disable memory-efficient implementation</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">sdp_kernel</span><span class="p">(</span><span class="n">enable_flash</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">enable_math</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">enable_mem_efficient</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FlashAttention kernel was successfully used.&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output shape:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to use FlashAttention kernel exclusively: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code uses the <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.sdp_kernel</span></code> context manager to <strong>force use of only the FlashAttention path</strong>. If the GPU supports it and input conditions (half-precision, no mask, etc.) are met, the FlashAttention kernel is successfully called internally. If conditions are not met, PyTorch outputs useful warning messages like “Flash attention kernel not used because…” and safely falls back to other implementations outside the context manager. This allows developers to easily verify whether the intended optimization is being applied.</p>
</section>
<section id="id2">
<h3>Checkpoint Questions<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What happens when <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> function is executed in FP32? Why is FlashAttention not used?</p></li>
<li><p>Will FlashAttention be used if the <code class="docutils literal notranslate"><span class="pre">attn_mask</span></code> argument is provided? What mask types does FlashAttention support?</p></li>
<li><p>What hints can be obtained through the warning messages provided by PyTorch?</p></li>
</ul>
</section>
</section>
<section id="hugging-face-transformers-ecosystem-latest-trends-and-practice">
<h2>3. Hugging Face Transformers Ecosystem: Latest Trends and Practice<a class="headerlink" href="#hugging-face-transformers-ecosystem-latest-trends-and-practice" title="Link to this heading">#</a></h2>
<p>Hugging Face 🤗 Transformers library has evolved beyond a simple <strong>model repository</strong> into a massive integrated platform that supports making the latest AI technologies easily accessible and utilizable by anyone.</p>
<section id="latest-trends">
<h3>3.1 Latest Trends<a class="headerlink" href="#latest-trends" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Rapid Support for Latest Model Architectures</strong>: Support for <strong>multimodal models</strong> has been significantly strengthened, including the latest LLMs like Vault-GEMMA and EmbeddingGemma, as well as Florence-2 (unified vision) and SAM-2 (advanced segmentation).</p></li>
<li><p><strong>Integration of Advanced Quantization Technologies</strong>: Natively supports <strong>4-bit floating-point quantization</strong> methods like <strong>MXFP4</strong> introduced with OpenAI’s GPT-OSS model. This is more advantageous for dynamic range representation than existing 4-bit integer (INT4) quantization, dramatically reducing memory usage while minimizing accuracy loss, such as loading 120B parameter models on a single 80GB GPU.</p></li>
<li><p><strong>Zero-Build Kernels</strong>: Through a package called <code class="docutils literal notranslate"><span class="pre">kernels</span></code>, <strong>pre-compiled high-performance kernels</strong> like FlashAttention-3 and Megablocks MoE kernels can be downloaded directly from the hub and used. This eliminates the complex and error-prone process of users compiling source code directly in their own environments.</p></li>
</ul>
</section>
<section id="practice-korean-sentiment-analysis-using-pipeline-api">
<h3>3.2 Practice: Korean Sentiment Analysis Using Pipeline API<a class="headerlink" href="#practice-korean-sentiment-analysis-using-pipeline-api" title="Link to this heading">#</a></h3>
<p>Hugging Face’s <strong><code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API</strong> is the most convenient and intuitive tool that abstracts the entire process from tokenization, model inference, to post-processing into just a few lines of code. Let’s analyze the positive/negative sentiment of sentences using a model fine-tuned on Korean movie review data (NSMC).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Create a pipeline using a model fine-tuned for Korean sentiment analysis</span>
<span class="c1"># Model: WhitePeak/bert-base-cased-Korean-sentiment (fine-tuned on NSMC dataset)</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;sentiment-analysis&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;WhitePeak/bert-base-cased-Korean-sentiment&quot;</span>
<span class="p">)</span>

<span class="c1"># Sentences to analyze</span>
<span class="n">reviews</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;이 영화는 제 인생 최고의 영화입니다. 배우들의 연기가 정말 인상 깊었어요.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;기대했던 것보다는 조금 아쉬웠어요. 스토리가 너무 평범했습니다.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;시간 가는 줄 모르고 봤네요. 강력 추천합니다!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;음악은 좋았지만 전체적으로 지루한 느낌을 지울 수 없었다.&quot;</span>
<span class="p">]</span>

<span class="c1"># Execute sentiment analysis</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">reviews</span><span class="p">)</span>

<span class="c1"># Output results</span>
<span class="k">for</span> <span class="n">review</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">reviews</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
    <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;긍정&quot;</span> <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;LABEL_1&#39;</span> <span class="k">else</span> <span class="s2">&quot;부정&quot;</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;리뷰: </span><span class="se">\&quot;</span><span class="si">{</span><span class="n">review</span><span class="si">}</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;결과: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2"> (신뢰도: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Example execution results:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">리뷰</span><span class="p">:</span> <span class="s2">&quot;이 영화는 제 인생 최고의 영화입니다. 배우들의 연기가 정말 인상 깊었어요.&quot;</span>
<span class="n">결과</span><span class="p">:</span> <span class="n">긍정</span> <span class="p">(</span><span class="n">신뢰도</span><span class="p">:</span> <span class="mf">0.9985</span><span class="p">)</span>

<span class="n">리뷰</span><span class="p">:</span> <span class="s2">&quot;기대했던 것보다는 조금 아쉬웠어요. 스토리가 너무 평범했습니다.&quot;</span>
<span class="n">결과</span><span class="p">:</span> <span class="n">부정</span> <span class="p">(</span><span class="n">신뢰도</span><span class="p">:</span> <span class="mf">0.9978</span><span class="p">)</span>

<span class="n">리뷰</span><span class="p">:</span> <span class="s2">&quot;시간 가는 줄 모르고 봤네요. 강력 추천합니다!&quot;</span>
<span class="n">결과</span><span class="p">:</span> <span class="n">긍정</span> <span class="p">(</span><span class="n">신뢰도</span><span class="p">:</span> <span class="mf">0.9982</span><span class="p">)</span>

<span class="n">리뷰</span><span class="p">:</span> <span class="s2">&quot;음악은 좋았지만 전체적으로 지루한 느낌을 지울 수 없었다.&quot;</span>
<span class="n">결과</span><span class="p">:</span> <span class="n">부정</span> <span class="p">(</span><span class="n">신뢰도</span><span class="p">:</span> <span class="mf">0.9969</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>Checkpoint Questions<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What is <strong>Zero-Build Kernels</strong> among the latest trends in Hugging Face Transformers, and what advantages does it provide?</p></li>
<li><p>What meanings do <code class="docutils literal notranslate"><span class="pre">LABEL_0</span></code> and <code class="docutils literal notranslate"><span class="pre">LABEL_1</span></code> output from the sentiment analysis pipeline above have?</p></li>
<li><p>Why might results differ when running with different Korean sentiment analysis models?</p></li>
</ul>
</section>
</section>
<section id="ai-agent-frameworks-the-era-of-automation-and-collaboration">
<h2>4. AI Agent Frameworks: The Era of Automation and Collaboration<a class="headerlink" href="#ai-agent-frameworks-the-era-of-automation-and-collaboration" title="Link to this heading">#</a></h2>
<p>The development of LLMs has opened the <strong>AI agent</strong> paradigm that goes beyond a single model performing one task, to <strong>autonomously using multiple tools</strong> and <strong>collaborating with other agents to achieve complex goals</strong>. Various frameworks have emerged to systematically support this, each with distinct philosophies and strengths.</p>
<section id="comparison-of-major-ai-agent-frameworks">
<h3>4.1 Comparison of Major AI Agent Frameworks<a class="headerlink" href="#comparison-of-major-ai-agent-frameworks" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Framework</p></th>
<th class="head text-left"><p>Core Philosophy</p></th>
<th class="head text-left"><p>Architecture Style</p></th>
<th class="head text-left"><p>Main Use Cases</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>LangGraph</strong></p></td>
<td class="text-left"><p>Explicit Control and State-based Orchestration</p></td>
<td class="text-left"><p>Directed Acyclic Graph (DAG) with state, allowing cycles</p></td>
<td class="text-left"><p>Building reliable and auditable complex multi-stage agents where human supervision is important</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>CrewAI</strong></p></td>
<td class="text-left"><p>Role-based Collaborative Intelligence</p></td>
<td class="text-left"><p>Hierarchical, role-based <strong>multi-agent system</strong></p></td>
<td class="text-left"><p>Automating complex business workflows with clear role division (e.g., market analysis team)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>LlamaIndex</strong></p></td>
<td class="text-left"><p>Data-centric Agents and Advanced RAG</p></td>
<td class="text-left"><p>Data-centric, event-based workflow</p></td>
<td class="text-left"><p>Building <strong>question-answering and reasoning</strong> systems for large-scale private/unstructured databases</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Haystack</strong></p></td>
<td class="text-left"><p>Production-ready Modular Pipeline</p></td>
<td class="text-left"><p>Modular, branching/looping capable <strong>pipeline</strong></p></td>
<td class="text-left"><p>Building scalable and robust production-grade <strong>AI applications</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>DSPy</strong></p></td>
<td class="text-left"><p>Declarative LM Programming (“Programming, not prompting”)</p></td>
<td class="text-left"><p>Declarative, optimizable <strong>pipeline</strong></p></td>
<td class="text-left"><p>Tasks requiring highest performance by replacing manual prompt tuning with <strong>data-driven optimization</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p>As shown in the table, <strong>LangGraph</strong> focuses on <strong>long-term execution and reliability</strong> by explicitly managing agent states and control flow, while <strong>CrewAI</strong> emphasizes <strong>collaboration</strong> among agents with different expertise. <strong>LlamaIndex</strong> aims for data-centric agents combined with vast knowledge bases, and <strong>Haystack</strong> is strong in practical application of <strong>modular combination pipelines</strong> like search-reasoning. Finally, <strong>DSPy</strong> abstracts prompt engineering itself to advance LLM utilization in a <strong>declarative programming</strong> style. Understanding the philosophy and structure of each framework allows selecting the most appropriate tool based on the nature of the problem to be solved.</p>
</section>
<section id="dspy-declarative-prompt-programming">
<h3>4.2 DSPy: Declarative Prompt Programming<a class="headerlink" href="#dspy-declarative-prompt-programming" title="Link to this heading">#</a></h3>
<p><strong>DSPy</strong> stands for <em>Declarative Self-Improving Python</em> and is a <strong>declarative prompt programming</strong> framework released by Databricks. It reduces the complexity of managing <strong>long prompt strings</strong> that arise when directly handling LLMs, and allows you to create AI programs with modular composition as if <strong>writing code</strong>. In short, it’s designed with the philosophy of “don’t hardcode prompts, write them <strong>like programming</strong>.”</p>
<p>DSPy’s core concepts are divided into three: <strong>LM</strong>, <strong>Signature</strong>, <strong>Module</strong>:</p>
<ul class="simple">
<li><p><strong>LM</strong>: Specifies the language model to use. For example, if you set desired models like OpenAI API’s GPT-4, HuggingFace’s Llama2, etc. with <code class="docutils literal notranslate"><span class="pre">dspy.LM(...)</span></code> and <code class="docutils literal notranslate"><span class="pre">dspy.configure(lm=...)</span></code>, then all subsequent modules generate results through this LM.</p></li>
<li><p><strong>Signature</strong>: Like specifying input and output types of functions, it declares the <strong>input and output format</strong> of prompt programs. For example, if you define signature like <code class="docutils literal notranslate"><span class="pre">&quot;question</span> <span class="pre">-&gt;</span> <span class="pre">answer:</span> <span class="pre">int&quot;</span></code>, DSPy automatically generates prompts in a structure that takes <code class="docutils literal notranslate"><span class="pre">question</span></code>(str) and outputs <code class="docutils literal notranslate"><span class="pre">answer</span></code>(int). Signatures describe the structure of prompts given to models and expected output forms (e.g., JSON format, etc.).</p></li>
<li><p><strong>Module</strong>: Encapsulates <strong>prompt techniques</strong> themselves for solving problems as modules. For example, simple Q&amp;A can be expressed as <code class="docutils literal notranslate"><span class="pre">dspy.Predict</span></code>, complex thinking cases as <code class="docutils literal notranslate"><span class="pre">dspy.ChainOfThought</span></code> (chain of thought), tool-using agents as <code class="docutils literal notranslate"><span class="pre">dspy.ReAct</span></code> modules. Modules have logic implemented internally for how to compose prompts according to the corresponding techniques.</p></li>
</ul>
<p>Users combine these three to create <strong>AI programs</strong>, then can optimize by automatically improving module prompts or adding few-shot examples through <strong>Optimizer</strong> built into DSPy. For example, you can make simple combinations like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">dspy</span> <span class="c1"># Install required library (Databricks DSPy)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dspy</span>

<span class="c1"># 1) LM setup (example: using local Llama2 model API)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">LM</span><span class="p">(</span><span class="s1">&#39;ollama/llama2&#39;</span><span class="p">,</span> <span class="n">api_base</span><span class="o">=</span><span class="s1">&#39;http://localhost:11434&#39;</span><span class="p">)</span> <span class="c1"># local server example</span>
<span class="n">dspy</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span>

<span class="c1"># 2) Signature declaration: &quot;question(str) -&gt; answer(int)&quot; format</span>
<span class="n">simple_sig</span> <span class="o">=</span> <span class="s2">&quot;question -&gt; answer: int&quot;</span>

<span class="c1"># 3) Module selection: Predict (basic single-step Q&amp;A)</span>
<span class="n">simple_model</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">simple_sig</span><span class="p">)</span>

<span class="c1"># 4) Execute</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">simple_model</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">&quot;How many hours does it take from Seoul to Busan by KTX?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code creates a module called <code class="docutils literal notranslate"><span class="pre">simple_model</span></code> that defines the task of “output integer answers when receiving questions”. Internally, DSPy generates optimal prompts matching these requirements and passes them to the LM. (For example, it constructs prompts like “Q: How many hours does it take from Seoul to Busan by KTX?\nA:” and expects numeric answers.) If the initially obtained answer is inaccurate, you can apply Optimizers like <strong>BootstrapFewShot</strong> to automatically add few-shot examples, or instruct continuous answer improvement with <strong>Refine</strong> modules. In this way, DSPy enables composition and optimization of complex LLM pipelines (e.g., RAG systems, multi-stage chains, agent loops, etc.) in module units.</p>
<p>DSPy’s advantage is <strong>improved productivity in prompt engineering</strong>. Since LLM calls are designed within structured frameworks like code, it reduces the time people spend writing long prompt sentences manually and going through trial and error. Also, you can maintain the same module interface while switching various <strong>models/techniques</strong>, enabling <strong>flexible experiments</strong> like testing the same Chain-of-Thought module on both GPT-4 and Llama2 to compare performance. Thanks to the declarative approach, even <strong>changing only part of the program</strong> easily reflects in the entire LLM pipeline, making maintenance easy. Although it’s still an early-stage framework, it’s gaining attention for presenting the paradigm of <strong>“handling LLMs like programming”</strong>.</p>
</section>
<section id="haystack-document-based-search-and-reasoning">
<h3>4.3 Haystack: Document-based Search and Reasoning<a class="headerlink" href="#haystack-document-based-search-and-reasoning" title="Link to this heading">#</a></h3>
<p><strong>Haystack</strong> is an <strong>open-source NLP framework</strong> developed by Deepset in Germany, mainly used for building <strong>knowledge-based question answering</strong> systems. Haystack’s strength lies in <strong>flexible pipeline composition</strong>. Users can easily create <strong>end-to-end NLP systems</strong> that return answers when questions are input by linking a series of stages from databases (document stores) to search engines, reader (Reader) or generator (Generator) models into one Pipeline. For example, <strong>Retrieval QA</strong> like “find answers to questions from given document sets” or Wikipedia-based chatbots can be implemented with Haystack.</p>
<p>Haystack’s main components are as follows:</p>
<ul class="simple">
<li><p><strong>DocumentStore</strong>: Literally a database for storing documents. It supports backends like In-Memory, Elasticsearch, FAISS, etc., and stores document text, metadata, embeddings, etc.</p></li>
<li><p><strong>Retriever</strong>: Plays the role of <strong>searching</strong> for relevant documents regarding user questions (Query). It’s diversely implemented from traditional keyword-based methods like BM25 to <strong>Dense Passage Retrieval</strong> models like SBERT, DPR, etc. Retriever finds <strong>top k</strong> relevant documents from DocumentStore.</p></li>
<li><p><strong>Reader</strong> or <strong>Generator</strong>: Takes searched documents as input to generate final <strong>answers</strong>. <strong>Reader</strong> usually uses Extractive QA models (BERT-based, etc.) to extract correct answer spans from the documents, and <strong>Generator</strong> can generate answers using generative models like GPT. Both can be plugged in as nodes (Node) in Haystack.</p></li>
<li><p><strong>Pipeline</strong>: Structure that defines <strong>query-&gt;response flow</strong> by combining the above elements. There are simple ExtractiveQAPipeline that puts Retriever results into Reader, and GenerativeQAPipeline that creates answers generatively. You can also connect <strong>Retriever + Large LM</strong> like Retrieval-Augmented Generation, or implement multi-stage conditional flows.</p></li>
</ul>
<p>Let’s look at a <strong>simple practice example</strong> using Haystack. For example, if you want to create a QA system that answers questions using FAQ document collections:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">haystack.document_stores</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryDocumentStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">haystack.nodes</span><span class="w"> </span><span class="kn">import</span> <span class="n">BM25Retriever</span><span class="p">,</span> <span class="n">FARMReader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">haystack</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># 1) Create document store and write documents</span>
<span class="n">document_store</span> <span class="o">=</span> <span class="n">InMemoryDocumentStore</span><span class="p">()</span>
<span class="n">docs</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Drama **Squid Game** is a Korean survival drama...&quot;</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;Wikipedia&quot;</span><span class="p">}}]</span>
<span class="n">document_store</span><span class="o">.</span><span class="n">write_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="c1"># 2) Configure Retriever and Reader</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">BM25Retriever</span><span class="p">(</span><span class="n">document_store</span><span class="o">=</span><span class="n">document_store</span><span class="p">)</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">FARMReader</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="s2">&quot;monologg/koelectra-base-v3-finetuned-korquad&quot;</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 3) Build pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">component</span><span class="o">=</span><span class="n">retriever</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Retriever&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Query&quot;</span><span class="p">])</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">component</span><span class="o">=</span><span class="n">reader</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Reader&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Retriever&quot;</span><span class="p">])</span>

<span class="c1"># 4) Execute QA</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Who is the director of Squid Game?&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Retriever&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="s2">&quot;Reader&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;answers&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above code, we simply put one document in an in-memory document store and built a pipeline combining BM25-based Retriever and Electra Reader trained on Korean KorQuAD data. When you put a query in pipeline.run(), Retriever finds the top 5 documents, and Reader extracts and returns the answer from among them. As a result, you can get an answer like “Hwang Dong-hyuk”.</p>
<p>Haystack’s powerful point is that <strong>components can be easily replaced or extended</strong> like this. You can switch to Dense Retriever, or attach generative models like GPT-3 as Generator instead of Reader. It also supports complex reasoning scenarios by sequentially/parallelly configuring multiple nodes in the middle like multi-hop QA.</p>
<p>In industrial settings, there are many cases of using Haystack to configure <strong>domain document search</strong> + <strong>QA</strong> services or RAG pipelines that inject external knowledge into <strong>chatbots</strong>. In summary, Haystack is a <strong>framework that ties search engines and NLP models together</strong>, a tool that enables building powerful document-based QA systems with relatively little code.</p>
</section>
<section id="crewai-role-based-multi-agent-framework">
<h3>4.4 CrewAI: Role-based Multi-Agent Framework<a class="headerlink" href="#crewai-role-based-multi-agent-framework" title="Link to this heading">#</a></h3>
<p><strong>CrewAI</strong> is one of the recently spotlighted <strong>AI agent</strong> frameworks, a platform that organizes multiple LLM agents in <strong>team (crew)</strong> form to perform <strong>collaborative work</strong>. While existing frameworks like LangChain were centered on single agents or chains, CrewAI specializes in <strong>role-based multi-agents</strong>. For example, to solve one problem, you can divide roles like <strong>Researcher, Analyst, Writer</strong>, etc., and configure each agent to act autonomously with their own tools and goals while collaborating overall to produce final results.</p>
<p>CrewAI’s concepts can be organized by main components as follows:</p>
<ul class="simple">
<li><p><strong>Crew (Team)</strong>: Organization or environment of all agents. Crew objects contain multiple agents and oversee their <strong>collaboration process</strong>. One Crew corresponds to one agent team for achieving specific goals.</p></li>
<li><p><strong>Agent</strong>: Independent <strong>autonomous AI</strong>, each with defined <strong>role</strong>, <strong>tools</strong>, and <strong>goals</strong>. For example, a “literature researcher” agent uses web search tools to collect information, and a “report writer” agent writes final reports with writing tools and appropriate style. Agents can delegate work to other agents or request results when needed (like people collaborating in teams).</p></li>
<li><p><strong>Process</strong>: Defines <strong>interaction rules</strong> or <strong>workflows</strong> of agents within Crew. For example, you can set up flows like “Step 1: Researcher collects materials -&gt; Step 2: Analyst summarizes -&gt; Step 3: Writer organizes” as processes. In CrewAI, such processes are also extended with the concept of <strong>Flow</strong>, and agent execution can be controlled according to events or conditions.</p></li>
</ul>
<p>Using CrewAI, developers can define each agent’s role and tools, create and execute Crews to <strong>automate complex tasks</strong>. Let’s look at a simple usage example. For instance, an agent team that finds materials and writes summary reports on given topics:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">crewai</span><span class="w"> </span><span class="kn">import</span> <span class="n">Crew</span><span class="p">,</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">tool</span>

<span class="c1"># Agent definition: searcher and writer</span>
<span class="n">searcher</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Researcher&quot;</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="s2">&quot;Information Collection&quot;</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">tool</span><span class="p">(</span><span class="s2">&quot;wiki_browser&quot;</span><span class="p">)])</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Writer&quot;</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="s2">&quot;Report Writing&quot;</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">tool</span><span class="p">(</span><span class="s2">&quot;text_editor&quot;</span><span class="p">)])</span>

<span class="c1"># Create Crew and add agents</span>
<span class="n">crew</span> <span class="o">=</span> <span class="n">Crew</span><span class="p">(</span><span class="n">agents</span><span class="o">=</span><span class="p">[</span><span class="n">searcher</span><span class="p">,</span> <span class="n">writer</span><span class="p">],</span> <span class="n">goal</span><span class="o">=</span><span class="s2">&quot;Write a 1-page summary report on the given topic&quot;</span><span class="p">)</span>
<span class="n">crew</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;Investigate and summarize traditional Korean food.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The above example is conceptual code, but it describes the flow of assigning roles and tools (e.g., wiki browser, text editor functions) to Agents, registering them in Crew, and then executing. During execution, the Researcher agent first searches Wikipedia to gather information, then passes the results to the Writer agent. The Writer organizes the received information and writes a summary report to produce the final answer. All these processes occur automatically without human intervention, and the CrewAI framework manages <strong>execution of each step and message exchange between agents</strong>.</p>
<p>CrewAI’s characteristics are <strong>high flexibility and control</strong>. Rather than simply running multiple agents independently, developers can design <strong>collaboration patterns</strong> as desired. Additionally, by finely configuring prompt rules, response formats, etc. for individual agents, <strong>specialized AIs</strong> can be built within teams. In practice, it can be applied to <strong>automated customer support</strong> (e.g., one agent understanding user intent, another agent searching FAQs, another agent generating responses) or <strong>research assistants</strong> (dividing roles to organize literature).</p>
<p>CrewAI is designed to be <strong>compatible with LangChain and others</strong> rather than being a completely new framework, allowing reuse of existing tool chains. However, due to the nature of multi-agent systems, <strong>safety mechanism design</strong> to prevent unexpected interactions or infinite loops is also important. CrewAI recommends setting <strong>restrictions and policies</strong> by role so agents only act within defined boundaries.</p>
<p>In summary, CrewAI is a framework that <strong>systematizes collaboration of role-based autonomous agents</strong>, helping multiple specialized LLMs perform more complex tasks through <strong>division of labor and cooperation</strong> instead of one giant LLM doing everything. This enables approaching multi-agent AI system development in an easy and standardized way.</p>
</section>
<section id="id4">
<h3>Checkpoint Questions<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What roles do the core components of CrewAI - <strong>Crew</strong>, <strong>Agent</strong>, and <strong>Process</strong> - play?</p></li>
<li><p>In what types of AI tasks can CrewAI demonstrate advantages? Consider utilization in specialized document writing or customer support scenarios.</p></li>
<li><p>What design is needed to prevent problems that may occur when multiple agents work together (e.g., infinite loops, conflicts)?</p></li>
</ul>
</section>
<section id="langgraph-state-based-multi-agent-orchestration">
<h3>4.5 LangGraph: State-based Multi-Agent Orchestration<a class="headerlink" href="#langgraph-state-based-multi-agent-orchestration" title="Link to this heading">#</a></h3>
<p><strong>LangGraph</strong> is a <strong>low-level orchestration framework</strong> developed by the LangChain team, specialized for building <strong>multi-agent systems with persistent state</strong>. LangGraph manages agent execution as <strong>graph data structures</strong>, where each node represents an agent’s state and behavior, and edges express interaction paths. This allows explicit handling of inter-agent message flow, state changes, and <strong>recovery points (checkpoints)</strong> when errors occur, making it suitable for scenarios requiring <strong>reliability</strong> and <strong>durability</strong>.</p>
<p>The core of LangGraph usage is defining a graph object called <em>StateGraph</em> and, when necessary, linking it with <strong>checkpoint storage</strong> to continuously save/recover agent states. For example, even if one agent fails during <strong>long conversations</strong> or <strong>plan execution</strong>, you can design <strong>fault-tolerant</strong> systems that rollback to the last saved state and retry. Additionally, <strong>Human-in-the-loop</strong> intervention is easy, allowing people to review or modify at intermediate states and then continue execution.</p>
<p>Let’s examine the concept through a simple LangGraph example. The code below creates a React-style agent with one tool and processes user questions on the graph (assuming Anthropic Claude model):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">langgraph</span> <span class="c1"># Install LangGraph library</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_react_agent</span>

<span class="c1"># Define a simple tool function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># Return fixed answer instead of actual external API (example)</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;The weather in </span><span class="si">{</span><span class="n">city</span><span class="si">}</span><span class="s2"> is always sunny!&quot;</span>

<span class="c1"># Create React agent (assuming Anthropic Claude API usage)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">create_react_agent</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;anthropic:claude-2&quot;</span><span class="p">,</span>    <span class="c1"># Anthropic Claude model (API key required)</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">get_weather</span><span class="p">],</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant.&quot;</span>  <span class="c1"># Basic prompt</span>
<span class="p">)</span>

<span class="c1"># Execute agent (pass user message as graph input)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather like in Seoul?&quot;</span><span class="p">}]})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above code, a <strong>ReAct pattern</strong> agent is created through the <code class="docutils literal notranslate"><span class="pre">create_react_agent</span></code> function. The <code class="docutils literal notranslate"><span class="pre">tools</span></code> list provides the <code class="docutils literal notranslate"><span class="pre">get_weather</span></code> function, allowing the agent to use the tool when needed. When <code class="docutils literal notranslate"><span class="pre">agent.invoke(...)</span></code> is called with the user’s message as graph input, LangGraph internally constructs a <strong>state graph (StateGraph)</strong> to track the agent’s reasoning process. The <code class="docutils literal notranslate"><span class="pre">response</span></code> contains the final answer produced by the agent (e.g., “The weather in Seoul is always sunny!”).</p>
<p>Using LangGraph, such <strong>agent workflows</strong> can be expanded more complexly. You can add multiple agents as nodes, define <strong>message passing paths</strong> between nodes, and set up processes where different agents perform tasks like “question analysis -&gt; information search -&gt; answer organization” according to graph order. LangGraph’s <strong>checkpoint</strong> feature allows periodic saving of intermediate states of long-running agents, enabling resumption from the middle rather than the beginning when unexpected errors occur. Additionally, it can integrate with monitoring tools like <strong>LangSmith</strong> to visualize and debug graph execution.</p>
<p>In summary, LangGraph is a framework for ensuring <strong>reliability and persistence</strong> in multi-agent system development. It’s useful for building <strong>agent teams that must operate continuously without interruption for long periods</strong> in web services or business automation. Since it integrates with the LangChain ecosystem, existing LangChain users can easily introduce state-based approaches.</p>
</section>
<section id="id5">
<h3>Checkpoint Questions<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What is LangGraph’s core feature of <strong>state-based orchestration</strong>, and what advantages does it provide?</p></li>
<li><p>How can LangGraph’s <strong>checkpoint</strong> and <strong>Human-in-the-loop</strong> features be utilized in long processes or long-running agents?</p></li>
<li><p>What design is needed to prevent infinite conversations or conflicts between agents in LangGraph-built agent systems?</p></li>
</ul>
</section>
</section>
<section id="practice-bert-vs-mamba-model-comparison-experiment">
<h2>5. Practice: BERT vs Mamba Model Comparison Experiment<a class="headerlink" href="#practice-bert-vs-mamba-model-comparison-experiment" title="Link to this heading">#</a></h2>
<p>Due to the absence of publicly available Korean (NSMC) Mamba classification models, we configured a comparison experiment using the <strong>IMDB English dataset</strong>. We compare <strong>accuracy, inference speed, and GPU memory</strong> using the publicly available checkpoint <code class="docutils literal notranslate"><span class="pre">trinhxuankhai/mamba_text_classification</span></code> for Mamba and a public IMDB classification baseline for BERT.</p>
<section id="environment-setup">
<h3>5.1 Environment Setup<a class="headerlink" href="#environment-setup" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU recommended. Colab/CUDA environment recommended</span>
!pip<span class="w"> </span>-q<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cu121
!pip<span class="w"> </span>-q<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>datasets<span class="w"> </span>accelerate
</pre></div>
</div>
</section>
<section id="dataset-loading-imdb">
<h3>5.2 Dataset Loading (IMDB)<a class="headerlink" href="#dataset-loading-imdb" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">imdb</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">)</span>
<span class="n">imdb_test</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>  <span class="c1"># 25k samples</span>

<span class="c1"># For speed comparison, evaluating on a small sample (e.g., 1000) is acceptable</span>
<span class="n">imdb_test_small</span> <span class="o">=</span> <span class="n">imdb_test</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="model-and-tokenizer-loading">
<h3>5.3 Model and Tokenizer Loading<a class="headerlink" href="#model-and-tokenizer-loading" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Mamba</strong>: <code class="docutils literal notranslate"><span class="pre">trinhxuankhai/mamba_text_classification</span></code> (user-provided card)</p></li>
<li><p><strong>BERT</strong>: Public IMDB classification model (e.g., <code class="docutils literal notranslate"><span class="pre">textattack/bert-base-uncased-imdb</span></code>)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">dtype</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>

<span class="c1"># 1) Mamba classification model (IMDB trained)</span>
<span class="n">mamba_id</span> <span class="o">=</span> <span class="s2">&quot;trinhxuankhai/mamba_text_classification&quot;</span>
<span class="n">tok_mamba</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">mamba_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model_mamba</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">mamba_id</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_mamba</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># 2) BERT classification model (public IMDB baseline)</span>
<span class="n">bert_id</span> <span class="o">=</span> <span class="s2">&quot;textattack/bert-base-uncased-imdb&quot;</span>
<span class="n">tok_bert</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_id</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model_bert</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_id</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_bert</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loaded:&quot;</span><span class="p">,</span> <span class="n">mamba_id</span><span class="p">,</span> <span class="s2">&quot;|&quot;</span><span class="p">,</span> <span class="n">bert_id</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="evaluation-function-accuracy-speed-memory">
<h3>5.4 Evaluation Function (Accuracy, Speed, Memory)<a class="headerlink" href="#evaluation-function-accuracy-speed-memory" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1"># Preprocessing</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">enc</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
        <span class="n">encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
        <span class="p">)</span>
        <span class="n">encodings</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">encodings</span>

    <span class="c1"># Pre-tensorize</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span>

    <span class="c1"># Batch encoding (on-the-fly also possible for memory saving)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">enc</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">l</span><span class="p">]})</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">labels</span><span class="p">)]</span>

    <span class="c1"># Warmup</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">warmup</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoded</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)))],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">encoded</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Memory/time measurement</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoded</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)))],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                 <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">encoded</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
        <span class="n">preds</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="n">duration</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">duration</span>
    <span class="n">peak_mem_mb</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="n">peak_mem_mb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s2">&quot;sec&quot;</span><span class="p">:</span> <span class="n">duration</span><span class="p">,</span> <span class="s2">&quot;throughput&quot;</span><span class="p">:</span> <span class="n">throughput</span><span class="p">,</span> <span class="s2">&quot;peak_mem_mb&quot;</span><span class="p">:</span> <span class="n">peak_mem_mb</span><span class="p">}</span>

<span class="c1"># Execute evaluation (1k samples)</span>
<span class="n">res_mamba</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model_mamba</span><span class="p">,</span> <span class="n">tok_mamba</span><span class="p">,</span> <span class="n">imdb_test_small</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">res_bert</span>  <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model_bert</span><span class="p">,</span>  <span class="n">tok_bert</span><span class="p">,</span>  <span class="n">imdb_test_small</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mamba results:&quot;</span><span class="p">,</span> <span class="n">res_mamba</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BERT results:&quot;</span><span class="p">,</span> <span class="n">res_bert</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-results-and-interpretation">
<h3>5.5 Example Results and Interpretation<a class="headerlink" href="#example-results-and-interpretation" title="Link to this heading">#</a></h3>
<p><strong>Example (may vary by environment):</strong></p>
<ul class="simple">
<li><p><strong>Mamba(IMDB)</strong> → <code class="docutils literal notranslate"><span class="pre">{'accuracy':</span> <span class="pre">0.94,</span> <span class="pre">'throughput':</span> <span class="pre">X</span> <span class="pre">samples/sec,</span> <span class="pre">'peak_mem_mb':</span> <span class="pre">Y</span> <span class="pre">MB}</span></code></p></li>
<li><p><strong>BERT(IMDB)</strong> → <code class="docutils literal notranslate"><span class="pre">{'accuracy':</span> <span class="pre">0.94,</span> <span class="pre">'throughput':</span> <span class="pre">Z</span> <span class="pre">samples/sec,</span> <span class="pre">'peak_mem_mb':</span> <span class="pre">W</span> <span class="pre">MB}</span></code></p></li>
<li><p><strong>Accuracy</strong>: Based on the provided Mamba model card, Val/Test Acc ≈ 0.94, showing <strong>similar high performance</strong> to BERT baseline.</p></li>
<li><p><strong>Inference Speed (Throughput)</strong>: Under conditions of input length 256, batch 32, there are variations depending on quantization application or hardware specs. As input length increases, BERT’s speed drops sharply due to attention’s O(N²) complexity, while <strong>Mamba’s linear time O(N) advantage</strong> becomes prominent.</p></li>
<li><p><strong>Memory Usage (Peak Memory)</strong>: Mamba is theoretically more memory efficient as it doesn’t generate massive attention matrices due to state space model (SSM) characteristics. This difference becomes clearer with longer sequences.</p></li>
</ul>
<p><strong>Note</strong></p>
<ul class="simple">
<li><p>The above code compares speed/memory with <strong>1000 samples</strong> (full 25k also possible, but considering practice time).</p></li>
<li><p>For experimental fairness, maintain <strong>same <code class="docutils literal notranslate"><span class="pre">max_length</span></code>, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, <code class="docutils literal notranslate"><span class="pre">dtype</span></code></strong>.</p></li>
<li><p>Results may vary significantly depending on <strong>GPU specs</strong> like Colab T4/V100, A100, RTX 30/40 series.</p></li>
</ul>
</section>
<section id="id6">
<h3>Checkpoint Questions<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Even though <strong>accuracy was similar</strong> in this comparison, predict how <strong>inference speed/memory</strong> of the two models will differ with <strong>longer input lengths</strong>.</p></li>
<li><p>List 2 advantages and risks each of Mamba for real service deployment (e.g., library maturity, ecosystem, debugging tools).</p></li>
<li><p>Change <strong><code class="docutils literal notranslate"><span class="pre">max_length=512/1024</span></code></strong> in the same code and re-measure, then summarize Throughput/PeakMem changes in a report.</p></li>
</ol>
</section>
</section>
<section id="experiment-summary-and-implications">
<h2>6. Experiment Summary and Implications<a class="headerlink" href="#experiment-summary-and-implications" title="Link to this heading">#</a></h2>
<p>Through the <strong>BERT vs Mamba comparison experiment</strong>, we examined the characteristics and pros/cons of both models. In summary, <strong>existing BERT (Transformer)</strong> models still show high accuracy and stable speed for medium-length inputs and are <strong>still efficient in short input environments</strong>. On the other hand, <strong>Mamba (SSM)</strong> models show potential for ultra-long context processing and <strong>efficiency without performance degradation</strong> as input length increases. However, at the current point, Transformer series are validated in terms of model completeness and optimization, while Mamba is in the research stage, so <strong>Transformers have some advantage in general tasks</strong> (e.g., accuracy comparison in this experiment).</p>
<p><strong>Which model is suitable for which situation?</strong> First, <strong>input sequence length</strong> is the determining factor. For <strong>short sentence-level tasks</strong> (e.g., sentence classification, short-answer QA, etc.), using Transformers like BERT is advantageous in terms of implementation ease and performance. Rich pre-training and tuning techniques are accumulated, making it easy to achieve high accuracy with short inference latency. For <strong>long context or document-level tasks</strong> (e.g., summarizing documents of thousands of words, sentiment analysis of long texts, etc.), linear architectures like Mamba may be advantageous. This is because Mamba can efficiently process input lengths that are impossible or would consume many resources with Transformers. In fact, Mamba shows the ability to process up to <strong>1 million tokens</strong>, suggesting the possibility of opening the era of ultra-long context LLMs.</p>
<p>From an <strong>inference speed</strong> perspective, judgment should also be based on context length. With short inputs, the two models may have similar speeds or Transformers may be faster, but as input length increases, Transformers <strong>slow down dramatically</strong> as O(n²), so reports suggest that Mamba will show <strong>up to 5x faster inference</strong> in sufficiently long contexts. Additionally, Mamba has strengths in time series data and continuous stream processing due to the nature of state space models, and also has generality that can be widely applied to <strong>speech and sequence data processing beyond language</strong>.</p>
<p><strong>Service/Production Application Implications:</strong> Currently in production environments, Transformer series (e.g., BERT, GPT) models are mature and widely used in terms of performance and tooling. Mamba is a very promising technology but <strong>library support, community, and pre-trained model pools</strong> are not as rich as Transformers yet. Therefore, more stability validation may be needed to immediately introduce Mamba as a replacement in industry. However, for services that have had difficulty with ultra-long context processing due to <strong>memory capacity limits or latency issues</strong>, introducing models like Mamba in the future could provide a breakthrough. For example, in <strong>long legal document analysis services</strong> or <strong>chatbots that need to maintain long-term conversation history</strong>, Mamba architecture has the potential to be a game changer.</p>
<p>Additionally, attention should be paid to future hybrid models (e.g., <strong>Jamba: Transformer+Mamba mixed experts</strong>) and competition with other linear sequence models. Currently, it can be viewed as <strong>Transformer’s universality vs. Mamba’s specificity</strong>, and in actual production, approaches to <strong>mutually complementary utilization</strong> of both methods are also considered. For example, a system that processes general conversations with Transformers and switches to Mamba mode when ultra-long context processing is needed for specific requests would be possible.</p>
<p>In summary, <strong>BERT</strong> and <strong>Mamba</strong> each have their strengths and different use cases. <strong>Mature BERT series</strong> are suitable for <strong>short inputs/existing tasks</strong>, while <strong>Mamba</strong> shows potential for <strong>long inputs/new expansion tasks</strong>. If research and technological development continue, it is expected that cases where SSMs like Mamba complement or replace Transformer limitations will gradually increase. When applying to actual services, current model stability, support tools, licenses, etc. should be comprehensively considered, but from a <strong>future-oriented perspective, architectural innovation for ultra-long context and high-efficiency inference is being realized</strong>, and this comparison experiment of the two models provides meaningful insights.</p>
<section id="id7">
<h3>Checkpoint Questions<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What are the differences in <strong>time complexity</strong> between BERT and Mamba models, and how does this affect long sequence processing?</p></li>
<li><p>Predict how <strong>inference speed</strong> and <strong>memory usage</strong> results will change if the input sentence length is increased to 512 or 1024 tokens in this experiment.</p></li>
<li><p>Why is it difficult to immediately deploy Mamba models in current industry systems? Conversely, what utilization scenarios could make Mamba popular in the future?</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<section id="major-papers-and-research-materials">
<h3>Major Papers and Research Materials<a class="headerlink" href="#major-papers-and-research-materials" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>PyTorch Official Blog – <em>“Better Performance with torch.compile”</em> (2023)</p></li>
<li><p>Tri Dao Blog – <em>“FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-Precision”</em></p></li>
<li><p>Databricks DSPy Introduction – <em>Programming, not prompting</em></p></li>
</ul>
</section>
<section id="technical-documentation-and-implementations">
<h3>Technical Documentation and Implementations<a class="headerlink" href="#technical-documentation-and-implementations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hugging Face Transformers Documentation &amp; Tutorials</p></li>
<li><p>Deepset Haystack Documentation – <em>Flexible Open Source QA Framework</em></p></li>
<li><p>CrewAI Official Documentation – <em>Role-based Autonomous Agent Teams</em></p></li>
<li><p>LangGraph Official Documentation – <em>State-based Multi-Agent Orchestration</em></p></li>
</ul>
</section>
<section id="online-resources-and-blogs">
<h3>Online Resources and Blogs<a class="headerlink" href="#online-resources-and-blogs" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“torch.compile: A Deep Dive into PyTorch’s Compiler” - PyTorch Blog</p></li>
<li><p>“FlashAttention-3: The Next Generation of Attention Optimization” - Technical Blog</p></li>
<li><p>“AI Agent Frameworks: A Comprehensive Comparison” - Medium</p></li>
<li><p>“DSPy: The Future of Prompt Engineering” - Databricks Blog</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week01/qna.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</p>
      </div>
    </a>
    <a class="right-next"
       href="../week03/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-2-x-and-torch-compile-the-compiler-revolution">1. PyTorch 2.x and torch.compile: The Compiler Revolution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-torch-compile-works">1.1 How torch.compile Works</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-acquisition-torchdynamo">1. Graph Acquisition (TorchDynamo)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ahead-of-time-automatic-differentiation-aotautograd">2. Ahead-of-Time Automatic Differentiation (AOTAutograd)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-lowering-primtorch">3. Graph Lowering (PrimTorch)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-compilation-torchinductor">4. Graph Compilation (TorchInductor)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-improving-model-inference-speed-with-torch-compile">1.2 Practice: Improving Model Inference Speed with torch.compile</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-questions">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flashattention-3-attention-optimization-through-hardware-acceleration">2. FlashAttention-3: Attention Optimization through Hardware Acceleration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-principles-of-flashattention">2.1 Core Principles of FlashAttention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-acceleration-of-flashattention-3">2.2 Hardware Acceleration of FlashAttention-3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-enabling-flashattention-in-hugging-face-transformers">2.3 Practice: Enabling FlashAttention in Hugging Face Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Checkpoint Questions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-practice-direct-use-of-pytorch-scaled-dot-product-attention">2.4 Additional Practice: Direct Use of PyTorch scaled_dot_product_attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-transformers-ecosystem-latest-trends-and-practice">3. Hugging Face Transformers Ecosystem: Latest Trends and Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latest-trends">3.1 Latest Trends</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-korean-sentiment-analysis-using-pipeline-api">3.2 Practice: Korean Sentiment Analysis Using Pipeline API</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-agent-frameworks-the-era-of-automation-and-collaboration">4. AI Agent Frameworks: The Era of Automation and Collaboration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-major-ai-agent-frameworks">4.1 Comparison of Major AI Agent Frameworks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dspy-declarative-prompt-programming">4.2 DSPy: Declarative Prompt Programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#haystack-document-based-search-and-reasoning">4.3 Haystack: Document-based Search and Reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#crewai-role-based-multi-agent-framework">4.4 CrewAI: Role-based Multi-Agent Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Checkpoint Questions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph-state-based-multi-agent-orchestration">4.5 LangGraph: State-based Multi-Agent Orchestration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-bert-vs-mamba-model-comparison-experiment">5. Practice: BERT vs Mamba Model Comparison Experiment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">5.1 Environment Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-loading-imdb">5.2 Dataset Loading (IMDB)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-tokenizer-loading">5.3 Model and Tokenizer Loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-function-accuracy-speed-memory">5.4 Evaluation Function (Accuracy, Speed, Memory)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-results-and-interpretation">5.5 Example Results and Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-summary-and-implications">6. Experiment Summary and Implications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Checkpoint Questions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#major-papers-and-research-materials">Major Papers and Research Materials</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-documentation-and-implementations">Technical Documentation and Implementations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-resources-and-blogs">Online Resources and Blogs</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
