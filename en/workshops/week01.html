
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 1 Workshop: LLM Overview and Development Environment Setup &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'workshops/week01';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Team Project Guidelines" href="../projects/index.html" />
    <link rel="prev" title="LLM From Scratch Workshop" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Deep Learning for Natural Language Processing (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer and Next-Generation Architectures</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba Architecture Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x and Latest Deep Learning Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: Efficient Fine-Tuning with Modern PEFT Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: Advanced Prompting Techniques and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM Evaluation Paradigms and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: Advances in Multimodal NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: Ultra-Long Context Processing and Efficient Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: Core Review and Latest Trends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week09/index.html">Week 9: Advanced RAG Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week10/index.html">Week 10: Revolutionary Alignment Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week11/index.html">Week 11: Production Agent Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week12/index.html">Week 12: AI Regulation and Responsible AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="index.html">LLM From Scratch Workshop</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 1 Workshop: LLM Overview and Development Environment Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">Team Project Guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/en/workshops/week01.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fworkshops/week01.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/workshops/week01.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 1 Workshop: LLM Overview and Development Environment Setup</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-introduction-exploring-the-journey-of-large-language-models"><strong>Workshop Introduction: Exploring the Journey of Large Language Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-in-depth-analysis-of-the-complete-llm-lifecycle"><strong>Part 1: In-Depth Analysis of the Complete LLM Lifecycle</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-1-scope-definition-and-problem-formulation"><strong>Stage 1: Scope Definition and Problem Formulation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-2-data-collection-and-refinement"><strong>Stage 2: Data Collection and Refinement</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-3-pre-training"><strong>Stage 3: Pre-training</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-4-supervised-fine-tuning-sft"><strong>Stage 4: Supervised Fine-Tuning (SFT)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-5-alignment-and-safety-tuning-rlhf-dpo"><strong>Stage 5: Alignment and Safety Tuning (RLHF/DPO)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-6-evaluation-and-benchmarking"><strong>Stage 6: Evaluation and Benchmarking</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-7-deployment-and-inference-optimization"><strong>Stage 7: Deployment and Inference Optimization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-8-monitoring-and-maintenance"><strong>Stage 8: Monitoring and Maintenance</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-question-1-what-is-the-most-important-stage-in-the-llm-lifecycle"><strong>Checkpoint Question 1: What is the most important stage in the LLM lifecycle?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-development-environment-setup-nvidia-ngc-hands-on-guide"><strong>Part 2: Development Environment Setup: NVIDIA NGC Hands-on Guide</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-checklist"><strong>Prerequisites Checklist</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-installation-guide"><strong>Step-by-Step Installation Guide</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#essential-python-library-installation"><strong>Essential Python Library Installation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#troubleshooting-guide"><strong>Troubleshooting Guide</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-first-encounter-running-llms-with-hugging-face-transformers"><strong>Part 3: First Encounter: Running LLMs with Hugging Face Transformers</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-power-of-simplicity-pipeline-api"><strong>The Power of Simplicity: Pipeline API</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-1-first-text-generation"><strong>Practice 1: First Text Generation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-2-korean-text-generation"><strong>Practice 2: Korean Text Generation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-control-generation-parameter-guide"><strong>Output Control: Generation Parameter Guide</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4-the-two-frameworks-story-nemo-and-hugging-face"><strong>Part 4: The Two Frameworks Story: NeMo and Hugging Face</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#philosophical-deep-dive"><strong>Philosophical Deep Dive</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-analysis-nemo-vs-hugging-face-transformers"><strong>Comparative Analysis: NeMo vs Hugging Face Transformers</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bridging-the-gap-interoperability-and-coexistence"><strong>Bridging the Gap: Interoperability and Coexistence</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-and-week-1-team-challenge"><strong>Conclusion and Week 1 Team Challenge</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#week-1-summary"><strong>Week 1 Summary</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#week-2-preview"><strong>Week 2 Preview</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#week-1-team-challenge-recommended"><strong>Week 1 Team Challenge (Recommended)</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references"><strong>References</strong></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-1-workshop-llm-overview-and-development-environment-setup">
<h1>Week 1 Workshop: LLM Overview and Development Environment Setup<a class="headerlink" href="#week-1-workshop-llm-overview-and-development-environment-setup" title="Link to this heading">#</a></h1>
<section id="workshop-introduction-exploring-the-journey-of-large-language-models">
<h2><strong>Workshop Introduction: Exploring the Journey of Large Language Models</strong><a class="headerlink" href="#workshop-introduction-exploring-the-journey-of-large-language-models" title="Link to this heading">#</a></h2>
<p>Welcome to this LLM workshop. The objectives for Week 1 are as follows:</p>
<ol class="arabic simple">
<li><p>Establish a comprehensive understanding of the complete lifecycle of Large Language Model (LLM) projects, from ideation to deployment and maintenance.</p></li>
<li><p>Build a powerful GPU-accelerated development environment using industry-standard tools.</p></li>
<li><p>Successfully perform the first text generation task using pre-trained LLMs.</p></li>
</ol>
<p>This workshop focuses on hands-on practice with two core tools. The first is <strong>Hugging Face Transformers</strong>. This library serves as the de facto standard for accessing vast pre-trained models and datasets, acting as the “lingua franca” of the NLP community. The second is</p>
<p><strong>NVIDIA NeMo Framework</strong>. NeMo is an enterprise-grade cloud-native framework designed for building, customizing, and deploying large-scale generative AI models. The core value of this framework lies in its performance and scalability, deeply optimized for the NVIDIA hardware ecosystem.</p>
<p>This workshop combines Hugging Face’s extensive model and data ecosystem with NeMo’s powerful and scalable training and customization capabilities, presenting an approach that leverages the strengths of both worlds in modern LLM development.</p>
</section>
<hr class="docutils" />
<section id="part-1-in-depth-analysis-of-the-complete-llm-lifecycle">
<h2><strong>Part 1: In-Depth Analysis of the Complete LLM Lifecycle</strong><a class="headerlink" href="#part-1-in-depth-analysis-of-the-complete-llm-lifecycle" title="Link to this heading">#</a></h2>
<p>LLM projects encompass more than simple model training, involving a complex process from clear goal setting to continuous maintenance. This process goes through multiple stages, with each stage having a significant impact on subsequent stages.</p>
<section id="stage-1-scope-definition-and-problem-formulation">
<h3><strong>Stage 1: Scope Definition and Problem Formulation</strong><a class="headerlink" href="#stage-1-scope-definition-and-problem-formulation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Transform vague ideas into well-defined projects with clear business objectives and technical constraints.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>Problem Understanding:</strong> Clearly articulate the problem to be solved. It should be specific, such as “reducing customer support workload” or “automated marketing copy generation.”</p></li>
<li><p><strong>Feasibility Analysis:</strong> Review available technology, data, and resources, and evaluate whether LLMs are suitable tools for solving the problem.</p></li>
<li><p><strong>Success Metrics Definition:</strong> Establish quantifiable key performance indicators (KPIs). Examples include “30% reduction in support workload,” “maintaining customer satisfaction above 4.5/5,” and “response latency under 2 seconds.”</p></li>
<li><p><strong>Constraint Identification:</strong> List all operational, legal, and ethical boundaries, including data privacy protection (GDPR, etc.), latency requirements, and conditions requiring human agent handoff.</p></li>
</ul>
</li>
</ul>
</section>
<section id="stage-2-data-collection-and-refinement">
<h3><strong>Stage 2: Data Collection and Refinement</strong><a class="headerlink" href="#stage-2-data-collection-and-refinement" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Build high-quality, diverse, and representative datasets that will serve as the foundation for the model’s knowledge and behavior.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>Data Sourcing:</strong> Collect vast amounts of text from various sources such as books, websites, articles, and code.</p></li>
<li><p><strong>Preprocessing and Refinement:</strong> Perform crucial steps including noise removal, normalization, deduplication, and filtering of low-quality or harmful content.</p></li>
<li><p><strong>Tokenization:</strong> Convert raw text into numerical format (tokens) that models can process.</p></li>
<li><p><strong>Bias and Privacy Mitigation:</strong> Actively work to mitigate biases present in source data and address privacy issues such as personally identifiable information (PII) removal.</p></li>
</ul>
</li>
</ul>
</section>
<section id="stage-3-pre-training">
<h3><strong>Stage 3: Pre-training</strong><a class="headerlink" href="#stage-3-pre-training" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Train a general-purpose “Foundation Model” based on large-scale unlabeled corpora. This model learns broad understanding of grammar, facts, reasoning abilities, and language.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>Unsupervised Learning:</strong> Typically uses next-token prediction objectives. The model learns by predicting the next word in sentences.</p></li>
<li><p><strong>Large-scale Computing:</strong> This is the most expensive stage financially and computationally, requiring millions of GPU hours.</p></li>
</ul>
</li>
<li><p><strong>Challenges:</strong> Massive costs, overfitting risks, and the need for careful regularization techniques.</p></li>
</ul>
</section>
<section id="stage-4-supervised-fine-tuning-sft">
<h3><strong>Stage 4: Supervised Fine-Tuning (SFT)</strong><a class="headerlink" href="#stage-4-supervised-fine-tuning-sft" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Adapt general-purpose foundation models to specific tasks or domains using smaller, labeled datasets.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>High-quality Dataset Generation:</strong> Curate or directly generate examples of desired input-output pairs (e.g., instruction-response pairs).</p></li>
<li><p><strong>Task-specific Training:</strong> Further train the model with labeled data to improve performance on specific tasks such as summarization, question answering, and instruction following.</p></li>
</ul>
</li>
<li><p><strong>Challenges:</strong> Catastrophic forgetting phenomenon where models lose general capabilities while specializing in specific tasks, and difficulties in securing substantial amounts of high-quality labeled data.</p></li>
</ul>
</section>
<section id="stage-5-alignment-and-safety-tuning-rlhf-dpo">
<h3><strong>Stage 5: Alignment and Safety Tuning (RLHF/DPO)</strong><a class="headerlink" href="#stage-5-alignment-and-safety-tuning-rlhf-dpo" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Align model behavior with human values, preferences, and safety guidelines to make them more useful, harmless, and honest.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>Reinforcement Learning from Human Feedback (RLHF):</strong> A three-stage process involving (1) human preference data collection (ranking various model outputs), (2) training a “reward model” that predicts human preferences, and (3) fine-tuning LLMs using reinforcement learning (PPO, etc.) to maximize reward scores.</p></li>
<li><p><strong>Direct Preference Optimization (DPO):</strong> A newer and more direct method that directly fine-tunes LLMs using the same preference data without training a separate reward model.</p></li>
</ul>
</li>
<li><p><strong>Challenges:</strong> Reward hacking where models find loopholes to receive high rewards without being truly useful, and the high cost and complexity of scaling human feedback.</p></li>
</ul>
</section>
<section id="stage-6-evaluation-and-benchmarking">
<h3><strong>Stage 6: Evaluation and Benchmarking</strong><a class="headerlink" href="#stage-6-evaluation-and-benchmarking" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Rigorously test model performance through extensive academic benchmarks and custom task-specific test sets.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>Automated Metrics:</strong> Use indicators such as ROUGE scores for summarization tasks or accuracy for classification tasks.</p></li>
<li><p><strong>Human Evaluation:</strong> Assess qualities such as usefulness, consistency, and safety that are difficult to capture with automated metrics.</p></li>
<li><p><strong>Red Teaming:</strong> Actively seek out and attempt to exploit model vulnerabilities, biases, or safety failures.</p></li>
</ul>
</li>
</ul>
</section>
<section id="stage-7-deployment-and-inference-optimization">
<h3><strong>Stage 7: Deployment and Inference Optimization</strong><a class="headerlink" href="#stage-7-deployment-and-inference-optimization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Enable trained and aligned models to be used in real applications in a reliable, scalable, and cost-effective manner.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>Model Optimization:</strong> Apply techniques such as quantization (reducing weight precision), pruning (removing unnecessary weights), and distillation (training smaller models to mimic larger models) to reduce model size and improve latency.</p></li>
<li><p><strong>Serving Infrastructure:</strong> Deploy models to cloud infrastructure, on-premises servers, or edge devices using inference servers such as Triton or vLLM.</p></li>
</ul>
</li>
</ul>
</section>
<section id="stage-8-monitoring-and-maintenance">
<h3><strong>Stage 8: Monitoring and Maintenance</strong><a class="headerlink" href="#stage-8-monitoring-and-maintenance" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Ensure model performance remains high over time and continuously improve based on real usage data.</p></li>
<li><p><strong>Key Activities:</strong></p>
<ul>
<li><p><strong>Performance Monitoring:</strong> Track metrics such as latency, error rates, and user satisfaction.</p></li>
<li><p><strong>Drift Detection:</strong> Identify when model performance degrades due to changes in input data patterns (“data drift”).</p></li>
<li><p><strong>Continuous Improvement Loop (Data Flywheel):</strong> Generate new training data for additional fine-tuning and model updates using actual inference data and user feedback.</p></li>
</ul>
</li>
</ul>
</section>
<section id="checkpoint-question-1-what-is-the-most-important-stage-in-the-llm-lifecycle">
<h3><strong>Checkpoint Question 1: What is the most important stage in the LLM lifecycle?</strong><a class="headerlink" href="#checkpoint-question-1-what-is-the-most-important-stage-in-the-llm-lifecycle" title="Link to this heading">#</a></h3>
<p>While all stages are interdependent, <strong>scope definition (Stage 1) and data refinement (Stage 2)</strong> can be considered the most important. Poorly defined problems lead to developing wrong solutions, wasting all subsequent efforts. Similarly, low-quality, biased, or irrelevant data fundamentally limits model potential regardless of how much computing resources are invested in pre-training or how sophisticated alignment techniques are. The principle of “garbage in, garbage out” applies thoroughly. Errors in these early stages are very difficult to fix later. Models pre-trained on flawed data require much more effort to align and may never reach desired performance or safety levels.</p>
<p>While it’s easy to understand these stages as a linear process, actual LLM development is much more dynamic and iterative. For example, when performance degradation is detected in the monitoring stage (Stage 8), it doesn’t simply end with redeploying the model. This feedback becomes an opportunity to build new fine-tuning datasets (Stage 4) or even return to previous stages to address fundamental data quality issues (Stage 2). Therefore, the LLM lifecycle should be understood not as a straight line but as a cyclical system like a “Data Flywheel,” where each stage provides feedback to others and continuously drives improvement.</p>
</section>
</section>
<hr class="docutils" />
<section id="part-2-development-environment-setup-nvidia-ngc-hands-on-guide">
<h2><strong>Part 2: Development Environment Setup: NVIDIA NGC Hands-on Guide</strong><a class="headerlink" href="#part-2-development-environment-setup-nvidia-ngc-hands-on-guide" title="Link to this heading">#</a></h2>
<p>This section provides step-by-step guidance on building a development environment for the workshop.</p>
<section id="prerequisites-checklist">
<h3><strong>Prerequisites Checklist</strong><a class="headerlink" href="#prerequisites-checklist" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Hardware:</strong> NVIDIA GPU (Pascal architecture or higher recommended for WSL2).</p></li>
<li><p><strong>Software:</strong></p>
<ul>
<li><p>Latest NVIDIA drivers for your operating system</p></li>
<li><p>Docker Engine installation and execution</p></li>
<li><p>Windows users: Windows 10 (21H2 or higher) or Windows 11, WSL2 activation and Linux distribution installation (e.g., Ubuntu).</p></li>
</ul>
</li>
</ul>
</section>
<section id="step-by-step-installation-guide">
<h3><strong>Step-by-Step Installation Guide</strong><a class="headerlink" href="#step-by-step-installation-guide" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>NGC Access:</strong></p>
<ul class="simple">
<li><p>Go to the NGC website (<a class="reference external" href="http://ngc.nvidia.com">ngc.nvidia.com</a>) and log in or create a free account.</p></li>
</ul>
</li>
<li><p><strong>API Key Generation:</strong></p>
<ul class="simple">
<li><p>Navigate to Setup &gt; API Keys and click Generate Personal Key. This key serves as a password for programmatic access to NGC services.</p></li>
<li><p><strong>Important:</strong> Copy the generated key immediately and store it securely. NGC does not store keys.</p></li>
</ul>
</li>
<li><p><strong>Docker and NGC Registry Authentication:</strong></p>
<ul class="simple">
<li><p>Open a terminal (PowerShell for Windows).</p></li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">login</span> <span class="pre">nvcr.io</span></code> command.</p></li>
<li><p>Enter <code class="docutils literal notranslate"><span class="pre">$oauthtoken</span></code> as the username.</p></li>
<li><p>Paste the NGC API key you just generated as the password.</p></li>
</ul>
</li>
<li><p><strong>Workshop Container Download:</strong></p>
<ul class="simple">
<li><p>Use a specific version of the NVIDIA PyTorch container to ensure reproducibility.</p></li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">pull</span> <span class="pre">nvcr.io/nvidia/pytorch:23.10-py3</span></code> command. This container includes all essential libraries optimized for NVIDIA GPUs, including PyTorch, CUDA, and cuDNN.</p></li>
</ul>
</li>
<li><p><strong>Interactive Container Execution:</strong></p>
<ul>
<li><p>Run the following command. The meaning of each flag is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>-it<span class="w"> </span>--rm<span class="w"> </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span>nvcr.io/nvidia/pytorch:23.10-py3
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gpus</span> <span class="pre">all</span></code>: Exposes all available host GPUs to the container, forming the core of the GPU acceleration environment.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-it</span></code>: Runs the container in interactive mode, allowing access to the container’s internal shell.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--rm</span></code>: Automatically deletes the container when terminated to keep the system clean.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-v</span> <span class="pre">$(pwd):/workspace</span></code>: Mounts the current directory of the host machine to the <code class="docutils literal notranslate"><span class="pre">/workspace</span></code> directory inside the container. This is essential for saving work and accessing local data.</p></li>
</ul>
</li>
<li><p><strong>GPU Access Verification:</strong></p>
<ul class="simple">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> inside the container shell.</p></li>
<li><p>If a table containing GPU, driver version, and CUDA version is output, the container is properly accessing the hardware.</p></li>
</ul>
</li>
</ol>
</section>
<section id="essential-python-library-installation">
<h3><strong>Essential Python Library Installation</strong><a class="headerlink" href="#essential-python-library-installation" title="Link to this heading">#</a></h3>
<p>Run the following pip commands inside the running container to install the NeMo and Hugging Face ecosystems:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Hugging Face libraries</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>datasets<span class="w"> </span>accelerate

<span class="c1"># Install full NVIDIA NeMo toolkit</span>
pip<span class="w"> </span>install<span class="w"> </span>nemo-toolkit<span class="o">[</span>all<span class="o">]</span>
</pre></div>
</div>
</section>
<section id="troubleshooting-guide">
<h3><strong>Troubleshooting Guide</strong><a class="headerlink" href="#troubleshooting-guide" title="Link to this heading">#</a></h3>
<p>It’s important to resolve the most common installation issues in advance for smooth workshop progress.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Error Message</p></th>
<th class="head text-left"><p>Common Causes</p></th>
<th class="head text-left"><p>Solutions</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>docker: Error response from daemon: OCI runtime create failed… nvidia-container-cli: initialization error…</p></td>
<td class="text-left"><p>Host NVIDIA driver issues; NVIDIA Container Toolkit not configured; Docker daemon restart needed.</p></td>
<td class="text-left"><p>1. Check host driver: Verify that the latest NVIDIA driver is properly installed on the host machine. nvidia-smi should work on the host. 2. Configure toolkit: Run <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">nvidia-ctk</span> <span class="pre">runtime</span> <span class="pre">configure</span> <span class="pre">--runtime=docker</span></code> followed by <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">systemctl</span> <span class="pre">restart</span> <span class="pre">docker</span></code>. 3. WSL2 specifics: Ensure Linux GPU drivers are not installed inside WSL2. Windows drivers automatically interface.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Failed to initialize NVML: Driver/library version mismatch</p></td>
<td class="text-left"><p>Mismatch between NVIDIA driver version loaded in kernel and user-space library (<a class="reference external" href="http://libnvidia-ml.so">libnvidia-ml.so</a>) version. Often occurs when not rebooting after driver updates.</p></td>
<td class="text-left"><p>1. Easiest solution: Reboot. System reboot is the most reliable way to synchronize kernel modules and user-space libraries. 2. Manual reload (advanced): Unload all NVIDIA kernel modules (sudo rmmod…) and reload them. Complex and risky, so rebooting is recommended. 3. Clean reinstall: Completely remove all NVIDIA drivers and install the latest version fresh.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>WSL2 nvidia-smi execution error: couldn’t communicate with the NVIDIA driver</p></td>
<td class="text-left"><p>(Legacy issue) Early WSL2 versions didn’t fully support NVML.</p></td>
<td class="text-left"><p>1. Update WSL kernel: Run <code class="docutils literal notranslate"><span class="pre">wsl</span> <span class="pre">--update</span></code> in PowerShell to get the latest kernel. 2. Windows update: Ensure you’re using the latest build of Windows 10/11. 3. Update NVIDIA drivers: Install the latest drivers designed for WSL2 support.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Using pre-configured NGC containers goes beyond simple convenience; it’s a fundamental practice that ensures scientific reproducibility and mitigates dependency issues. Attempts to manually install drivers, toolkits, and libraries in different environments for each developer easily lead to failures and time waste. NGC containers encapsulate the perfect combination of NVIDIA-optimized and tested drivers, CUDA libraries, cuDNN, and PyTorch. By having all participants start with the same container (<a class="reference external" href="http://nvcr.io/nvidia/pytorch:23.10-py3">nvcr.io/nvidia/pytorch:23.10-py3</a>), we fundamentally block “it works on my machine” type problems. This containerized approach transforms complex and error-prone environment setup tasks into a single docker run command, providing all participants with a consistent baseline. This is both a cornerstone of modern MLOps and an important lesson in itself.</p>
</section>
</section>
<hr class="docutils" />
<section id="part-3-first-encounter-running-llms-with-hugging-face-transformers">
<h2><strong>Part 3: First Encounter: Running LLMs with Hugging Face Transformers</strong><a class="headerlink" href="#part-3-first-encounter-running-llms-with-hugging-face-transformers" title="Link to this heading">#</a></h2>
<p>This section conducts the first hands-on coding practice that demonstrates the power of the built environment.</p>
<section id="the-power-of-simplicity-pipeline-api">
<h3><strong>The Power of Simplicity: Pipeline API</strong><a class="headerlink" href="#the-power-of-simplicity-pipeline-api" title="Link to this heading">#</a></h3>
<p>Hugging Face pipeline is the highest-level, easy-to-use API for inference. This API abstracts and provides three core stages: (1) tokenization (preprocessing), (2) model inference (forward pass), and (3) output decoding (postprocessing).</p>
</section>
<section id="practice-1-first-text-generation">
<h3><strong>Practice 1: First Text Generation</strong><a class="headerlink" href="#practice-1-first-text-generation" title="Link to this heading">#</a></h3>
<p>Here is the provided Python script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Simple text generation pipeline</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
                     <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>
                     <span class="n">device</span><span class="o">=</span><span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Text generation test</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;The future of Artificial Intelligence is&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Code Analysis:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">pipeline(&quot;text-generation&quot;,</span> <span class="pre">model=&quot;gpt2&quot;,...)</span></code>: Instantiates the pipeline.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">task=&quot;text-generation&quot;</span></code>: Specifies the desired task. The complete list of available tasks can be found in the documentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model=&quot;gpt2&quot;</span></code>: Specifies the model to use from Hugging Face Hub. gpt2 is a good starting point due to its small size and familiarity.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device=0</span></code>: This is the core of GPU acceleration. It instructs the pipeline to load the model and data onto the first GPU (CUDA device 0).</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="practice-2-korean-text-generation">
<h3><strong>Practice 2: Korean Text Generation</strong><a class="headerlink" href="#practice-2-korean-text-generation" title="Link to this heading">#</a></h3>
<p>Let’s apply what we’ve learned to a Korean model. EleutherAI/polyglot-ko-1.3b is a good choice as a well-documented open-source (Apache 2.0 license) Korean LLM.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Korean text generation pipeline</span>
<span class="n">korean_generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
                           <span class="n">model</span><span class="o">=</span><span class="s2">&quot;EleutherAI/polyglot-ko-1.3b&quot;</span><span class="p">,</span>
                           <span class="n">device</span><span class="o">=</span><span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;대한민국 인공지능의 미래는&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">korean_generator</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="output-control-generation-parameter-guide">
<h3><strong>Output Control: Generation Parameter Guide</strong><a class="headerlink" href="#output-control-generation-parameter-guide" title="Link to this heading">#</a></h3>
<p>Basic generation results may be repetitive or meaningless. Using parameters that control decoding strategies, we can go beyond just “making it work” to “making it work well.”</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Parameter</p></th>
<th class="head text-left"><p>Description</p></th>
<th class="head text-left"><p>Impact on Output</p></th>
<th class="head text-left"><p>Recommended Use Cases</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>max_length / max_new_tokens</p></td>
<td class="text-left"><p>Maximum total length of output sequence or maximum number of new tokens to generate.</p></td>
<td class="text-left"><p>Controls response length and prevents infinite loops. Set to appropriate values for application requirements.</p></td>
<td class="text-left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>temperature</p></td>
<td class="text-left"><p>Real number greater than 0. Controls next token probability. Low values (&lt;1.0) make the model more deterministic, high values (&gt;1.0) make it more random.</p></td>
<td class="text-left"><p>Low: Predictable, conservative, repetitive. High: Creative, diverse, increased risk of meaningless results.</p></td>
<td class="text-left"><p><strong>Creative tasks:</strong> 0.7 - 1.0. <strong>Fact-based tasks:</strong> 0.2 - 0.5.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>top_k</p></td>
<td class="text-left"><p>Integer. Filters vocabulary to the k most likely next tokens. The model samples from this reduced set.</p></td>
<td class="text-left"><p>Limits the selection pool to prevent very low probability tokens from being selected. Setting too low can hinder creativity.</p></td>
<td class="text-left"><p>Good starting point is 50. Use when you want to avoid strange word choices.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>top_p (Nucleus Sampling)</p></td>
<td class="text-left"><p>Real number between 0 and 1. Filters vocabulary to the smallest token set whose cumulative probability exceeds p.</p></td>
<td class="text-left"><p>More dynamic than top_k. Adjusts vocabulary size based on next token predictability. Better general-purpose sampling method.</p></td>
<td class="text-left"><p>Good starting point is 0.9 to 0.95. OpenAI recommends changing either temperature or top_p, not both.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>num_return_sequences</p></td>
<td class="text-left"><p>Integer. Number of sequences to generate independently.</p></td>
<td class="text-left"><p>Can generate multiple different completion sentences from the same prompt.</p></td>
<td class="text-left"><p>Useful for brainstorming or providing multiple options to users.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>no_repeat_ngram_size</p></td>
<td class="text-left"><p>Integer. Prevents n-grams of this size from appearing more than once.</p></td>
<td class="text-left"><p>Directly addresses repetitive phrase problems, a common failure mode of greedy/beam search.</p></td>
<td class="text-left"><p>Set to 2 or 3 to improve fluency and reduce obvious repetition.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Various decoding strategies represent approaches to managing the fundamental tension between generating creative and diverse text versus generating consistent and predictable text, rather than simply adjusting arbitrary values. The core of LLMs is the probability distribution of vocabulary for the next token. Pure greedy approaches that select only the most likely token are deterministic but often generate monotonous and unnatural text. To introduce creativity, we must sample from the distribution, but unconstrained sampling produces meaningless results. Therefore,</p>
<p>parameters like top_k, top_p, and temperature are tools for exploring the spectrum between pure exploitation (greedy search) and pure exploration (unconstrained sampling). Finding the right balance for a given task is the core skill of inference configuration.</p>
</section>
</section>
<hr class="docutils" />
<section id="part-4-the-two-frameworks-story-nemo-and-hugging-face">
<h2><strong>Part 4: The Two Frameworks Story: NeMo and Hugging Face</strong><a class="headerlink" href="#part-4-the-two-frameworks-story-nemo-and-hugging-face" title="Link to this heading">#</a></h2>
<p>This section compares the two core frameworks of the workshop and provides an in-depth answer to the second checkpoint question.</p>
<section id="philosophical-deep-dive">
<h3><strong>Philosophical Deep Dive</strong><a class="headerlink" href="#philosophical-deep-dive" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Hugging Face:</strong></p>
<ul>
<li><p><strong>Core Philosophy:</strong> Democratization and collaboration. The goal is to make cutting-edge ML accessible to everyone.</p></li>
<li><p><strong>Implementation:</strong> Appears as a massive community-based Model Hub with hundreds of thousands of models and datasets, easy-to-use high-level APIs (pipeline), and open-source libraries that have become industry standards (transformers, datasets, accelerate).</p></li>
</ul>
</li>
<li><p><strong>NVIDIA NeMo:</strong></p>
<ul>
<li><p><strong>Core Philosophy:</strong> Performance, scalability, and enterprise readiness. The goal is to provide an end-to-end platform for efficiently building, customizing, and deploying generative AI models on NVIDIA hardware, from single GPU experiments to large-scale multi-node training clusters.</p></li>
<li><p><strong>Implementation:</strong> Appears as deep integration with NVIDIA hardware and software stacks (CUDA, Transformer Engine, Megatron-Core), support for advanced parallel processing techniques (tensor, pipeline, data), and focus on the entire MLOps lifecycle including data curation (NeMo Curator) and optimized deployment.</p></li>
</ul>
</li>
</ul>
</section>
<section id="comparative-analysis-nemo-vs-hugging-face-transformers">
<h3><strong>Comparative Analysis: NeMo vs Hugging Face Transformers</strong><a class="headerlink" href="#comparative-analysis-nemo-vs-hugging-face-transformers" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature</p></th>
<th class="head text-left"><p>Hugging Face Transformers</p></th>
<th class="head text-left"><p>NVIDIA NeMo Framework</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Primary Goal</strong></p></td>
<td class="text-left"><p>Accessibility, community, ease of use</p></td>
<td class="text-left"><p>Performance, scalability, enterprise-grade training</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Architecture</strong></p></td>
<td class="text-left"><p>High-level API (pipeline) and modular Auto* classes based on PyTorch/TensorFlow/JAX. Fundamentally framework-agnostic.</p></td>
<td class="text-left"><p>Tightly integrated with NVIDIA’s own high-performance backends like PyTorch Lightning and Megatron-Core, Transformer Engine.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Core Strengths</strong></p></td>
<td class="text-left"><p><strong>Ecosystem:</strong> Unparalleled collection of models and datasets in the Hub. Very easy to start with inference and basic fine-tuning.</p></td>
<td class="text-left"><p><strong>Large-scale Performance:</strong> Optimized for large-scale model training on thousands of GPUs. Provides excellent throughput and efficiency on NVIDIA hardware.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Ease of Use</strong></p></td>
<td class="text-left"><p><strong>Very High.</strong> Pipeline API is beginner-friendly. Extensive documentation and community support.</p></td>
<td class="text-left"><p><strong>Intermediate to Advanced.</strong> Steeper learning curve with emphasis on distributed training configuration (improved with NeMo 2.0’s Python-based configuration).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Configuration</strong></p></td>
<td class="text-left"><p>Primarily programmatic through Python code. Uses TrainingArguments class for Trainer API.</p></td>
<td class="text-left"><p>Historically YAML-based. NeMo 2.0 transitions to more flexible and powerful Python-based configuration system (Fiddle).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Use Cases</strong></p></td>
<td class="text-left"><p>Rapid prototyping, inference applications, academic research, small to medium-scale model fine-tuning.</p></td>
<td class="text-left"><p>Foundation model pre-training from scratch, large-scale SFT/RLHF on proprietary data, enterprise R&amp;D.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="bridging-the-gap-interoperability-and-coexistence">
<h3><strong>Bridging the Gap: Interoperability and Coexistence</strong><a class="headerlink" href="#bridging-the-gap-interoperability-and-coexistence" title="Link to this heading">#</a></h3>
<p>Developers are not forced to choose “either this or that.” The two frameworks are increasingly working in a complementary manner.</p>
<ul class="simple">
<li><p><strong>NeMo’s AutoModel Feature:</strong> This is a game-changing feature. It’s a high-level interface within NeMo designed to directly load and fine-tune Hugging Face models without manual conversion steps. This enables “Day-0 support” for new models by combining Hugging Face Hub’s vast ecosystem with NeMo’s high-performance training environment.</p></li>
<li><p><strong>Checkpoint Conversion Scripts:</strong> When not using AutoModel, NeMo provides explicit scripts like convert_llama_hf_to_nemo.py to convert model weights between Hugging Face and NeMo’s .nemo format.</p></li>
<li><p><strong>Data Integration through HFDatasetDataModule:</strong> NeMo provides a dedicated data module (HFDatasetDataModule) that can directly wrap and use datasets from the Hugging Face datasets library within NeMo training pipelines, simplifying the data loading process.</p></li>
</ul>
<p>The increasing interoperability between NeMo and Hugging Face shows that the AI development ecosystem is maturing. This means that rather than competing as closed single platforms, specialized tools are being designed to work together. Hugging Face serves as a universal “marketplace” for sharing and discovering models and data, while NeMo serves as a specialized “high-performance engine.” This modular, interoperable approach allows developers to use the most suitable tools for each stage of the LLM lifecycle, accelerating innovation for everyone.</p>
</section>
</section>
<hr class="docutils" />
<section id="conclusion-and-week-1-team-challenge">
<h2><strong>Conclusion and Week 1 Team Challenge</strong><a class="headerlink" href="#conclusion-and-week-1-team-challenge" title="Link to this heading">#</a></h2>
<section id="week-1-summary">
<h3><strong>Week 1 Summary</strong><a class="headerlink" href="#week-1-summary" title="Link to this heading">#</a></h3>
<p>This week, we built a solid conceptual model of the LLM lifecycle, set up a fully functional GPU-based development environment, and gained hands-on experience directly running pre-trained models with configurable output.</p>
</section>
<section id="week-2-preview">
<h3><strong>Week 2 Preview</strong><a class="headerlink" href="#week-2-preview" title="Link to this heading">#</a></h3>
<p>Next week, we will cover in-depth analysis of data preparation using datasets and NeMo Curator, and performing supervised fine-tuning (SFT) on custom datasets.</p>
</section>
<section id="week-1-team-challenge-recommended">
<h3><strong>Week 1 Team Challenge (Recommended)</strong><a class="headerlink" href="#week-1-team-challenge-recommended" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong> Apply the techniques learned this week to a real team-based exploration task.</p></li>
<li><p><strong>Tasks:</strong></p>
<ol class="arabic simple">
<li><p><strong>Exploration:</strong> Explore Hugging Face Hub together with team members.</p></li>
<li><p><strong>Task Selection:</strong> Choose a task of interest (e.g., summarization, sentiment analysis).</p></li>
<li><p><strong>Find Korean Datasets:</strong> Find Korean datasets suitable for the selected task.</p>
<ul>
<li><p><em>Sentiment Analysis Recommendation:</em> e9t/nsmc (Naver Movie Review Corpus).</p></li>
<li><p><em>Summarization Recommendation:</em> Explore models like nglaura/koreascience-summarization or gogamza/kobart-summarization.</p></li>
</ul>
</li>
<li><p><strong>Find Korean Models:</strong> Find pre-trained models suitable for the task. This could be general models like EleutherAI/polyglot-ko-1.3b or task-specific fine-tuned models.</p></li>
<li><p><strong>Experiment:</strong> Load models using the pipeline API in the NGC container environment and run inference on 3-5 examples from the selected dataset.</p></li>
<li><p><strong>Discussion:</strong> Analyze results with team members. Are the results good? How can they be improved? This discussion will be a perfect entry point for next week’s fine-tuning topic.</p></li>
</ol>
</li>
</ul>
</section>
</section>
<section id="references">
<h2><strong>References</strong><a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>What are the features of Hugging Face’s Transformers? - Milvus, 2025, <a class="reference external" href="https://milvus.io/ai-quick-reference/what-are-the-features-of-hugging-faces-transformers">https://milvus.io/ai-quick-reference/what-are-the-features-of-hugging-faces-transformers</a></p></li>
<li><p>Hugging Face Transformers: Leverage Open-Source AI in Python, 2025, <a class="reference external" href="https://realpython.com/huggingface-transformers/">https://realpython.com/huggingface-transformers/</a></p></li>
<li><p>Introduction to Hugging Face Transformers - GeeksforGeeks, 2025, <a class="reference external" href="https://www.geeksforgeeks.org/artificial-intelligence/Introduction-to-hugging-face-transformers/">https://www.geeksforgeeks.org/artificial-intelligence/Introduction-to-hugging-face-transformers/</a></p></li>
<li><p>NVIDIA NeMo Framework, 2025, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/index.html">https://docs.nvidia.com/nemo-framework/index.html</a></p></li>
<li><p>What Is The Difference Between NVIDIA NeMo Framework &amp; NeMo Microservices?, 2025, <a class="reference external" href="https://cobusgreyling.medium.com/what-is-the-difference-between-nvidia-nemo-framework-nemo-microservices-9339dbb62226">https://cobusgreyling.medium.com/what-is-the-difference-between-nvidia-nemo-framework-nemo-microservices-9339dbb62226</a></p></li>
<li><p>LLM Development: Step-by-Step Phases of LLM Training - lakeFS, 2025, <a class="reference external" href="https://lakefs.io/blog/llm-development/">https://lakefs.io/blog/llm-development/</a></p></li>
<li><p>LLM Project Lifecycle: Revolutionized by Generative AI - Data Science Dojo, 2025, <a class="reference external" href="https://datasciencedojo.com/blog/llm-project-lifecycle/">https://datasciencedojo.com/blog/llm-project-lifecycle/</a></p></li>
<li><p>Large Language Model Lifecycle: An Examination Challenges, 2025, <a class="reference external" href="https://www.computer.org/publications/tech-news/trends/large-language-model-lifecycle/">https://www.computer.org/publications/tech-news/trends/large-language-model-lifecycle/</a></p></li>
<li><p>What are the Stages of the LLMOps Lifecycle? - <a class="reference external" href="http://Klu.ai">Klu.ai</a>, 2025, <a class="reference external" href="https://klu.ai/glossary/llm-ops-lifecycle">https://klu.ai/glossary/llm-ops-lifecycle</a></p></li>
<li><p>The LLM Project Lifecycle: A Practical Guide | by Tony Siciliani - Medium, 2025, <a class="reference external" href="https://medium.com/&#64;tsiciliani/the-llm-project-lifecycle-a-practical-guide-9117228664d4">https://medium.com/&#64;tsiciliani/the-llm-project-lifecycle-a-practical-guide-9117228664d4</a></p></li>
<li><p>LLM Development Life Cycle, 2025, <a class="reference external" href="https://muoro.io/llm-development-life-cycle">https://muoro.io/llm-development-life-cycle</a></p></li>
<li><p>Analysis of the Hugging Face Transformers Library: Purpose and Component Classes : 1, 2025, <a class="reference external" href="https://medium.com/&#64;danushidk507/analysis-of-the-hugging-face-transformers-library-purpose-and-component-classes-1-8f5bdc7a3b17">https://medium.com/&#64;danushidk507/analysis-of-the-hugging-face-transformers-library-purpose-and-component-classes-1-8f5bdc7a3b17</a></p></li>
<li><p>LLM Post-Training: A Deep Dive into Reasoning Large Language Models - arXiv, 2025, <a class="reference external" href="https://arxiv.org/pdf/2502.21321">https://arxiv.org/pdf/2502.21321</a></p></li>
<li><p>Understanding the Effects of RLHF on LLM Generalisation and Diversity - arXiv, 2025, <a class="reference external" href="https://arxiv.org/html/2310.06452v2">https://arxiv.org/html/2310.06452v2</a></p></li>
<li><p>Reinforcement Learning From Human Feedback (RLHF) For LLMs - <a class="reference external" href="http://Neptune.ai">Neptune.ai</a>, 2025, <a class="reference external" href="https://neptune.ai/blog/reinforcement-learning-from-human-feedback-for-llms">https://neptune.ai/blog/reinforcement-learning-from-human-feedback-for-llms</a></p></li>
<li><p>Summarization - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers/tasks/summarization">https://huggingface.co/docs/transformers/tasks/summarization</a></p></li>
<li><p>Run Hugging Face Models Instantly with Day-0 Support from NVIDIA …, 2025, <a class="reference external" href="https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework/">https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework/</a></p></li>
<li><p>CUDA on WSL User Guide — CUDA on WSL 13.0 documentation, 2025, <a class="reference external" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">https://docs.nvidia.com/cuda/wsl-user-guide/index.html</a></p></li>
<li><p>Enable NVIDIA CUDA on WSL 2 - Microsoft Learn, 2025, <a class="reference external" href="https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl">https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl</a></p></li>
<li><p>GPU support - Docker Docs, 2025, <a class="reference external" href="https://docs.docker.com/desktop/features/gpu/">https://docs.docker.com/desktop/features/gpu/</a></p></li>
<li><ol class="arabic simple">
<li><p>Overview — NVIDIA GPU Cloud Documentation, 2025, <a class="reference external" href="https://docs.nvidia.com/ngc/latest/ngc-catalog-user-guide.html">https://docs.nvidia.com/ngc/latest/ngc-catalog-user-guide.html</a></p></li>
</ol>
</li>
<li><p>Containers For Deep Learning Frameworks User Guide - NVIDIA Docs, 2025, <a class="reference external" href="https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html">https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html</a></p></li>
<li><p>Troubleshooting — NVIDIA Container Toolkit - NVIDIA Docs Hub, 2025, <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/troubleshooting.html">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/troubleshooting.html</a></p></li>
<li><p>Docker Nvidia Runtime error : r/archlinux - Reddit, 2025, <a class="reference external" href="https://www.reddit.com/r/archlinux/comments/1mqlv5k/docker_nvidia_runtime_error/">https://www.reddit.com/r/archlinux/comments/1mqlv5k/docker_nvidia_runtime_error/</a></p></li>
<li><p>NVIDIA Docker - initialization error: nvml error: driver not loaded · Issue #1393 - GitHub, 2025, <a class="github reference external" href="https://github.com/NVIDIA/nvidia-docker/issues/1393">NVIDIA/nvidia-docker#1393</a></p></li>
<li><p>Docker Fails to Launch GPU Containers with NVIDIA Runtime, but Podman Works, 2025, <a class="reference external" href="https://forums.docker.com/t/docker-fails-to-launch-gpu-containers-with-nvidia-runtime-but-podman-works/147966">https://forums.docker.com/t/docker-fails-to-launch-gpu-containers-with-nvidia-runtime-but-podman-works/147966</a></p></li>
<li><p>GPU Troubleshooting Guide: Resolving Driver/Library Version Mismatch Errors, 2025, <a class="reference external" href="https://support.exxactcorp.com/hc/en-us/articles/32810166604183-GPU-Troubleshooting-Guide-Resolving-Driver-Library-Version-Mismatch-Errors">https://support.exxactcorp.com/hc/en-us/articles/32810166604183-GPU-Troubleshooting-Guide-Resolving-Driver-Library-Version-Mismatch-Errors</a></p></li>
<li><p>How to resolve “Failed to initialize NVML: Driver/library version mismatch” error - D2iQ, 2025, <a class="reference external" href="https://support.d2iq.com/hc/en-us/articles/4409480561300-How-to-resolve-Failed-to-initialize-NVML-Driver-library-version-mismatch-error">https://support.d2iq.com/hc/en-us/articles/4409480561300-How-to-resolve-Failed-to-initialize-NVML-Driver-library-version-mismatch-error</a></p></li>
<li><p>Failed to initialize NVML: Driver/library version mismatch - Reddit, 2025, <a class="reference external" href="https://www.reddit.com/r/freebsd/comments/18zhf55/failed_to_initialize_nvml_driverlibrary_version/">https://www.reddit.com/r/freebsd/comments/18zhf55/failed_to_initialize_nvml_driverlibrary_version/</a></p></li>
<li><p>Nvidia NVML Driver/library version mismatch [closed] - Stack Overflow, 2025, <a class="reference external" href="https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch">https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch</a></p></li>
<li><p>Hiccups setting up WSL2 + CUDA - NVIDIA Developer Forums, 2025, <a class="reference external" href="https://forums.developer.nvidia.com/t/hiccups-setting-up-wsl2-cuda/128641">https://forums.developer.nvidia.com/t/hiccups-setting-up-wsl2-cuda/128641</a></p></li>
<li><p>Installing the NVIDIA Container Toolkit, 2025, <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html</a></p></li>
<li><p>Install NeMo Framework - NVIDIA Docs Hub, 2025, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/latest/installation.html">https://docs.nvidia.com/nemo-framework/user-guide/latest/installation.html</a></p></li>
<li><p>NVIDIA-NeMo/NeMo: A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech) - GitHub, 2025, <a class="github reference external" href="https://github.com/NVIDIA-NeMo/NeMo">NVIDIA-NeMo/NeMo</a></p></li>
<li><p>Quickstart - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers/quicktour">https://huggingface.co/docs/transformers/quicktour</a></p></li>
<li><p>Pipelines - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/pipelines">https://huggingface.co/docs/transformers/main_classes/pipelines</a></p></li>
<li><p>The pipeline API - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers.js/v3.0.0/pipelines">https://huggingface.co/docs/transformers.js/v3.0.0/pipelines</a></p></li>
<li><p>pipelines - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers.js/api/pipelines">https://huggingface.co/docs/transformers.js/api/pipelines</a></p></li>
<li><p>Pipeline - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers/pipeline_tutorial">https://huggingface.co/docs/transformers/pipeline_tutorial</a></p></li>
<li><p>EnverLee/polyglot-ko-1.3b-Q4_0-GGUF - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/EnverLee/polyglot-ko-1.3b-Q4_0-GGUF">https://huggingface.co/EnverLee/polyglot-ko-1.3b-Q4_0-GGUF</a></p></li>
<li><p>EleutherAI/polyglot-ko-1.3b at main - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/EleutherAI/polyglot-ko-1.3b/tree/main">https://huggingface.co/EleutherAI/polyglot-ko-1.3b/tree/main</a></p></li>
<li><p>EleutherAI/polyglot-ko-1.3b · Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/EleutherAI/polyglot-ko-1.3b">https://huggingface.co/EleutherAI/polyglot-ko-1.3b</a></p></li>
<li><p>How to run a Hugging Face text generation AI model Locally? step-by-step tutorial, 2025, <a class="reference external" href="https://www.youtube.com/watch?v=Ez_bHdET0iw">https://www.youtube.com/watch?v=Ez_bHdET0iw</a></p></li>
<li><p>Generation - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/text_generation">https://huggingface.co/docs/transformers/main_classes/text_generation</a></p></li>
<li><p>How to generate text: using different decoding methods for language generation with Transformers - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/blog/how-to-generate">https://huggingface.co/blog/how-to-generate</a></p></li>
<li><p>Temperature, top_p and top_k for chatbot responses - OpenAI Developer Community, 2025, <a class="reference external" href="https://community.openai.com/t/temperature-top-p-and-top-k-for-chatbot-responses/295542">https://community.openai.com/t/temperature-top-p-and-top-k-for-chatbot-responses/295542</a></p></li>
<li><p>Utilities for Generation - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers/en/internal/generation_utils">https://huggingface.co/docs/transformers/en/internal/generation_utils</a></p></li>
<li><p>Transformers - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a></p></li>
<li><p>Using transformers at Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/docs/hub/transformers">https://huggingface.co/docs/hub/transformers</a></p></li>
<li><p>Hugging Face pitches HUGS as an alternative to Nvidia’s NIM for open models - Reddit, 2025, <a class="reference external" href="https://www.reddit.com/r/AMD_Stock/comments/1gct7mt/hugging_face_pitches_hugs_as_an_alternative_to/">https://www.reddit.com/r/AMD_Stock/comments/1gct7mt/hugging_face_pitches_hugs_as_an_alternative_to/</a></p></li>
<li><p>Master Generative AI with NVIDIA NeMo, 2025, <a class="reference external" href="https://resources.nvidia.com/en-us-ai-large-language-models/watch-78">https://resources.nvidia.com/en-us-ai-large-language-models/watch-78</a></p></li>
<li><p>NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support, 2025, <a class="reference external" href="https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/">https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/</a></p></li>
<li><p>Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities, 2025, <a class="reference external" href="https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/">https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/</a></p></li>
<li><p>NVIDIA NeMo Framework Developer Docs, 2025, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/index.html">https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/index.html</a></p></li>
<li><p>Advantage of NEMO models &amp; Toolkit #7073 - GitHub, 2025, <a class="github reference external" href="https://github.com/NVIDIA/NeMo/discussions/7073">NVIDIA/NeMo#7073</a></p></li>
<li><p>Configuring Nemo-Guardrails Your Way: An Alternative Method for Large Language Models, 2025, <a class="reference external" href="https://towardsdatascience.com/configuring-nemo-guardrails-your-way-an-alternative-method-for-large-language-models-c82aaff78f6e/">https://towardsdatascience.com/configuring-nemo-guardrails-your-way-an-alternative-method-for-large-language-models-c82aaff78f6e/</a></p></li>
<li><p>Configure NeMo-Run — NVIDIA NeMo Framework User Guide - NVIDIA Docs Hub, 2025, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemorun/guides/configuration.html">https://docs.nvidia.com/nemo-framework/user-guide/latest/nemorun/guides/configuration.html</a></p></li>
<li><p>RunBot’s math-to-text on NVIDIA NeMo Framework AutoModel - LoRA - vLLM Forums, 2025, <a class="reference external" href="https://discuss.vllm.ai/t/runbots-math-to-text-on-nvidia-nemo-framework-automodel/637">https://discuss.vllm.ai/t/runbots-math-to-text-on-nvidia-nemo-framework-automodel/637</a></p></li>
<li><p>Community Model Converter User Guide — NVIDIA NeMo …, 2025, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/24.09/nemotoolkit/checkpoints/user_guide.html">https://docs.nvidia.com/nemo-framework/user-guide/24.09/nemotoolkit/checkpoints/user_guide.html</a></p></li>
<li><p>Checkpoint Conversion — NVIDIA NeMo Framework User Guide 24.07 documentation, 2025, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/24.07/llms/starcoder2/checkpointconversion.html">https://docs.nvidia.com/nemo-framework/user-guide/24.07/llms/starcoder2/checkpointconversion.html</a></p></li>
<li><p>Checkpoint conversion - NeMo-Skills, 2025, <a class="reference external" href="https://nvidia.github.io/NeMo-Skills/pipelines/checkpoint-conversion/">https://nvidia.github.io/NeMo-Skills/pipelines/checkpoint-conversion/</a></p></li>
<li><p>HFDatasetDataModule — NVIDIA NeMo Framework User Guide, 2025, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/automodel/codedocs/hf_dataset_data_module.html">https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/automodel/codedocs/hf_dataset_data_module.html</a></p></li>
<li><p>e9t/nsmc · Datasets at Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/datasets/e9t/nsmc">https://huggingface.co/datasets/e9t/nsmc</a></p></li>
<li><p>nglaura/koreascience-summarization · Datasets at Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/datasets/nglaura/koreascience-summarization">https://huggingface.co/datasets/nglaura/koreascience-summarization</a></p></li>
<li><p>gogamza/kobart-summarization - Hugging Face, 2025, <a class="reference external" href="https://huggingface.co/gogamza/kobart-summarization">https://huggingface.co/gogamza/kobart-summarization</a></p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./workshops"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LLM From Scratch Workshop</p>
      </div>
    </a>
    <a class="right-next"
       href="../projects/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Team Project Guidelines</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workshop-introduction-exploring-the-journey-of-large-language-models"><strong>Workshop Introduction: Exploring the Journey of Large Language Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-in-depth-analysis-of-the-complete-llm-lifecycle"><strong>Part 1: In-Depth Analysis of the Complete LLM Lifecycle</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-1-scope-definition-and-problem-formulation"><strong>Stage 1: Scope Definition and Problem Formulation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-2-data-collection-and-refinement"><strong>Stage 2: Data Collection and Refinement</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-3-pre-training"><strong>Stage 3: Pre-training</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-4-supervised-fine-tuning-sft"><strong>Stage 4: Supervised Fine-Tuning (SFT)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-5-alignment-and-safety-tuning-rlhf-dpo"><strong>Stage 5: Alignment and Safety Tuning (RLHF/DPO)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-6-evaluation-and-benchmarking"><strong>Stage 6: Evaluation and Benchmarking</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-7-deployment-and-inference-optimization"><strong>Stage 7: Deployment and Inference Optimization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stage-8-monitoring-and-maintenance"><strong>Stage 8: Monitoring and Maintenance</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-question-1-what-is-the-most-important-stage-in-the-llm-lifecycle"><strong>Checkpoint Question 1: What is the most important stage in the LLM lifecycle?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-development-environment-setup-nvidia-ngc-hands-on-guide"><strong>Part 2: Development Environment Setup: NVIDIA NGC Hands-on Guide</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-checklist"><strong>Prerequisites Checklist</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-installation-guide"><strong>Step-by-Step Installation Guide</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#essential-python-library-installation"><strong>Essential Python Library Installation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#troubleshooting-guide"><strong>Troubleshooting Guide</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-first-encounter-running-llms-with-hugging-face-transformers"><strong>Part 3: First Encounter: Running LLMs with Hugging Face Transformers</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-power-of-simplicity-pipeline-api"><strong>The Power of Simplicity: Pipeline API</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-1-first-text-generation"><strong>Practice 1: First Text Generation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-2-korean-text-generation"><strong>Practice 2: Korean Text Generation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-control-generation-parameter-guide"><strong>Output Control: Generation Parameter Guide</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4-the-two-frameworks-story-nemo-and-hugging-face"><strong>Part 4: The Two Frameworks Story: NeMo and Hugging Face</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#philosophical-deep-dive"><strong>Philosophical Deep Dive</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-analysis-nemo-vs-hugging-face-transformers"><strong>Comparative Analysis: NeMo vs Hugging Face Transformers</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bridging-the-gap-interoperability-and-coexistence"><strong>Bridging the Gap: Interoperability and Coexistence</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-and-week-1-team-challenge"><strong>Conclusion and Week 1 Team Challenge</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#week-1-summary"><strong>Week 1 Summary</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#week-2-preview"><strong>Week 2 Preview</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#week-1-team-challenge-recommended"><strong>Week 1 Team Challenge (Recommended)</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references"><strong>References</strong></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
