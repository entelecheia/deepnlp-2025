
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 14: 2025년 NLP 현황: 확장 모델에서 능동 에이전트로 &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week14/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LLM From Scratch 워크숍" href="../workshops/index.html" />
    <link rel="prev" title="Week 13: 온톨로지와 AI" href="../week13/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          한국어 <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    딥러닝자연어처리 (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer 및 차세대 아키텍처</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x와 최신 딥러닝 프레임워크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: 현대적 PEFT 기법을 활용한 효율적 파인튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: 고급 프롬프트 기법과 최적화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM 평가 패러다임과 벤치마크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: 멀티모달 NLP의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: 초장문맥 처리와 효율적 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: 핵심 복습 및 최신 동향</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week09/index.html">Week 9: 고급 RAG 아키텍처</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week10/index.html">Week 10: 정렬 기법의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week11/index.html">Week 11: 프로덕션 에이전트 시스템</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week12/index.html">Week 12: AI 규제와 책임 있는 AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week13/index.html">Week 13: 온톨로지와 AI</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 14: 2025년 NLP 현황: 확장 모델에서 능동 에이전트로</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workshops/index.html">LLM From Scratch 워크숍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workshops/week01.html">1주차 워크숍: LLM 개요 및 개발 환경 구축</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">프로젝트 운영 가이드라인</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">강의계획서</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/ko/week14/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek14/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week14/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 14: 2025년 NLP 현황: 확장 모델에서 능동 에이전트로</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.0 서론: 스케일링 이후 시대</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1 배경 설정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.2 2025년 연구 현황</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">1.3 시장 및 산업 맥락 (2025)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.0 1부: 아키텍처 혁명 (트랜스포머를 넘어서)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">2.1 문제: 트랜스포머의 병목</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssm-mamba">2.2 심화: 상태 공간 모델(SSM)과 Mamba의 부상</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">2.2.1 개념적 개요</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-gu-dao-2023-2024">2.2.2 주요 논문 리뷰: “Mamba: 선택적 상태 공간을 사용한 선형 시간 시퀀스 모델링” (Gu &amp; Dao, 2023/2024)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe">2.3 심화: 스케일링 패러다임으로서의 전문가 혼합(MoE)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">2.3.1 개념적 개요</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mome-neurips-2024">2.3.2 주요 논문 리뷰: “MoME: 범용 멀티모달 대규모 언어 모델을 위한 멀티모달 전문가 혼합” (NeurIPS 2024)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">2.4 분기된 아키텍처 미래</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vs-ssm-mamba-vs-moe">2.5 아키텍처 비교: 트랜스포머 vs. SSM (Mamba) vs. MoE</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">3.0 2부: 새로운 능력 전선: 에이전트 AI</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.1 생성 모델에서 자율 에이전트로</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mas">3.2 심화: 다중 에이전트 시스템(MAS)과 창발적 행동</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-laboratory-llm-schmidgall-et-al-2025">3.3 주요 논문 리뷰: “Agent Laboratory: 연구 보조원으로서의 LLM 에이전트 사용” (Schmidgall et al., 2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-vs">3.4 2025년 에이전트 AI 논쟁: 자율성 vs. 제어</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3.5 평가 위기: 에이전트를 어떻게 벤치마크할 것인가?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">4.0 3부: 새로운 영역: 진정한 멀티모달리티</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#any-to-any-mllm">4.1 융합 인코더를 넘어서: “Any-to-Any” MLLM으로</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-gpt-any-to-any-llm-wu-et-al-icml-2024">4.2 주요 논문 리뷰: “NExT-GPT: Any-to-Any 멀티모달 LLM” (Wu et al., ICML 2024)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">4.3 심화: 비디오-언어 전선</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grounded-videollm-wang-et-al-emnlp-2025">4.4 주요 논문 리뷰: “Grounded-VideoLLM: 비디오 대규모 언어 모델에서 세밀한 시간적 그라운딩 강화” (Wang et al., EMNLP 2025)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">5.0 4부: 큰 논쟁: 추론, 신뢰성, 안전</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">5.1 추론 논쟁 (2025): 앵무새인가 사고자인가?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-meincke-et-al-2025">5.2 주요 보고서 리뷰: “프롬프팅에서 Chain of Thought의 감소하는 가치” (Meincke et al., 2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-as-a-judge">5.3 감독 자동화: LLM-as-a-Judge</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evalplanner-thinking-llm-as-a-judge-saha-et-al-2025">5.4 주요 논문 리뷰: “EvalPlanner: Thinking-LLM-as-a-Judge를 위한 선호도 최적화 알고리즘” (Saha et al., 2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vs">5.5 정렬 트레이드오프: 안전 vs. 능력</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">5.6 주요 논문 리뷰: “안전 세금: 안전 정렬이 대규모 추론 모델을 덜 합리적으로 만든다” (2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">5.7 사전 방어: 레드 팀의 공식화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mart-llm-zhu-et-al-naacl-2024">5.8 주요 논문 리뷰: “MART: 다중 라운드 자동 레드 팀으로 LLM 안전 개선” (Zhu et al., NAACL 2024)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">6.0 5부: 데이터 엔진: 자기 지도 학습과 합성 생성</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssl">6.1 자기 지도 학습(SSL)의 역할</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm">6.2 2025년 동향: 데이터 생성기로서의 LLM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-2025">6.3 조사 리뷰: “LLM 기반 합성 데이터 생성에 대한 조사” (2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">6.4 자기 소비 루프</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">7.0 6부: 결론 강의: 2026년 및 그 이후의 전선</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">7.1 효율성과 보편성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qnlp">7.2 양자 도약: QNLP 소개</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">7.3 최종 요약: 2026년을 위한 미해결 연구 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-14-2025-nlp">
<h1>Week 14: 2025년 NLP 현황: 확장 모델에서 능동 에이전트로<a class="headerlink" href="#week-14-2025-nlp" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>1.0 서론: 스케일링 이후 시대<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>1.1 배경 설정<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>이번 14주차 강의는 자연어 처리를 위한 딥러닝에 대한 포괄적 연구의 종합 강의다. 지난 주차들에서 우리는 분야의 진화를 면밀히 추적했다. 순환 신경망(RNN)과 합성곱 신경망(CNN)을 포함한 기초 시퀀스 모델로 시작하여, BERT와 생성형 사전 훈련 트랜스포머(GPT) 시리즈와 같은 아키텍처로 대표되는 현대 대규모 언어 모델(LLM) 혁명의 “초석” 역할을 하는 트랜스포머 아키텍처의 전환점으로 이어졌다.</p>
<p>이제 우리는 최첨단에 도달했다: 2025년 현재 NLP 연구의 상태다. 지난 5년간의 서사는 단일 추구, 즉 스케일링에 압도적으로 지배되었다. 모델 크기, 데이터, 컴퓨팅을 증가시키면 예측 가능하게 새로운 능력이 해제된다는 “스케일링 가설”은 상당한 정도로 검증되었다. 그러나 2025년에 들어서면서 이 단일 초점은 분열되고 있다. 분야는 이제 스케일링 우선 패러다임의 근본적 한계와 결과에 직면하고 있다.</p>
<p>따라서 현재 연구 현황은 중요한 패러다임 전환으로 정의된다. 주요 질문은 더 이상 단순히 “얼마나 크게 구축할 수 있는가?”가 아니라 “얼마나 효율적으로 구축할 수 있는가?”, “이 모델들이 얼마나 능동적으로 행동할 수 있는가?”, “얼마나 신뢰할 수 있는가?”로 진화했다. 이 강의는 2024년과 2025년의 최신 연구를 종합하여 분야의 새로운 전선인 에이전시, 효율성, 신뢰성을 탐구한다.</p>
</section>
<section id="id3">
<h3>1.2 2025년 연구 현황<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>이 분석을 구성하기 위해, 최신 기술을 정의하는 주요 학회의 논문집과 사전 출판물을 종합했다. 검토에는 2024년 자연어 처리 경험적 방법 학회(EMNLP 2024), 계산언어학 협회 제63차 연례 총회(ACL 2025), EMNLP 2025, 2024년 및 2025년 신경 정보 처리 시스템 학회(NeurIPS), 그리고 2024년 및 2025년 국제 기계학습 학회(ICML)의 주요 논문과 동향이 포함된다.</p>
<p>이 종합은 2025년 현황을 정의하는 세 가지 지배적이고 상호 연결된 연구 주제를 드러낸다:</p>
<ol class="arabic simple">
<li><p>에이전시: LLM이 수동적 텍스트 생성기에서 목표 지향적 자율 시스템인 LLM 에이전트로의 개념적이고 실용적인 전환. 이는 계획, 도구 사용, 다단계 추론 능력을 모델에 추가하는 것을 포함한다.</p></li>
<li><p>효율성: 표준 트랜스포머에서 벗어난 뚜렷한 아키텍처적 분기. 이 동향은 트랜스포머의 이차 스케일링 병목을 해결하려는 긴급한 필요에 의해 추진되며, 상태 공간 모델(SSM)과 전문가 혼합(MoE)이라는 두 가지 주요 해결책이 논의를 지배한다.</p></li>
<li><p>신뢰성: LLM 추론의 진정한 본질에 대한 비판적이고 내성적인 검토. 이는 모델이 “생각하는” 것인지 아니면 “따라 말하는” 것인지에 대한 분야 전반의 논쟁, 능력을 희생하지 않고 정렬을 구축하는 방법(“안전 세금”), 그리고 적대적 공격에 대한 방어를 공식화하는 방법을 포함한다.</p></li>
</ol>
</section>
<section id="id4">
<h3>1.3 시장 및 산업 맥락 (2025)<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>이 학술적 전환은 진공 상태에서 발생하는 것이 아니다. 이는 대규모 산업 변환에 대한 직접적인 대응이자 동력이다. 글로벌 NLP 시장은 2025년에 약 393억 7천만 달러에 도달할 것으로 예상되며, 연평균 성장률(CAGR) 21.82%를 보여준다.</p>
<p>이 성장은 Microsoft, Google, OpenAI와 같은 기술 거대 기업들이 지배한다. 예를 들어, Microsoft는 2025년 기준 기업 채택에서 15-20%의 시장 점유율을 보유한다. 이러한 기업들은 오늘 논의할 학술적 동향을 활용하여 차세대 제품을 구동한다.</p>
<p>주요 응용 분야는 다음과 같다:</p>
<ul class="simple">
<li><p>대화형 AI 및 고객 지원: 챗봇과 가상 어시스턴트는 더 이상 단순한 규칙 기반 시스템이 아니다. 감정적 뉘앙스와 맥락을 이해하는 딥러닝 모델로 구동되며, 24/7 지원을 제공한다.</p></li>
<li><p>감정 분석: 기업들은 정교한 NLP를 사용하여 브랜드 인식을 실시간으로 모니터링하고, 미묘함, 풍자, 복잡한 감정을 감지할 수 있는 뉘앙스로 소셜 미디어, 제품 리뷰, 설문조사를 분석한다.</p></li>
<li><p>의료 및 임상 정보학: NLP는 구조화되지 않은 임상 기록에서 실행 가능한 통찰을 추출하고, 문헌을 종합하여 의학 연구를 가속화하며, 고급 임상 의사결정 지원 시스템을 구동하는 데 사용된다.</p></li>
</ul>
<p>오늘 강의를 진행하면서, 우리는 이러한 추상적 연구 동향을 실제 세계에서 해제하는 실용적 도전과 기회로 다시 연결할 것이다.</p>
</section>
</section>
<section id="id5">
<h2>2.0 1부: 아키텍처 혁명 (트랜스포머를 넘어서)<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>2.1 문제: 트랜스포머의 병목<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>2017년 논문 “Attention Is All You Need”은 트랜스포머를 소개했으며, 이 아키텍처는 거의 10년 동안 현대 NLP의 논란의 여지가 없는 초석이었다. 핵심 메커니즘인 자기 주의(self-attention)는 모델이 시퀀스의 모든 다른 토큰을 살펴봄으로써 풍부하고 맥락 인식 표현을 구축할 수 있게 한다.</p>
<p>그러나 이 메커니즘은 또한 가장 큰 약점이기도 하다. 자기 주의의 계산 및 메모리 비용은 시퀀스 길이 <span class="math notranslate nohighlight">\(n\)</span>에 대해 이차적으로 스케일된다. 이는 일반적으로 <span class="math notranslate nohighlight">\(O(n^2)\)</span>로 표현된다. 1,000개의 토큰 시퀀스의 경우 이는 관리 가능하다. 의료 기록, 책, 또는 게놈 시퀀스와 같은 1,000,000개의 토큰 시퀀스의 경우, 이 이차 비용은 계산적으로 불가능해진다.</p>
<p>이 “이차 병목”은 모델을 진정으로 긴 맥락으로 스케일링하는 데 가장 큰 장벽이었다. 희소 주의나 슬라이딩 윈도우 주의와 같은 기법이 임시 해결책을 제공했지만, 2024-2025 연구 현황은 스케일링 특성에서 <em>이차 미만</em>—이상적으로는 <em>선형</em>—인 진정한 후속 아키텍처를 찾는 것으로 정의된다. 이 탐색은 두 가지 주요하고 상호 배타적이지 않은 방향으로 분기되었다: 핵심 순환 메커니즘 변경(상태 공간 모델)과 파라미터 활성화 메커니즘 변경(전문가 혼합).</p>
</section>
<section id="ssm-mamba">
<h3>2.2 심화: 상태 공간 모델(SSM)과 Mamba의 부상<a class="headerlink" href="#ssm-mamba" title="Link to this heading">#</a></h3>
<p>첫 번째이자 아마도 가장 혁명적인 아키텍처 전환은 대규모 시퀀스 모델링을 위한 실행 가능한 백본으로서 상태 공간 모델(SSM)의 검증이다.</p>
<section id="id7">
<h4>2.2.1 개념적 개요<a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<p>SSM은 새로운 것이 아니다. 이들은 고전 제어 이론에서 기원한다. 높은 수준에서, 이들은 시간에 따라 진화하는 내부 “상태” <span class="math notranslate nohighlight">\(x\)</span>로 시스템을 나타낸다. 이 설계는 개념적으로 두 가지 다른 아키텍처의 강점을 결합한다:</p>
<ol class="arabic simple">
<li><p>RNN처럼: 이들은 순환적이다. 시간 <span class="math notranslate nohighlight">\(t\)</span>에서의 상태는 시간 <span class="math notranslate nohighlight">\(t-1\)</span>에서의 상태와 시간 <span class="math notranslate nohighlight">\(t\)</span>에서의 입력의 함수다. 이 속성은 단계당 계산이 일정하고(관리할 큰 K-V 캐시가 없음) 시퀀스 길이에 대해 선형적으로(<span class="math notranslate nohighlight">\(O(n)\)</span>) 스케일되기 때문에 자기회귀 추론에 대해 매우 빠르게 만든다.</p></li>
<li><p>CNN처럼: 이들은 큰 합성곱 커널로 표현될 수 있어, 고도로 병렬화되고 비순환적인 방식으로 훈련될 수 있다.</p></li>
</ol>
<p>이전 SSM(S4와 같은)은 오디오와 같은 연속 시간 양식에서 유망했지만, 언어와 같은 이산적이고 내용이 밀집된 양식에서 트랜스포머와 경쟁하는 데 어려움을 겪었다. 이것은 Mamba의 도입으로 바뀌었다.</p>
</section>
<section id="mamba-gu-dao-2023-2024">
<h4>2.2.2 주요 논문 리뷰: “Mamba: 선택적 상태 공간을 사용한 선형 시간 시퀀스 모델링” (Gu &amp; Dao, 2023/2024)<a class="headerlink" href="#mamba-gu-dao-2023-2024" title="Link to this heading">#</a></h4>
<p>이 논문은 트랜스포머에 대한 첫 번째 설득력 있고 프로덕션 준비된 대안을 제공하기 때문에 2024-2025 기간의 가장 중요한 아키텍처 논문 중 하나로 여겨진다.</p>
<ul class="simple">
<li><p>핵심 문제: 논문은 이전의 이차 미만 모델(선형 주의나 게이트 합성곱과 같은)의 중요한 실패를 다룬다. 이러한 모델들은 효율적이지만 트랜스포머의 <em>성능</em>에 맞추지 못했다. 저자들은 이것이 “내용 기반 추론” 메커니즘이 부족하기 때문이라고 가설을 세웠다. 주의가 강력한 이유는 어떤 토큰에 초점을 맞추는지가 토큰 자체의 <em>내용의 함수</em>이기 때문이다(Query-Key-Value 메커니즘을 통해). 이전 SSM은 시간 및 입력 불변성이었다.</p></li>
<li><p>새로운 방법론: “선택적 SSM”(S6): Mamba의 핵심 혁신은 선택 메커니즘의 도입이다. SSM의 핵심 파라미터(특히 상태 역학을 지배하는 <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span> 행렬)는 더 이상 정적이지 않다. 이들은 <em>입력 의존적</em>으로 만들어진다. 이 겉보기에 단순한 변경은 심오한 결과를 가진다. 모델이 현재 토큰의 <em>내용</em>을 기반으로 정보를 전파할지(상태에 유지) 또는 잊을지(상태를 플러시)를 <em>선택적으로</em> 결정할 수 있게 한다. 이것은 Mamba에 주의의 내용 인식 라우팅 능력을 부여하지만, SSM의 선형 시간, 순환 구조를 유지한다. 이를 효율적으로 만들기 위해, 저자들은 GPU 메모리에서 전체 상태를 구체화하는 것을 피하는 하드웨어 인식 병렬 스캔 알고리즘을 개발했다.</p></li>
<li><p>주요 결과: 결과는 변혁적이었다.</p>
<ol class="arabic simple">
<li><p>선형 시간 스케일링: Mamba는 시퀀스 길이에 대해 선형적으로(<span class="math notranslate nohighlight">\(O(n)\)</span>) 스케일된다. 이를 통해 저자들은 <em>백만 길이 시퀀스</em>까지 실제 데이터에서 강력한 성능을 입증할 수 있었다.</p></li>
<li><p>빠른 추론: 자기회귀 생성에서 Mamba는 동등한 크기의 트랜스포머보다 5배 높은 처리량을 달성한다. 이는 순환 상태가 컴팩트하기 때문이다. 컨텍스트 윈도우와 함께 성장하는 큰 메모리 대역폭 집약적 K-V 캐시가 필요하지 않다.</p></li>
<li><p>최신 성능: Mamba는 언어에서 트랜스포머 품질 성능을 달성한 첫 번째 선형 시간 모델이었다. Mamba-3B 모델은 <em>크기가 두 배인</em> 트랜스포머 모델(예: 7B 파라미터 모델)의 성능과 일치하는 것으로 나타났다. 트랜스포머가 어려움을 겪는 긴 시퀀스 양식(게놈학 및 오디오와 같은)에서 Mamba는 새로운 최신 기록을 세웠다.</p></li>
</ol>
</li>
<li><p>결론 및 영향: Mamba는 성능을 희생하지 않고 트랜스포머의 이차 병목을 효과적으로 “해결”한다. 이것은 SSM을 차세대 기초 모델의 백본 아키텍처로서 트랜스포머의 <em>후속자</em>가 될 강력하고 실행 가능한 후보로 확립했다.</p></li>
</ul>
</section>
</section>
<section id="moe">
<h3>2.3 심화: 스케일링 패러다임으로서의 전문가 혼합(MoE)<a class="headerlink" href="#moe" title="Link to this heading">#</a></h3>
<p>두 번째 주요 아키텍처 동향인 전문가 혼합(MoE)은 다른 각도에서 효율성을 공격한다. 이것은 (그 자체로는) <span class="math notranslate nohighlight">\(O(n^2)\)</span> 시퀀스 길이 문제를 해결하지 않는다. 대신, 파라미터 수 문제를 해결한다.</p>
<section id="id8">
<h4>2.3.1 개념적 개요<a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<p>표준 “밀집” 트랜스포머에서 모델의 모든 단일 파라미터는 모든 단일 토큰을 처리하기 위해 활성화된다. 모델이 수천억 개의 파라미터로 스케일링됨에 따라, 이것은 예외적으로 계산 집약적이 된다.</p>
<p>수년에 걸쳐 정제된 아이디어인 MoE는 밀집 순전달 네트워크(FFN) 레이어를 희소 MoE 레이어로 대체한다. MoE 레이어는 다음으로 구성된다:</p>
<ol class="arabic simple">
<li><p>“전문가” 집합: <span class="math notranslate nohighlight">\(N\)</span>개의 병렬 FFN(예: 8개의 전문가).</p></li>
<li><p>“라우터” 네트워크: 각 토큰을 분석하고 어떤 전문가에게 보낼지 동적으로 결정하는 작은 훈련 가능한 네트워크.</p></li>
</ol>
<p>일반적인 설정에서 라우터는 8개 전문가 중 상위 2개를 선택할 수 있다. 이것은 모델이 총 1조 개의 파라미터를 가질 수 있지만, 주어진 토큰에 대해서는 일부(예: 2천억 개)만 <em>사용</em>한다는 것을 의미한다. 이것은 추론 <em>계산</em>(FLOP)을 일정하게 유지하면서 모델 <em>용량</em>(지식)의 대규모 증가를 허용한다.</p>
</section>
<section id="mome-neurips-2024">
<h4>2.3.2 주요 논문 리뷰: “MoME: 범용 멀티모달 대규모 언어 모델을 위한 멀티모달 전문가 혼합” (NeurIPS 2024)<a class="headerlink" href="#mome-neurips-2024" title="Link to this heading">#</a></h4>
<p>(참고: 이 논문은 “MoE-LLaVA”로도 인용됨)</p>
<p>이 논문은 MoE의 2025년 진화를 예시한다. 이것은 더 이상 단순한 스케일링 트릭이 아니다. 이것은 <em>범용</em> 모델을 구축하기 위한 정교한 메커니즘이다.</p>
<ul class="simple">
<li><p>핵심 문제: 범용 멀티모달 대규모 언어 모델(MLLM)—많은 다른 작업(예: 이미지 캡셔닝, 시각 질문 답변(VQA), 광학 문자 인식(OCR))을 처리하도록 훈련된 모델—은 “작업 간섭”을 겪는다. 이러한 다양한 작업에 대한 훈련은 종종 모델이 “모든 것에 약간씩, 아무것에도 뛰어나지 않은” 상태가 되어 거의 모든 작업에서 전문 모델보다 성능이 떨어지게 만든다.</p></li>
<li><p>새로운 방법론: 저자들은 멀티모달 전문가 혼합(MoME) 프레임워크를 제안한다. 이것은 MoE 개념을 <em>언어</em> FFN뿐만 아니라 <em>비전</em> 인코더에도 통찰력 있게 적용한다.</p>
<ol class="arabic simple">
<li><p>비전 전문가 혼합(MoVE): 모델은 <em>여러</em> 전문 비전 인코더(예: 일반 개념을 위한 CLIP-ViT, 세밀한 특징을 위한 DINOv2, 문서 텍스트를 위한 Pix2Struct)에 대한 접근 권한을 부여받는다. 라우터 네트워크는 사용자의 지시와 입력 이미지를 기반으로 이러한 인코더의 특징을 <em>적응적으로 조절</em>하고 결합하는 방법을 학습한다.</p></li>
<li><p>언어 전문가 혼합(MoLE): 모델은 또한 LLM의 FFN 레이어에 표준 MoE 레이어(파라미터 효율적 어댑터로 구현됨)를 통합하여 작업별 언어적 뉘앙스를 처리한다.</p></li>
</ol>
</li>
<li><p>주요 결과: MoME 아키텍처는 매우 효과적이었다. 라우터의 게이팅 결정 시각화는 모델이 작업 간섭을 완화하기 위해 <em>전문화</em>하는 방법을 학습했음을 확인했다. “문서” 그룹의 작업(예: OCR)이 주어졌을 때, MoVE 라우터는 “Pix2Struct” 비전 전문가에 대한 <em>강한 선호도</em>를 보여주며, 가중치의 70% 이상을 할당했다. 참조 표현 이해(REC) 작업이 주어졌을 때, 그것은 DINOv2 전문가로 크게 라우팅되었다. 이 “명확한 전문화”는 모델이 작업을 이를 처리할 수 있는 최적의 전문가에게 동적으로 라우팅했음을 보여준다.</p></li>
<li><p>결론 및 영향: 이 논문은 다른 논문들과 함께 MoE가 단순한 스케일링 기법에서 <em>범용 멀티모달 에이전트</em>를 구축하기 위한 강력한 프레임워크로 진화했음을 보여준다. 이것은 단일 모델이 전문가 “팀”을 수용하고 작업에 맞는 올바른 것을 동적으로 배치하여 “작업 간섭” 문제를 해결할 수 있게 한다. 이 동향은 도메인별 코드 생성과 같은 산업별 응용 분야에서도 활용되고 있다.</p></li>
</ul>
</section>
</section>
<section id="id9">
<h3>2.4 분기된 아키텍처 미래<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>Mamba와 MoE의 분석은 2024-2025 아키텍처 동향이 단일 전진 경로가 아니라 두 가지 구별되는 문제를 해결하기 위해 두 가지 구별되는 해결책이 등장하는 분기임을 드러낸다.</p>
<ol class="arabic simple">
<li><p>첫째, 표준 트랜스포머 아키텍처는 시퀀스 길이에 대한 <span class="math notranslate nohighlight">\(O(n^2)\)</span> 계산 복잡도로 인해 하드 스케일링 벽에 직면한다.</p></li>
<li><p>Mamba로 구현된 상태 공간 모델(SSM) 아키텍처는 이 문제를 직접 공격한다. 선택적 순환 상태를 사용하여 시퀀스 길이에서 <span class="math notranslate nohighlight">\(O(n)\)</span> 스케일링을 달성한다. 주요 이점은 <em>극도로 긴 맥락</em>(수백만 토큰)을 처리하는 능력과 <em>매우 빠른 자기회귀 추론</em>(컴팩트한 순환 상태로 인해)이다.</p></li>
<li><p>둘째, 별도의 문제는 밀집 모델의 파라미터 수다. Llama 3 70B와 같은 모델은 처리하는 <em>모든</em> 토큰에 대해 모든 700억 개의 파라미터를 활성화해야 한다. 이것은 계산적으로 비용이 많이 든다.</p></li>
<li><p>전문가 혼합(MoE) 아키텍처는 이 문제를 공격한다. 이것은 파라미터 수를 추론에 필요한 계산과 분리한다. 총 1조 개의 파라미터를 가진 모델은 토큰당 1천억~2천억 개의 활성 파라미터만 사용하도록 설계될 수 있다. 주요 이점은 <em>일정한 추론 비용</em>에서 <em>대규모 지식 용량</em>이다.</p></li>
</ol>
<p>이 두 해결책—SSM과 MoE—은 상호 배타적이지 않다. 사실, 이들은 상호 보완적이다. SSM은 시퀀스 길이 병목을 해결하고, MoE는 파라미터 지식 병목을 해결한다. 미래 SOTA 모델에 대한 명확한 함의는 둘을 결합한 하이브리드 아키텍처다: Mamba 스타일 SSM 블록으로 구축된 백본으로, 각 블록의 밀집 FFN 구성 요소가 희소 MoE 레이어로 대체된다. 이론적으로 이 “Mamba 혼합”은 양쪽의 최선을 달성할 것이다: 컨텍스트 길이에서 선형 시간 스케일링 <em>및</em> 대규모, 희소 활성화 지식.</p>
</section>
<section id="vs-ssm-mamba-vs-moe">
<h3>2.5 아키텍처 비교: 트랜스포머 vs. SSM (Mamba) vs. MoE<a class="headerlink" href="#vs-ssm-mamba-vs-moe" title="Link to this heading">#</a></h3>
<p>이 새로운 아키텍처 현황을 요약하기 위해, 다음 표는 2025년 설계 공간을 정의하는 트레이드오프의 명확한 비교를 제공한다.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>아키텍처</p></th>
<th class="head text-left"><p>핵심 메커니즘</p></th>
<th class="head text-left"><p>시퀀스 스케일링 (계산)</p></th>
<th class="head text-left"><p>파라미터 스케일링 (추론 계산)</p></th>
<th class="head text-left"><p>자기회귀 추론 속도</p></th>
<th class="head text-left"><p>주요 사용 사례 (2025)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>표준 트랜스포머 5</p></td>
<td class="text-left"><p>밀집 자기 주의</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n^2)\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(N)\)</span> (밀집)</p></td>
<td class="text-left"><p>느림 (K-V 캐시와 함께 증가)</p></td>
<td class="text-left"><p>범용, &lt;128k 컨텍스트</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>SSM (Mamba) 25</p></td>
<td class="text-left"><p>선택적 스캔 (S6)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n)\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(N)\)</span> (밀집)</p></td>
<td class="text-left"><p>매우 빠름 (토큰당 일정)</p></td>
<td class="text-left"><p>긴 컨텍스트 (&gt;1M), 빠른 추론</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MoE-트랜스포머 29</p></td>
<td class="text-left"><p>희소 게이팅 / 라우팅</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n^2)\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(k)\)</span> (희소, <span class="math notranslate nohighlight">\(k \\ll N\)</span>)</p></td>
<td class="text-left"><p>느림 (K-V 캐시와 함께 증가)</p></td>
<td class="text-left"><p>대규모 지식 스케일링</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="ai">
<h2>3.0 2부: 새로운 능력 전선: 에이전트 AI<a class="headerlink" href="#ai" title="Link to this heading">#</a></h2>
<section id="id10">
<h3>3.1 생성 모델에서 자율 에이전트로<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>아키텍트들이 LLM의 <em>엔진</em>을 재구축하는 동안, 다른 커뮤니티는 <em>엔진이 무엇에 사용되는지</em>를 재정의하고 있다. 2025년의 가장 지배적인 단일 응용 및 연구 동향은 LLM이 정적 텍스트 생성기에서 AI 에이전트로 진화하는 것이다.</p>
<p>이것은 근본적인 개념적 전환을 나타낸다. “생성 모델”은 프롬프트를 받아 텍스트 완성을 생성하는 수동 시스템이다. 대조적으로 “AI 에이전트”는 목표를 달성하기 위해 “환경을 감지하고, 결정을 내리며, 행동을 취하는” 자율 시스템이다.</p>
<p>이 전환은 기초 LLM에 새로운 능력 세트를 추가함으로써 가능해지며, 종종 “에이전트 스택”이라고 불린다. 2024년 조사 논문은 이러한 에이전트를 구축하기 위한 세 가지 주요 패러다임을 식별한다:</p>
<ol class="arabic simple">
<li><p>추론 및 계획: 복잡한 다단계 목표(예: “도쿄 5일 여행 계획하기”)를 받아 실행 가능한 하위 작업 시퀀스로 분해하는 능력.</p></li>
<li><p>도구 사용: “외부 세계”와 상호작용하는 능력. 여기에는 외부 도구 사용, API 호출(예: 항공편 예약, 날씨 확인), 또는 지식 기반을 쿼리하기 위한 검색 증강 생성(RAG) 수행이 포함된다. 이 증강은 2025-2026 연구의 중심 초점이다.</p></li>
<li><p>메모리 및 자기 개선: 과거 상호작용에서 정보를 저장하는 능력(메모리)과 피드백(예: 인간 수정 또는 도구 실행 실패)으로부터 학습하여 미래 성능을 개선하는 능력.</p></li>
</ol>
</section>
<section id="mas">
<h3>3.2 심화: 다중 에이전트 시스템(MAS)과 창발적 행동<a class="headerlink" href="#mas" title="Link to this heading">#</a></h3>
<p>2025년 연구 전선은 이미 단일 에이전트 시스템을 넘어 다중 에이전트 시스템(MAS)을 조사하는 것으로 밀어붙였다. MAS에서 여러 전문 에이전트가 협력, 토론 또는 경쟁하여 단일 에이전트에게는 너무 복잡한 문제를 해결한다. 예를 들어, 한 에이전트는 “계획자”일 수 있고, 다른 하나는 “코드 실행자”, 세 번째는 “비판자”일 수 있다.</p>
<p>이것은 새로운 매력적인 연구 분야를 열었다: 이러한 “에이전트 앙상블”의 창발적 속성. 연구자들은 더 이상 LLM만 연구하는 것이 아니다. 그들은 LLM 사회의 “사회적” 역학을 연구하고 있다.</p>
<ul class="simple">
<li><p>창발적 조정: “다중 에이전트 언어 모델의 창발적 조정”이라는 제목의 2025년 논문은 “동적 창발”과 “에이전트 간 시너지”를 측정하기 위한 정보 이론적 프레임워크를 소개한다. 이것은 간단한 프롬프트 설계가 에이전트 그룹을 “단순 집계”(개인들의 모음)로부터 목표 지향적 상호 보완성을 보여주는 “고차” 협력 시스템으로 이끌 수 있음을 보여준다.</p></li>
<li><p>창발적 언어: 2025년 조사는 다중 에이전트 강화 학습(MARL)에서 “창발적 언어”에 대한 181개 논문을 검토한다. 이 연구는 에이전트가 협력하도록 인센티브를 받을 때 목표를 달성하기 위해 새로운 효율적인 통신 프로토콜을 개발할 수 있는 방법을 탐구한다.</p></li>
<li><p>창발의 위험: 이 새로운 능력은 위험이 없다는 것이 아니다. 2025년 ICML 워크숍 논문은 중요한 경고를 제공한다: “격리된 LLM의 안전 및 정렬 성능…은 다중 에이전트… 앙상블로 전이되지 않을 가능성이 높다.” 저자들은 다중 에이전트 시스템이 “창발적 그룹 역학”을 보여주며, 감독자의 지도를 받을 때조차도 개별적으로 정렬된 에이전트가 안전하지 않은 결정에 수렴하게 만들 수 있는 “동료 압력”을 포함한다고 발견했다. 이것은 MAS를 정렬하는 것이 단일 LLM을 정렬하는 것보다 근본적으로 새로운 그리고 더 어려운 문제임을 의미한다.</p></li>
</ul>
</section>
<section id="agent-laboratory-llm-schmidgall-et-al-2025">
<h3>3.3 주요 논문 리뷰: “Agent Laboratory: 연구 보조원으로서의 LLM 에이전트 사용” (Schmidgall et al., 2025)<a class="headerlink" href="#agent-laboratory-llm-schmidgall-et-al-2025" title="Link to this heading">#</a></h3>
<p>2025년 1월에 제출된 이 논문은 매우 복잡한 실제 작업에 대해 에이전트가 “장난감”에서 “도구”로 이동하는 구체적이고 강력한 예를 제공한다. 이것은 에이전트 전선의 높은 인용 예가 되었다.</p>
<ul class="simple">
<li><p>핵심 문제: 전통적인 과학적 발견 과정은 느리고, 비용이 많이 들며, 노동 집약적이어서 연구자가 탐구할 수 있는 아이디어의 수를 제한한다.</p></li>
<li><p>새로운 방법론: “Agent Laboratory”는 단일 인간 제공 아이디어로부터 <em>전체 기계학습 연구 과정</em>을 완료하도록 설계된 자율 LLM 기반 프레임워크다. 파이프라인은 다음을 수행하는 전문 에이전트로 구성된다:</p>
<ol class="arabic simple">
<li><p>문헌 검토 수행: 기존 지식을 자율적으로 검색, 읽기 및 종합.</p></li>
<li><p>실험 수립: 연구 아이디어를 구현하고 실험을 실행하기 위한 새로운 코드 작성 및 실행.</p></li>
<li><p>보고서 작성: 결과를 분석하고 초록, 방법 섹션, 결과를 포함한 전체 컨퍼런스 스타일 연구 논문(예: ICLR용) 초안 작성.</p></li>
</ol>
</li>
<li><p>주요 결과: 프레임워크는 놀라운 능력을 보여주었다.</p>
<ol class="arabic simple">
<li><p>SOTA 코드 성능: 에이전트 생성 코드는 기계학습 작업을 위한 벤치마크인 MLE-Bench의 하위 집합에서 최신 성능을 달성했다.</p></li>
<li><p>인간-루프-중심이 중요함: 논문은 완전 자율 모드와 “코파일럿” 모드를 모두 테스트했다. 핵심 발견은 자율 모드가 <em>기능했지만</em>, 인간 연구자가 각 단계에서 피드백을 제공한 “코파일럿” 모드가 최종 연구의 전체 품질을 <em>크게</em> 개선했다는 것이었다.</p></li>
</ol>
</li>
<li><p>결론 및 영향: “Agent Laboratory”는 에이전트가 고인지, 도메인 특정 작업을 자동화할 수 있음을 보여주는 개념 증명이다. 그러나 가장 중요한 결론은 미래가 완전 자율이 아니라 인간-에이전트 협력이라는 것이다. 에이전트는 “저수준 코딩 및 작성”에서 뛰어나며, 인간 연구자가 “창의적 아이디어 생성”에 집중할 수 있게 해준다.</p></li>
</ul>
</section>
<section id="ai-vs">
<h3>3.4 2025년 에이전트 AI 논쟁: 자율성 vs. 제어<a class="headerlink" href="#ai-vs" title="Link to this heading">#</a></h3>
<p>에이전트가 “Agent Laboratory”와 같은 연구 프로토타입에서 실제 제품으로 이동함에 따라, 2025년에 중심적이고 중요한 논쟁이 등장했다: 그들은 얼마나 많은 자율성을 가져야 하는가?</p>
<p>한편으로, 연구 비전과 미디어 과대 광고는 복잡한 작업을 수행하기 위해 장기간 독립적으로 작동할 수 있는 “완전 자율 시스템”의 약속을 내세운다.</p>
<p>다른 한편으로, 기업 및 안전 의식 조직은 심오한 시스템적 위험을 인용하여 반대하고 있다. McKinsey의 2025년 보고서는 이 새로운 패러다임이 전통적인 genAI 아키텍처가 처리하도록 구축되지 않은 위험을 도입한다고 경고한다: “통제되지 않은 자율성”, “분산된 시스템 접근”, “관찰 가능성 부족”, “에이전트 확산 및 중복”. 지능형 자동화로 시작하는 것이 빠르게 “운영 혼란”이 될 수 있다고 그들은 경고한다.</p>
<p>이것은 더 실용적이고 산업 주도적 접근으로 이어졌다. 예를 들어, Anthropic은 2025년에 개발자에게 중요한 아키텍처 구분을 만들도록 조언하는 게시물을 발행했다:</p>
<ul class="simple">
<li><p>워크플로우: LLM과 도구가 “<em>사전 정의된</em> 코드 경로”를 통해 “오케스트레이션”되는 시스템. 인간이 워크플로우를 정의하고, LLM이 단계를 실행한다.</p></li>
<li><p>에이전트: LLM이 “자신의 프로세스와 도구 사용을 동적으로 지시”하는 시스템.</p></li>
</ul>
<p>Anthropic은 대부분의 실제 응용에서 “워크플로우”가 “예측 가능성”과 “제어”를 제공하기 때문에 우수하다고 결론지었다. 이 “자율성 vs. 제어” 스펙트럼은 2025년 에이전트 AI의 <em>배포</em>에 대한 정의적 도전을 나타낸다. 연구자들은 <em>가능한</em> 것의 경계를 밀어붙이고 있는 반면, 산업은 그것을 <em>신뢰할 수 있고 안전하게</em> 만들기 위한 가드레일을 구축하려고 한다.</p>
</section>
<section id="id11">
<h3>3.5 평가 위기: 에이전트를 어떻게 벤치마크할 것인가?<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>이 새로운 에이전트 패러다임은 평가 위기를 만들었다. 2025년 ArXiv 논문이 지적하듯이, 전통적인 LLM 벤치마크—일반적으로 정적 지식이나 텍스트 생성 품질을 테스트하는—는 에이전트를 평가하기에 “불충분”하다.</p>
<p>논문은 강력한 비유를 제공한다: “LLM 평가는 엔진의 성능을 검사하는 것과 같다. 대조적으로, 에이전트 평가는 다양한 운전 조건에서 자동차의 성능을 포괄적으로 평가한다.”</p>
<p>에이전트의 성능은 단순히 “엔진”(LLM)에 관한 것이 아니라 동적 환경과의 <em>상호작용</em>에 관한 것이다. 이것은 API 실패, 모호한 지시, 장기 계획을 처리해야 하는 확률적 시스템이다.</p>
<p>새로운 조사에 자세히 설명된 대로, 2025년 해결책은 완전히 새로운 평가 프레임워크의 개발이다. 이것은 EMNLP 2025의 주요 주제다. 이러한 새로운 벤치마크는 단순한 정확도에서 벗어나 새로운 “에이전트” 차원 세트를 측정한다:</p>
<ul class="simple">
<li><p>작업 완료: 에이전트가 다단계 목표를 달성했는가?</p></li>
<li><p>메모리 및 컨텍스트 유지: 에이전트가 이전 단계의 지시와 발견을 기억하는가?</p></li>
<li><p>계획 및 도구 통합: 에이전트가 계획을 수행하기 위해 도구를 올바르게 선택하고 사용할 수 있는가?</p></li>
<li><p>사용자 경험: 인간-에이전트 협력이 얼마나 효율적이고 직관적인가?</p></li>
</ul>
</section>
</section>
<section id="id12">
<h2>4.0 3부: 새로운 영역: 진정한 멀티모달리티<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<p>2025년의 세 번째 주요 동향은 멀티모달 대규모 언어 모델(MLLM)의 급속한 성숙으로, 단순한 이미지 캡셔너에서 텍스트, 이미지, 오디오, 비디오를 통해 유창하게 추론할 수 있는 진정한 “any-to-any” 시스템으로 이동하는 것이다.</p>
<section id="any-to-any-mllm">
<h3>4.1 융합 인코더를 넘어서: “Any-to-Any” MLLM으로<a class="headerlink" href="#any-to-any-mllm" title="Link to this heading">#</a></h3>
<p>첫 번째 세대 MLLM(약 2023-2024), 예를 들어 LLaVA는 주로 “입력 측”만이었다. 이들은 사전 훈련된 비전 인코더를 LLM에 융합하여 LLM이 이미지를 <em>이해</em>하고 그것에 대해 <em>텍스트를 생성</em>하는 능력을 부여했다.</p>
<p>2024-2025 패러다임 전환은 “any-to-any” 모델을 향한다. 목표는 입력으로 <em>어떤</em> 양식 조합도 받아들일 수 있고(예: 비디오와 오디오 질문) 어떤 양식으로도 출력을 <em>생성</em>할 수 있는 단일 모델이다(예: 텍스트 설명이 있는 편집된 비디오).</p>
</section>
<section id="next-gpt-any-to-any-llm-wu-et-al-icml-2024">
<h3>4.2 주요 논문 리뷰: “NExT-GPT: Any-to-Any 멀티모달 LLM” (Wu et al., ICML 2024)<a class="headerlink" href="#next-gpt-any-to-any-llm-wu-et-al-icml-2024" title="Link to this heading">#</a></h3>
<p>이 ICML 2024 논문은 이 “any-to-any” 비전을 위한 아키텍처 청사진을 제공한다.</p>
<ul class="simple">
<li><p>핵심 문제: “입력 측” 멀티모달 이해에서 “any-to-any” 멀티모달 <em>생성</em>으로의 격차를 메우는 것.</p></li>
<li><p>새로운 방법론: NExT-GPT의 아키텍처는 우아하다. 이것은 LLM을 중앙 인지 라우터로 배치한다. 시스템은 세 가지 기성품, 사전 훈련된 구성 요소를 연결한다:</p>
<ol class="arabic simple">
<li><p>멀티모달 인코더: 입력(이미지, 비디오, 오디오)을 “인지”하는 기존 모델.</p></li>
<li><p>중앙 LLM: 추론을 수행하고, 결정적으로 “양식 전환”을 수행하는 “뇌”.</p></li>
<li><p>멀티모달 확산 디코더: LLM의 지시에 따라 콘텐츠(이미지, 오디오)를 생성할 수 있는 기존 모델(Stable Diffusion과 같은).<br />
시스템은 새로운 “양식 전환 지시 튜닝(MosIT)” 데이터셋을 사용하여 파라미터의 1%만(구성 요소를 연결하는 작은 어댑터)으로 훈련된다.</p></li>
</ol>
</li>
<li><p>주요 결과: MosIT 데이터셋은 성공적으로 “복잡한 교차 모달 의미 이해 및 콘텐츠 생성으로 NExT-GPT를 강화”했다. 모델은 예를 들어, 이미지와 텍스트 프롬프트를 입력으로 받아 새로운 편집된 이미지를 출력으로 생성할 수 있다.</p></li>
<li><p>결론 및 영향: 이 논문은 “보편적 양식을 모델링할 수 있는 통합 AI 에이전트”를 보여준다. 이 아키텍처—기존 인코더와 디코더를 조정하기 위해 LLM을 중앙 추론 레이어로 사용—는 복잡한 생성 멀티모달 시스템을 구축하기 위한 2024-2025의 주요 동향이 되었다.</p></li>
</ul>
</section>
<section id="id13">
<h3>4.3 심화: 비디오-언어 전선<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>이미지-언어 작업이 성숙하는 동안, 비디오-언어 전선이 가장 활발한 연구가 일어나는 곳이다. 비디오는 시간의 근본적 복잡성을 도입한다. 많은 EMNLP 2025 논문에서 강조된 바와 같이, 2025년의 주요 도전은 “세밀한 시간적 그라운딩”—자연어 설명을 비디오의 <em>특정하고 정확한 순간</em>에 연결하는 능력—이다.</p>
</section>
<section id="grounded-videollm-wang-et-al-emnlp-2025">
<h3>4.4 주요 논문 리뷰: “Grounded-VideoLLM: 비디오 대규모 언어 모델에서 세밀한 시간적 그라운딩 강화” (Wang et al., EMNLP 2025)<a class="headerlink" href="#grounded-videollm-wang-et-al-emnlp-2025" title="Link to this heading">#</a></h3>
<p>이 EMNLP 2025 <em>Findings</em> 논문은 이 새로운 전선을 공격하는 연구의 완벽한 예이다.</p>
<ul class="simple">
<li><p>핵심 문제: 기존 Video-LLM은 세밀한 시간적 그라운딩에 어려움을 겪는다. 이들은 비디오의 거친 요약을 제공할 수 있지만 “0:31과 0:34 <em>사이에</em> 그 사람이 무엇을 말했는가?”와 같은 질문에 답할 수 없다. 논문은 원인을 “비효과적인 시간 모델링과 부적절한 타임스탬프 표현”으로 식별한다.</p></li>
<li><p>새로운 방법론: 저자들은 여러 혁신을 도입하는 Grounded-VideoLLM을 제안한다:</p>
<ol class="arabic simple">
<li><p>이중 스트림 인코더: 한 스트림에서 <em>프레임 간 관계</em>(예: 움직임)를 명시적으로 포착하면서 다른 스트림에서 <em>프레임 내 시각적 세부사항</em>을 보존하는 새로운 인코더.</p></li>
<li><p>점진적 훈련: “부드러운 학습 곡선”을 보장하는 다단계 훈련 전략. 모델은 먼저 간단한 비디오 캡션 작업에 대해 훈련된 다음, “복잡한 비디오 시간적 그라운딩 작업에 점진적으로 도입”된다.</p></li>
<li><p>합성 데이터: 시간적 추론을 강화하기 위해, 저자들은 자동 주석 파이프라인을 사용하여 그라운딩 정보가 있는 새로운 “VideoQA 데이터셋”을 구축했다. 이것은 곧 논의할 합성 데이터 동향의 또 다른 예이다.</p></li>
</ol>
</li>
<li><p>주요 결과: “광범위한 실험은 Grounded-VideoLLM이 세밀한 그라운딩 작업에서 기존 모델을 능가할 뿐만 아니라 일반 비디오 이해 어시스턴트로서 강력한 잠재력을 보여준다”는 것을 입증한다.</p></li>
<li><p>결론 및 영향: 이 논문은 2025년 멀티모달 동향을 대표한다: “전체 비디오” 이해를 넘어 정확한 “순간” 시간적 추론으로 이동. 이것은 비디오 분석, 로봇공학, 인간-컴퓨터 상호작용과 같은 응용 분야에 대한 중요한 능력이다.</p></li>
</ul>
</section>
</section>
<section id="id14">
<h2>5.0 4부: 큰 논쟁: 추론, 신뢰성, 안전<a class="headerlink" href="#id14" title="Link to this heading">#</a></h2>
<p>모델이 더 능력이 있게 됨에 따라, 2025년 연구 현황은 이러한 능력의 <em>신뢰성</em>에 대한 비판적이고 내성적인 논쟁에 지배된다. 이 모델들이 <em>진정으로</em> 추론할 수 있는가? 신뢰할 수 있는가? 그리고 그것들을 “안전하게” 만드는 숨겨진 트레이드오프는 무엇인가?</p>
<section id="id15">
<h3>5.1 추론 논쟁 (2025): 앵무새인가 사고자인가?<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<p>이 논쟁은 수년간 끓어왔지만, 2025년은 새로운 미묘한 증거를 가져왔다.</p>
<ul class="simple">
<li><p>배경: 논쟁은 LLM이 단순히 “이해”가 없는 대규모 패턴 매처라는 것을 주장한 2021년 “Stochastic Parrots” 논문에 의해 촉발되었다. 반론은 스케일링된 모델에 의해 입증된 “창발 능력”에서 나왔으며, 가장 주목할 만한 것은 Chain-of-Thought (CoT) 추론과 Tree of Thoughts (ToT)와 같은 후속 연구로, 모델에 “단계별로 생각하라”고 프롬프트하는 것이 그렇지 않으면 실패할 복잡한 추론 문제를 해결할 수 있게 했다는 것을 보여주었다.</p></li>
<li><p>미묘한 관점 (2025): 2025년의 논쟁은 훨씬 더 정교하다. Apple의 논란의 여지가 있는 “Illusion of Thinking”과 같은 최근 논문은 대규모 추론 모델(LRM)이 복잡한 분포 외 추론 작업(예: 하노이의 탑)에서 실패한다고 주장하며, 여전히 단순히 “확률적 앵무새”일 뿐이라고 제안했다.</p>
<ul>
<li><p>그러나 2025년에 발표된 후속 연구는 중요한 반박을 제공했다. 이 새로운 연구는 “Illusion of Thinking” 실험이 <em>근본적으로 결함이 있다</em>는 것을 발견했다. 예를 들어, 그들은 River Crossing 퍼즐의 <em>해결 불가능한</em> 구성에서 모델을 테스트했다. 연구자들이 <em>같은 모델</em>을 <em>해결 가능한</em> 퍼즐에서 테스트했을 때, LRM은 “100개 이상의 에이전트가 있는 인스턴스를 쉽게 해결”했다.</p></li>
<li><p>이것은 2025년 합의로 이어진다: 진실은 미묘하고 중간에 있다. LRM은 단순히 “패턴 매칭 앵무새”가 아니지만, 또한 “인간 수준의 추론자”도 아니다. 더 정확한 설명은 그것들이 고차원 잠재 공간에서 “확률적, RL 조정 검색자”라는 것이다. 일부 작업은 검색 메커니즘에 대해 사소하다(대규모에서도), 반면 다른 작업(복잡한 하노이의 탑과 같은)은 일관되게 그것들을 깨뜨린다.</p></li>
</ul>
</li>
</ul>
</section>
<section id="chain-of-thought-meincke-et-al-2025">
<h3>5.2 주요 보고서 리뷰: “프롬프팅에서 Chain of Thought의 감소하는 가치” (Meincke et al., 2025)<a class="headerlink" href="#chain-of-thought-meincke-et-al-2025" title="Link to this heading">#</a></h3>
<p>2025년 6월의 이 기술 보고서는 추론 논쟁에 비판적이고 실용적인 차원을 추가한다. 이것은 CoT 프롬프팅이 항상 우수한 방법이라는 보편적 가정에 도전한다.</p>
<ul class="simple">
<li><p>핵심 문제: 보고서는 CoT 프롬프팅(“단계별로 생각하기”)이 보편적으로 유익한 관행이라는 일반적인 지혜를 조사한다.</p></li>
<li><p>방법론: 연구자들은 박사 수준의 객관식 질문에서 현대 2025 모델 세트를 테스트했다. 그들은 결정적으로 구별했다:</p>
<ol class="arabic simple">
<li><p>“비추론 모델” (예: GPT-4o, Sonnet 3.5)</p></li>
<li><p>“추론 모델” (예: o4-mini, Flash 2.5)<br />
그들은 “직접” 프롬프트와 “단계별”(CoT) 프롬프트를 비교하여 각 질문을 25번 실행하여 성능과 일관성을 측정했다.</p></li>
</ol>
</li>
<li><p>주요 발견: 결과는 놀랍고 “CoT가 항상 더 낫다”는 독단에 도전했다.</p>
<ol class="arabic simple">
<li><p>추론 모델의 경우: CoT는 “수익 체감”을 보여주었다. 모델은 정확도에서 “소폭의 이점”만 보여주었으며, 이것은 “드물게” “상당한 시간 비용”(응답 시간의 20-80% 증가)을 정당화했다.</p></li>
<li><p>비추론 모델의 경우: 결과는 혼재되었다. CoT가 일부 모델에서 “적당한 평균 개선”을 생산했지만, 또한 “변동성을 증가”시켰다. 이것은 핵심 발견이다: CoT는 모델이 이전에 <em>올바르게</em> 답한 문제에서 <em>답을 변경</em>하게 만들어 새로운 오류로 이어졌다.</p></li>
</ol>
</li>
<li><p>결론 및 영향: 이 보고서는 모델(특히 “추론 모델”)이 복잡한 추론 능력을 내재화함에 따라, 명시적 CoT 프롬프팅이 신뢰할 수 없는 지주가 될 수 있음을 시사한다. 이것은 마법의 “생각하기” 버튼이 아니다. 이것은 단순히 모델을 다른, 더 긴 “확률적 검색” 경로로 강제하며, 이것이 더 나을 것이라고 보장되지 않으며, 실제로 더 나쁠 수 있다. 이것은 모델 아키텍처가 개선됨에 따라 CoT 프롬프팅의 가치가 <em>감소</em>할 것임을 시사한다.</p></li>
</ul>
</section>
<section id="llm-as-a-judge">
<h3>5.3 감독 자동화: LLM-as-a-Judge<a class="headerlink" href="#llm-as-a-judge" title="Link to this heading">#</a></h3>
<p>LLM 생성 출력(특히 에이전트로부터)이 더 길고 복잡해짐에 따라, 인간 평가는 중요한 병목이 되었다. 이것은 LLM-as-a-Judge 사용—다른 모델의 출력을 평가, 점수화, 심지어 피드백을 제공하기 위해 강력한 LLM을 사용—의 2024-2025 동향으로 이어졌다. 그러나 이것은 단순히 문제를 한 단계 위로 밀어올린다: <em>판사</em>가 신뢰할 수 있고 공정하며 투명한지 어떻게 보장할 것인가?</p>
</section>
<section id="evalplanner-thinking-llm-as-a-judge-saha-et-al-2025">
<h3>5.4 주요 논문 리뷰: “EvalPlanner: Thinking-LLM-as-a-Judge를 위한 선호도 최적화 알고리즘” (Saha et al., 2025)<a class="headerlink" href="#evalplanner-thinking-llm-as-a-judge-saha-et-al-2025" title="Link to this heading">#</a></h3>
<p>이 2025년 논문(ACL 2025에 수락됨)은 더 나은 LLM 판사를 구축하는 문제를 직접 다룬다.</p>
<ul class="simple">
<li><p>핵심 문제: 이전 판사 모델은 제한적이었다. 그들의 추론은 종종 “수동 설계 구성 요소”(예: 고정된 기준 목록)에 “제한”되었고, 그들은 “평가를 위한 추론과 계획을 얽히게” 했다. 그들은 고정된 인간 제공 루브릭을 따랐다.</p></li>
<li><p>새로운 방법론: EvalPlanner는 더 강력하고 분리된 CoT를 제안한다. 모델은 선호도 최적화(좋은/나쁜 평가 쌍으로부터 학습)를 통해 두 단계 프로세스를 수행하도록 훈련된다:</p>
<ol class="arabic simple">
<li><p>평가 계획 생성: 먼저, 모델은 주어진 응답을 <em>어떻게</em> 평가할 것인지에 대한 <em>제약 없는</em> “레시피”를 생성한다. 이 계획은 특정 질문과 답변에 맞춰진다.</p></li>
<li><p>계획 실행: 둘째, 모델은 최종 판결에 도달하기 위해 자신의 계획을 단계별로 따른다.</p></li>
</ol>
</li>
<li><p>주요 결과: EvalPlanner는 <em>더 적은</em> 합성 생성 선호도 쌍으로 훈련되었음에도 불구하고 RewardBench와 PPE에서 새로운 최신 성능을 달성했다. 핵심은 <em>평가 계획을 학습하는</em> 것이 단순히 고정된 평가 템플릿을 <em>따르는</em> 것보다 더 견고하고 일반화 가능한 전략이었다는 것이다.</p></li>
<li><p>결론 및 영향: 이 논문은 메타 추론의 중요한 단계를 보여준다: 모델에게 <em>계획을 계획하는 방법</em>을 가르치는 것. 이 능력은 에이전트 시대에 필요한 견고하고 투명하며 확장 가능한 자동 평가 시스템을 만드는 데 필수적이다.</p></li>
</ul>
</section>
<section id="vs">
<h3>5.5 정렬 트레이드오프: 안전 vs. 능력<a class="headerlink" href="#vs" title="Link to this heading">#</a></h3>
<p>아마도 2025년의 가장 중요하고 논쟁적인 논쟁은 AI 안전 및 정렬을 둘러싸고 있다. 최신 모델을 구축하기 위한 표준 산업 파이프라인은 다음과 같이 되었다: (1) 사전 훈련, (2) 지시 미세 조정(SFT) 및/또는 추론 미세 조정, 그리고 (3) 안전 정렬, 종종 인간 피드백으로부터의 강화 학습(RLHF) 또는 직접 선호도 최적화(DPO)를 사용.</p>
<p>2025년 사전 출판물에서 강조된 중요한 발견은 이 최종 안전 단계(3단계)가 모델의 추론 능력(2단계에서)을 적극적으로 <em>손상</em>시킬 수 있다는 것이다. 이 현상은 “안전 세금”이라고 불려왔다.</p>
<p>이 파이프라인을 조사한 연구는 다음을 발견했다:</p>
<ol class="arabic simple">
<li><p>추론을 위한 기본 모델 미세 조정(예: 수학 CoT 데이터에서)은 추론 점수를 크게 <em>개선</em>하지만 또한 안전을 <em>저하</em>시켜 오용에 더 취약하게 만들 수 있다.</p></li>
<li><p>그런 다음, 안전 정렬 적용(예: 유해 질문/정중한 거부 쌍에서 미세 조정)은 성공적으로 모델의 안전을 <em>복원</em>하고 무해하게 만든다.</p></li>
<li><p>그러나 이 안전 정렬은 동시에 모델의 추론 능력을 <em>저하</em>시킨다. 한 논문은 추론 정확도가 7%에서 30% 하락했다고 보고했다.</p></li>
</ol>
<p>이것은 근본적이고 해결되지 않은 트레이드오프를 의미한다. 현재 정렬 방법은 <em>똑똑한</em> 모델과 <em>안전한</em> 모델 사이의 선택을 강제하는 것으로 보인다. 이것은 모델을 안전하게 만드는 것이 그것들을 “덜 합리적으로” 만들 수 있음을 시사하기 때문에 분야에서 가장 중요한 미해결 문제 중 하나다.</p>
</section>
<section id="id16">
<h3>5.6 주요 논문 리뷰: “안전 세금: 안전 정렬이 대규모 추론 모델을 덜 합리적으로 만든다” (2025)<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<p>이 2025년 3월 사전 출판물은 이 트레이드오프에 대한 가장 직접적인 증거를 제공한다.</p>
<ul class="simple">
<li><p>핵심 문제: 높은 추론 성능을 위해 이미 미세 조정된 대규모 추론 모델(LRM)에 적용될 때 안전 정렬(거부 쌍에 대한 SFT)의 영향을 체계적으로 조사하는 것.</p></li>
<li><p>방법론: 저자들은 명확한 두 단계 파이프라인을 사용했다: (1) 추론 훈련, 그 다음 (2) 안전 정렬. 그들은 각 단계 후에 추론 정확도(GPQA와 같은 벤치마크에서)와 유해성(BeaverTails에서)을 측정했다. 그들은 두 가지 유형의 안전 데이터셋을 테스트했다: 긴 “COT Refusal” 추적이 있는 것과 “DirectRefusal” 답변이 있는 것.</p></li>
<li><p>주요 발견:</p>
<ul>
<li><p>안전 정렬은 모델을 안전하게 만드는 데 <em>작동</em>했으며, 유해성 점수를 성공적으로 감소시켰다.</p></li>
<li><p>그러나 이것은 “추론 능력 저하…”의 직접적인 <em>비용</em>으로 왔다.</p></li>
<li><p>트레이드오프는 명시적이었다: “DirectRefusal” 데이터로 안전 정렬은 안전을 복원하는 데 <em>가장 효과적</em>이었지만 추론에 대해서도 <em>가장 손상적</em>이었으며, 정확도가 30.91% 하락했다.</p></li>
</ul>
</li>
<li><p>결론 및 영향: 논문은 이 순차적 파이프라인이 그들이 “안전 세금”이라고 부르는 “피할 수 없는 트레이드오프”를 제시한다고 결론지었다. 이 발견은 전체 정렬 연구 프로그램에 중요한 도전을 제시하며 “정렬 세금”에 대한 더 넓은 대화의 일부다.</p></li>
</ul>
</section>
<section id="id17">
<h3>5.7 사전 방어: 레드 팀의 공식화<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>모델 실패의 높은 위험을 고려할 때, 분야는 반응적 패치에서 사전 방어로 이동했다. 이것이 레드 팀의 목표다: 배포 <em>전에</em> 모델의 취약점, 편향, 안전 결함을 찾기 위해 적대적으로 테스트하는 구조화된 프로세스. 이것은 모든 주요 연구소에서 공식화된 관행이 되었다.</p>
<p>그러나 인간 전문가에 의한 수동 레드 팀은 “비용이 많이 들고” 느리다. 이것은 <em>자동화된</em> 레드 팀 시스템에 대한 필요성을 만들었으며, 이것 자체가 2025년 주요 연구 주제다.</p>
</section>
<section id="mart-llm-zhu-et-al-naacl-2024">
<h3>5.8 주요 논문 리뷰: “MART: 다중 라운드 자동 레드 팀으로 LLM 안전 개선” (Zhu et al., NAACL 2024)<a class="headerlink" href="#mart-llm-zhu-et-al-naacl-2024" title="Link to this heading">#</a></h3>
<p>이 NAACL 2024 논문은 레드 팀 병목에 대한 확장 가능한 해결책을 제안한다.</p>
<ul class="simple">
<li><p>핵심 문제: 수동 레드 팀은 확장 가능하지 않다. 기존 자동 방법은 안전 위험을 <em>발견</em>하는 데 좋지만 그것들을 <em>해결</em>하지는 않는다.</p></li>
<li><p>새로운 방법론 (MART): 다중 라운드 자동 레드 팀(MART) 방법은 두 LLM 사이의 반복적 “게임”을 제안한다:</p>
<ol class="arabic simple">
<li><p>적대적 LLM은 대상 LLM으로부터 안전하지 않은 응답을 이끌어내기 위해 “도전적인 프롬프트”(탈옥)를 생성하도록 프롬프트된다.</p></li>
<li><p>대상 LLM의 안전하지 않은 응답이 수집된다.</p></li>
<li><p>이 새로운 실패 데이터셋은 대상 LLM을 안전 미세 조정하는 데 사용되어 “업데이트된” 더 안전한 버전을 만든다.</p></li>
<li><p>루프가 반복되며, 적대적 LLM이 이제 <em>개선된</em> 대상 LLM에 대해 <em>새롭고 더 어려운</em> 공격을 만든다.</p></li>
</ol>
</li>
<li><p>주요 결과: 방법은 매우 효과적이었다. 대상 LLM(제한된 초기 정렬을 가진)의 위반률은 MART의 단 4라운드 후 84.7% 감소했다.</p></li>
<li><p>중요한 발견: 가장 중요한 것은, “비적대적 프롬프트에서 모델의 도움은 반복 전반에 걸쳐 안정적으로 유지된다”는 것이다.</p></li>
<li><p>결론 및 영향: MART는 안전 정렬을 위한 확장 가능하고 자동화된 프레임워크를 제공한다. 이 발견은 “안전 세금” 논문에 대한 중요한 대조점을 제공한다. 이것은 안전 조정을 <em>어떻게</em> 하는지가 중요함을 시사한다. 반복적이고 적대적인 미세 조정 루프(MART와 같은)는 일반적인 도움의 재앙적 저하 <em>없이</em> 안전을 개선할 수 있을 것이다. 이것은 적응적 적대적 공격 및 방어에 대한 2025년 논문의 새로운 물결과 함께, 적대적 견고성을 공식 하위 분야로 성숙시킨다.</p></li>
</ul>
</section>
</section>
<section id="id18">
<h2>6.0 5부: 데이터 엔진: 자기 지도 학습과 합성 생성<a class="headerlink" href="#id18" title="Link to this heading">#</a></h2>
<section id="ssl">
<h3>6.1 자기 지도 학습(SSL)의 역할<a class="headerlink" href="#ssl" title="Link to this heading">#</a></h3>
<p>오늘 논의된 모든 단일 모델의 기초는 자기 지도 학습(SSL) 패러다임이다. SSL은 모델이 대규모의 <em>레이블 없는</em> 데이터셋(인터넷의 텍스트와 같은)으로부터 의미 있는 표현을 학습할 수 있게 하는 기계학습 기법이다.</p>
<p>SSL에서 암시적 레이블은 <em>데이터 자체에서</em> 생성된다. 예를 들어, GPT와 같은 LLM의 “전제 작업”은 “다음 단어 예측”이다. “레이블”은 단순히 텍스트의 다음 단어이며, 인간 주석이 필요하지 않다. 웹 규모의 레이블 없는 데이터로부터 학습하는 이 능력이 기초 모델의 생성을 가능하게 한다. SSL은 모든 대규모 모델의 사전 훈련 단계를 위한 기초 패러다임으로 남아 있다.</p>
</section>
<section id="llm">
<h3>6.2 2025년 동향: 데이터 생성기로서의 LLM<a class="headerlink" href="#llm" title="Link to this heading">#</a></h3>
<p>SSL이 <em>사전 훈련</em>에 사용되는 동안, <em>미세 조정</em> 및 <em>정렬</em> 단계는 고품질의 레이블된 데이터를 필요로 하며, 이것은 인간이 생성하기에 비용이 많이 들고 시간이 많이 걸린다.</p>
<p>이 병목은 2025년의 가장 중요하고 논쟁적인 동향 중 하나로 이어졌다: LLM 자체를 사용하여 합성 데이터를 생성하는 것. 이것은 ACL 2025의 중심 주제다. 약속은 LLM이 분류, 질문 답변, 지시 튜닝과 같은 작업을 위한 방대하고 다양한 저렴한 데이터셋을 생성할 수 있다는 것이다.</p>
<p>그러나 이 관행은 매우 논쟁적이다. ACL 논문 제출에 대한 2025년 검토는 연구자들이 “LLM을 사용하여 소위 벤치마크 데이터셋을 생성한 다음 이러한 데이터셋이 훈련/미세 조정에 사용될 수 있다고 주장하는” “우려스러운 동향”을 지적했다. 검토자는 이 접근 방식을 비판하며, 이러한 데이터셋이 “알려지지 않은 품질과 대표성”을 가지고 있으며 연구자들이 “엄격하기 때문이 아니라 쉽고 편리하기 때문에 LLM에 의존하여 데이터를 생성하고 있다”고 지적했다.</p>
</section>
<section id="llm-2025">
<h3>6.3 조사 리뷰: “LLM 기반 합성 데이터 생성에 대한 조사” (2025)<a class="headerlink" href="#llm-2025" title="Link to this heading">#</a></h3>
<p>이 2025년 조사 논문은 이 새로운 논쟁적인 하위 분야에 대한 체계적인 개요를 제공한다.</p>
<ul class="simple">
<li><p>주요 기법: 조사는 합성 데이터를 생성하기 위한 주요 방법을 설명한다:</p>
<ol class="arabic simple">
<li><p>프롬프트 기반 생성: 제로샷 또는 퓨샷 프롬프트를 사용하여 LLM에 대상 작업에 대한 새로운 레이블 예제를 생성하도록 지시.</p></li>
<li><p>검색 증강 파이프라인(RAG): 합성 데이터의 사실적 정확성을 개선하기 위해 실제 문서(코퍼스에서 검색됨)에서 LLM의 생성을 근거로 함.</p></li>
<li><p>반복적 자기 개선: 모델의 자체 출력(예: 생성된 코드)을 사용하여 반복적으로 미세 조정하고 개선.</p></li>
</ol>
</li>
<li><p>주요 도전: 조사는 또한 심오한 위험을 강조한다:</p>
<ol class="arabic simple">
<li><p>사실적 부정확성: LLM이 “환각”을 일으키고 사실적으로 잘못된 데이터를 생성하여 데이터셋을 오염시킬 수 있다.</p></li>
<li><p>편향 증폭: 부모 LLM에 존재하는 고유한 사회적 편향(예: 성별, 문화적)이 합성 데이터에 “구워져” 잠재적으로 증폭된다.</p></li>
<li><p>모델 붕괴: 이것은 가장 중요한 장기 위험으로 식별된다. “모델 붕괴” 또는 “모델 악화”는 모델이 <em>다른 모델</em>의 합성 출력에 대해 훈련될 때 발생한다. 이것은 연속적인 세대에 걸쳐 데이터 생태계가 다양성을 잃고, 실제 세계 분포를 “잊어버리고”, 돌이킬 수 없이 품질이 저하되는 “자기 소비 루프”를 만든다.</p></li>
</ol>
</li>
<li><p>완화 전략:</p>
<ul>
<li><p>필터링 및 혼합: 가장 일반적인 완화는 <em>순수하게</em> 합성 데이터에 대해 훈련하지 않는 것이다. 대신, 고품질 합성 데이터는 (더 작은) 실제 세계 데이터 세트와 “혼합”되어 모델을 현실에 “고정”시킨다.</p></li>
<li><p>실행 피드백으로부터의 강화 학습(RLEF): 주요 2025 기법인 RLEF는 <em>코드</em> 생성에 특히 강력하다. 합성 코드는 고유한 속성을 가진다: 외부의 객관적인 “진실”(코드 인터프리터 또는 컴파일러)에 의해 <em>자동으로 검증</em>될 수 있다. 코드를 생성하고, <em>실행하고</em>, 통과/실패 신호를 실행 피드백으로 사용함으로써, 연구자들은 <em>기능적으로 올바른</em> 합성 데이터를 필터링할 수 있다. 이것은 “환각” 루프를 깨는 견고하고 비인간 보상 신호를 제공한다.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id19">
<h3>6.4 자기 소비 루프<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<p>합성 데이터의 부상은 AI의 미래에 대한 근본적인 도전을 제시한다. 분야는 적극적으로 “자기 소비” 데이터 루프를 구축하고 있다.</p>
<p>논리적 체인은 다음과 같다:</p>
<ol class="arabic simple">
<li><p>현대 AI 모델 훈련은 대규모의 고품질 데이터셋을 필요로 한다.</p></li>
<li><p>인간으로부터 이 데이터를 획득하는 것은 주요 병목이다: 느리고, 비용이 많이 들며, 확장하기 어렵다.</p></li>
<li><p>따라서 연구자와 연구소는 점점 더 최고 성능 모델을 사용하여 <em>다음</em> 세대 모델을 훈련하기 위해 <em>합성 데이터</em>를 생성한다.</p></li>
<li><p>이것은 모델이 다른 모델의 출력에 대해 훈련되는 폐쇄적이고 자기회귀적인 루프를 만든다.</p></li>
</ol>
<p>이 문제는 NeurIPS 2025의 전용 워크숍 “합성 데이터 시대의 AI”의 초점이었으며, 이것은 명시적으로 “합성 데이터 훈련으로 인한 AI 모델 악화”를 연구하는 것을 목표로 한다.</p>
<p>이것이 합성 데이터가 쓸모없다는 것을 의미하는 것은 아니다. 대신, 생성 모델이 더 일반적이 됨에 따라, 데이터 큐레이션, 데이터 필터링, “데이터 중심” AI의 중요성이 덜 중요한 것이 아니라 <em>더</em> 중요해지고 있음을 의미한다. RLEF 기법은 코드 인터프리터가 “환각” 루프가 지배하는 것을 방지하는 <em>외부의 객관적 필터</em> 역할을 하기 때문에 성공적인 완화 전략의 강력한 예이다. 자연어에 대한 유사한 자동화된 “진실” 필터를 찾는 것은 여전히 거대한 미해결 도전이다.</p>
</section>
</section>
<section id="id20">
<h2>7.0 6부: 결론 강의: 2026년 및 그 이후의 전선<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>이 강의가 끝나면서, 우리는 가까운 미래를 바라본다. 2025년의 동향은 2026년 및 그 이후를 정의할 미해결 문제와 연구 전선을 직접 가리킨다.</p>
<section id="id21">
<h3>7.1 효율성과 보편성<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<p>효율성에 대한 추진은 단순히 SOTA 모델을 훈련하는 것이 아니다. 그것은 그것들을 <em>배포</em>하는 것이다.</p>
<ul class="simple">
<li><p>온디바이스 AI: 주요 2025-2026 동향은 기초 모델을 <em>온디바이스</em>(예: 휴대폰, 노트북)에서 실행하는 것이다. 이것은 프라이버시(데이터가 장치를 떠나지 않음)와 지연 시간(네트워크 왕복 없음)에 중요하다. 이것은 모델 압축, 양자화, 효율적인 아키텍처 설계의 새로운 기법을 필요로 한다.</p></li>
<li><p>효율적인 코드 생성: 에이전트가 더 많은 코드를 작성함에 따라, 그 코드의 <em>품질</em>이 검토를 받고 있다. ENAMEL과 같은 새로운 2024-2025 벤치마크는 LLM 생성 코드를 <em>기능적 정확성</em>(실행되는가?)뿐만 아니라 <em>알고리즘 효율성</em>(빠르게 실행되는가?)에 대해서도 평가하도록 설계되고 있다. 이것은 추론에 대한 훨씬 더 높은 기준이다.</p></li>
<li><p>하드웨어 공동 설계: 분야는 알고리즘 최적화를 넘어 하드웨어 자체를 살펴보고 있다. 새로운 연구는 LLM 추론의 근본적 병목을 식별하기 위해 “한계 연구”를 수행하고 있으며, 메모리 대역폭과 칩 간 상호 연결에 초점을 맞추고 있다. 이것은 미래 하드웨어가 SSM과 MoE와 같은 새로운 아키텍처를 위해 특별히 구축될 공동 설계의 새로운 시대를 신호한다.</p></li>
</ul>
</section>
<section id="qnlp">
<h3>7.2 양자 도약: QNLP 소개<a class="headerlink" href="#qnlp" title="Link to this heading">#</a></h3>
<p>더 먼 지평선에는 계산 규칙을 근본적으로 다시 쓸 수 있는 패러다임이 있다: 양자 자연어 처리(QNLP).</p>
<ul class="simple">
<li><p>7.2.1 개념: QNLP는 양자 컴퓨팅의 원리를 자연어 처리에 적용하려는 새로운 학제간 분야다. 핵심 아이디어는 언어의 풍부하고 구성적인 구조(예: 문법, 의미론)가 고전 통계로 모델링하기 어렵지만 양자 얽힘, 중첩, 간섭과 같은 양자 역학의 수학으로 자연스럽게 표현될 수 있다는 것이다.</p></li>
<li><p>7.2.2 2025년 상태: 이 분야는 매우 실험적이다.</p>
<ul>
<li><p>도구: Quantinuum의 오픈소스 Python 라이브러리인 lambeq 툴킷은 자연어 문장을 파라미터화된 양자 회로로 변환하여 양자 하드웨어에서 실행할 준비가 되어 있다.</p></li>
<li><p>이론: 2024-2025 조사의 물결이 이론적 현황을 매핑하고, “양자 임베딩”, “양자 주의 메커니즘”, 하이브리드 고전-양자 모델을 설계하고 있다.</p></li>
<li><p>실천: 모든 현재 연구는 분야가 “하드웨어 제한”, 노이즈, 탈코히어런스에 의해 심각하게 제약받고 있음을 쉽게 인정한다. 예를 들어, 2025년 논문은 <em>퓨샷</em> 자연어 추론 작업에서 QNLP 모델을 입증한다. 모든 현재 작업은 “작은 데이터셋”으로 제한된다.</p></li>
</ul>
</li>
<li><p>7.2.3 미래 약속: 아직 초기 단계에 있지만, QNLP는 “양자 이점”을 달성할 장기 <em>잠재력</em>을 가지고 있다—어떤 고전 모델도 할 수 없었던 것보다 더 효율적이거나 정확하게 NLP 작업을 해결하는 것. 가장 유망한 단기 응용은 양자 시뮬레이션이 복잡한 생물학적 “언어”에 적용될 수 있는 생물정보학, 약물 발견, 단백질 구조 예측과 같은 전문 과학 분야에 있다.</p></li>
</ul>
</section>
<section id="id22">
<h3>7.3 최종 요약: 2026년을 위한 미해결 연구 질문<a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<p>이 과정을 마치면서, 핵심 미해결 질문—많은 것이 논문이나 미래 연구 경력의 기초가 될 수 있는—은 더 이상 “스케일링할 수 있는가?”가 아니라 “우리가 무엇을 구축했는가, 그리고 어떻게 제어할 것인가?”다.</p>
<p>2024-2025 현황에 대한 검토를 바탕으로, 2026년을 위한 미해결 연구 질문은 다음과 같다:</p>
<ol class="arabic simple">
<li><p>아키텍처: SSM(Mamba와 같은)과 MoE를 단일 하이브리드 아키텍처로 성공적으로 결합할 수 있는가? 그러한 모델이 컨텍스트 길이에서 선형 시간 스케일링과 대규모 희소 활성화 지식 <em>둘 다</em>를 달성할 수 있는가?</p></li>
<li><p>에이전시: “자율성 vs. 제어” 딜레마를 어떻게 해결할 것인가? 능력 있고 검증 가능하게 안전한 에이전트를 어떻게 구축할 것인가? 그리고 결정적으로, <em>다중 에이전트 시스템</em>에 대한 <em>새로운</em> 정렬 문제를 어떻게 해결할 것인가?</p></li>
<li><p>신뢰성: “안전 세금”을 어떻게 해결할 것인가? 추론 및 기타 능력을 검증 가능하게 보존하는 새로운 정렬 기법(MART와 같은)을 개발할 수 있는가?</p></li>
<li><p>데이터: <em>지속 가능한</em> 데이터 생태계를 어떻게 구축할 것인가? “자기 소비 루프”를 어떻게 깨고 “모델 붕괴”를 방지할 것인가? 자연어에 대해 (코드에 대한 RLEF와 같은) 어떤 다른 객관적 외부 필터를 발견할 수 있는가?</p></li>
<li><p>학제간성: NLP의 미래는 “스케일링을 넘어선다”다. EMNLP 2025의 공식 주제는 “NLP의 학제간 재맥락화”이고, AAAI-26의 주제는 “협력적 다리”다. 가장 중요한 미해결 질문은 이제 NLP와 다른 분야의 교차점에 있다: 과학, 의학, 교육, 사회과학.</p></li>
</ol>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>A Comprehensive Review of Deep Learning: Architectures, Recent Advances, and Applications - MDPI, <a class="reference external" href="https://www.mdpi.com/2078-2489/15/12/755">https://www.mdpi.com/2078-2489/15/12/755</a></p></li>
<li><p>Deep learning for natural language processing: advantages and challenges | National Science Review | Oxford Academic, <a class="reference external" href="https://academic.oup.com/nsr/article/5/1/24/4107792">https://academic.oup.com/nsr/article/5/1/24/4107792</a></p></li>
<li><p>Deep Learning for Natural Language Processing: A Review of Models and Applications, <a class="reference external" href="https://www.researchgate.net/publication/395057230_Deep_Learning_for_Natural_Language_Processing_A_Review_of_Models_and_Applications">https://www.researchgate.net/publication/395057230_Deep_Learning_for_Natural_Language_Processing_A_Review_of_Models_and_Applications</a></p></li>
<li><p>Natural Language Processing (NLP) in Artificial Intelligence - | World Journal of Advanced Research and Reviews, <a class="reference external" href="https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-0275.pdf">https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-0275.pdf</a></p></li>
<li><p>Transformer Architecture Evolution in Large Language Models: A Survey - ResearchGate, <a class="reference external" href="https://www.researchgate.net/publication/394522965_Transformer_Architecture_Evolution_in_Large_Language_Models_A_Survey">https://www.researchgate.net/publication/394522965_Transformer_Architecture_Evolution_in_Large_Language_Models_A_Survey</a></p></li>
<li><p>A Survey of Large Language Models: Evolution, Architectures, Adaptation, Benchmarking, Applications, Challenges, and Societal Implications - MDPI, <a class="reference external" href="https://www.mdpi.com/2079-9292/14/18/3580">https://www.mdpi.com/2079-9292/14/18/3580</a></p></li>
<li><p>A Survey of Large Language Models - arXiv, <a class="reference external" href="https://arxiv.org/html/2303.18223v16">https://arxiv.org/html/2303.18223v16</a></p></li>
<li><p>The 2024 Conference on Empirical Methods in Natural Language …, <a class="reference external" href="https://2024.emnlp.org/">https://2024.emnlp.org/</a></p></li>
<li><p>ACL 2025 Highlights: Direction of NLP &amp; AI | by Megagon Labs, <a class="reference external" href="https://megagonlabs.medium.com/acl-2025-highlights-direction-of-nlp-ai-e9478c0b4ccf">https://megagonlabs.medium.com/acl-2025-highlights-direction-of-nlp-ai-e9478c0b4ccf</a></p></li>
<li><p>Accepted Main Conference Papers - ACL 2025, <a class="reference external" href="https://2025.aclweb.org/program/main_papers/">https://2025.aclweb.org/program/main_papers/</a></p></li>
<li><p>The 2025 Conference on Empirical Methods in Natural Language …, <a class="reference external" href="https://2025.emnlp.org/">https://2025.emnlp.org/</a></p></li>
<li><p>NeurIPS/ICLR/ICML Journal-to-Conference Track, <a class="reference external" href="https://neurips.cc/public/JournalToConference">https://neurips.cc/public/JournalToConference</a></p></li>
<li><p>Workshops - NeurIPS 2025, <a class="reference external" href="https://neurips.cc/virtual/2025/events/workshop">https://neurips.cc/virtual/2025/events/workshop</a></p></li>
<li><p>NeurIPS 2025 Papers, <a class="reference external" href="https://neurips.cc/virtual/2025/papers.html">https://neurips.cc/virtual/2025/papers.html</a></p></li>
<li><p>NeurIPS 2024 Papers, <a class="reference external" href="https://nips.cc/virtual/2024/papers.html">https://nips.cc/virtual/2024/papers.html</a></p></li>
<li><p>Proceedings of the 2025 Conference on Empirical … - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.emnlp-tutorials.pdf">https://aclanthology.org/2025.emnlp-tutorials.pdf</a></p></li>
<li><p>ICML 2024 Papers, <a class="reference external" href="https://icml.cc/virtual/2024/papers.html">https://icml.cc/virtual/2024/papers.html</a></p></li>
<li><p>Top 10 NLP Trends to Watch in 2025 – Future of AI &amp; Language Processing | Shaip, <a class="reference external" href="https://www.shaip.com/blog/nlp-trends-2025/">https://www.shaip.com/blog/nlp-trends-2025/</a></p></li>
<li><p>Natural Language Processing Statistics By Market, Revenue And Trends (2025) - ElectroIQ, <a class="reference external" href="https://electroiq.com/stats/natural-language-processing-statistics/">https://electroiq.com/stats/natural-language-processing-statistics/</a></p></li>
<li><p>Natural language processing (NLP) Decade Long Trends, Analysis and Forecast 2025-2033, <a class="reference external" href="https://www.archivemarketresearch.com/reports/natural-language-processing-nlp-559381">https://www.archivemarketresearch.com/reports/natural-language-processing-nlp-559381</a></p></li>
<li><p>Future of Natural Language Processing: Key Trends in 2025 - IABAC, <a class="reference external" href="https://iabac.org/blog/the-future-of-natural-language-processing">https://iabac.org/blog/the-future-of-natural-language-processing</a></p></li>
<li><p>The Future of Natural Language Processing: Trends to Watch in 2025 and Beyond, <a class="reference external" href="https://www.tekrevol.com/blogs/natural-language-processing-trends/">https://www.tekrevol.com/blogs/natural-language-processing-trends/</a></p></li>
<li><p>Natural language processing models in 2025 | Pre-trained NLP models | NLP solutions for businesses | Lumenalta, <a class="reference external" href="https://lumenalta.com/insights/7-of-the-best-natural-language-processing-models-in-2025">https://lumenalta.com/insights/7-of-the-best-natural-language-processing-models-in-2025</a></p></li>
<li><p>Transformer: A Novel Neural Network Architecture for Language Understanding, <a class="reference external" href="https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/">https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/</a></p></li>
<li><p>Mamba: Linear-Time Sequence Modeling with Selective … - arXiv, <a class="reference external" href="https://arxiv.org/abs/2312.00752">https://arxiv.org/abs/2312.00752</a></p></li>
<li><p>From S4 to Mamba: A Comprehensive Survey on Structured State Space Models - arXiv, <a class="reference external" href="https://arxiv.org/abs/2503.18970">https://arxiv.org/abs/2503.18970</a></p></li>
<li><p>Mamba State-Space Models Are Lyapunov-Stable Learners - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2406.00209">https://arxiv.org/pdf/2406.00209</a></p></li>
<li><p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2312.00752">https://arxiv.org/pdf/2312.00752</a></p></li>
<li><p>Mixture-of-Experts in the Era of LLMs A New Odyssey, <a class="reference external" href="https://icml.cc/media/icml-2024/Slides/35222_1r94S59.pdf">https://icml.cc/media/icml-2024/Slides/35222_1r94S59.pdf</a></p></li>
<li><p>Mixture of Demonstrations for In-Context Learning - NIPS, <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2024/file/a0da098e0031f58269efdcba40eedf47-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2024/file/a0da098e0031f58269efdcba40eedf47-Paper-Conference.pdf</a></p></li>
<li><p>MoME: Mixture of Multimodal Experts for Generalist … - NIPS papers, <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2024/file/4a3a14b9536806a0522930007c5512f7-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2024/file/4a3a14b9536806a0522930007c5512f7-Paper-Conference.pdf</a></p></li>
<li><p>A Perspective on LLM Data Generation with Few-shot Examples: from Intent to Kubernetes Manifest - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.acl-industry.27.pdf">https://aclanthology.org/2025.acl-industry.27.pdf</a></p></li>
<li><p>AI Agents in 2025: Expectations vs. Reality - IBM, <a class="reference external" href="https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality">https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality</a></p></li>
<li><p>5 Cutting-Edge Natural Language Processing Trends Shaping 2026 - KDnuggets, <a class="reference external" href="https://www.kdnuggets.com/5-cutting-edge-natural-language-processing-trends-shaping-2026">https://www.kdnuggets.com/5-cutting-edge-natural-language-processing-trends-shaping-2026</a></p></li>
<li><p>Large Language Models: A Survey - arXiv, <a class="reference external" href="https://arxiv.org/html/2402.06196v3">https://arxiv.org/html/2402.06196v3</a></p></li>
<li><p>[2406.05804] A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning - arXiv, <a class="reference external" href="https://arxiv.org/abs/2406.05804">https://arxiv.org/abs/2406.05804</a></p></li>
<li><p>Plan Then Action: High-Level Planning Guidance Reinforcement Learning for LLM Reasoning - arXiv, <a class="reference external" href="https://arxiv.org/html/2510.01833v1">https://arxiv.org/html/2510.01833v1</a></p></li>
<li><p>Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study - arXiv, <a class="reference external" href="https://arxiv.org/html/2507.23589v1">https://arxiv.org/html/2507.23589v1</a></p></li>
<li><p>Talk: Beyond Scaling: Frontiers of Retrieval-Augmented Language Models, <a class="reference external" href="https://today.wisc.edu/events/view/206147">https://today.wisc.edu/events/view/206147</a></p></li>
<li><p>Beyond Scaling: Frontiers of Retrieval-Augmented Language Models - Harvard SEAS, <a class="reference external" href="https://events.seas.harvard.edu/event/beyond-scaling-frontiers-of-retrieval-augmented-language-models">https://events.seas.harvard.edu/event/beyond-scaling-frontiers-of-retrieval-augmented-language-models</a></p></li>
<li><p>Large Language Model Agent: A Survey on Methodology, Applications and Challenges - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2503.21460">https://arxiv.org/pdf/2503.21460</a></p></li>
<li><p>Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research - arXiv, <a class="reference external" href="https://arxiv.org/html/2506.01839v1">https://arxiv.org/html/2506.01839v1</a></p></li>
<li><p>Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence - arXiv, <a class="reference external" href="https://arxiv.org/html/2503.03800v1">https://arxiv.org/html/2503.03800v1</a></p></li>
<li><p>LLM Multi-Agent Systems: Challenges and Open Problems - arXiv, <a class="reference external" href="https://arxiv.org/html/2402.03578v2">https://arxiv.org/html/2402.03578v2</a></p></li>
<li><p>[2510.05174] Emergent Coordination in Multi-Agent Language Models - arXiv, <a class="reference external" href="https://arxiv.org/abs/2510.05174">https://arxiv.org/abs/2510.05174</a></p></li>
<li><p>[2409.02645] Emergent Language: A Survey and Taxonomy - arXiv, <a class="reference external" href="https://arxiv.org/abs/2409.02645">https://arxiv.org/abs/2409.02645</a></p></li>
<li><p>MAEBE: Multi-Agent Emergent Behavior Framework - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2506.03053">https://arxiv.org/pdf/2506.03053</a></p></li>
<li><p>Agent Laboratory: Using LLM Agents as Research Assistants - arXiv, <a class="reference external" href="https://arxiv.org/abs/2501.04227">https://arxiv.org/abs/2501.04227</a></p></li>
<li><p>A Review of Large Language Models as Autonomous Agents and Tool Users - arXiv, <a class="reference external" href="https://arxiv.org/html/2508.17281v1">https://arxiv.org/html/2508.17281v1</a></p></li>
<li><p>Trending Papers - Hugging Face, <a class="reference external" href="https://huggingface.co/papers/trending">https://huggingface.co/papers/trending</a></p></li>
<li><p>Levels of Autonomy for AI Agents Working Paper - arXiv, <a class="reference external" href="https://arxiv.org/html/2506.12469v1">https://arxiv.org/html/2506.12469v1</a></p></li>
<li><p>Building Effective AI Agents - Anthropic, <a class="reference external" href="https://www.anthropic.com/research/building-effective-agents">https://www.anthropic.com/research/building-effective-agents</a></p></li>
<li><p>Seizing the agentic AI advantage - McKinsey, <a class="reference external" href="https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage">https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage</a></p></li>
<li><p>The State of AI Agent Platforms in 2025: Comparative Analysis - Ionio, <a class="reference external" href="https://www.ionio.ai/blog/the-state-of-ai-agent-platforms-in-2025-comparative-analysis">https://www.ionio.ai/blog/the-state-of-ai-agent-platforms-in-2025-comparative-analysis</a></p></li>
<li><p>Evaluation and Benchmarking of LLM Agents: A Survey - arXiv, <a class="reference external" href="https://arxiv.org/html/2507.21504v1">https://arxiv.org/html/2507.21504v1</a></p></li>
<li><p>[2503.22458] Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey - arXiv, <a class="reference external" href="https://arxiv.org/abs/2503.22458">https://arxiv.org/abs/2503.22458</a></p></li>
<li><p>The 2025 Conference on Empirical Methods in Natural Language Processing, <a class="reference external" href="https://aclanthology.org/events/emnlp-2025/">https://aclanthology.org/events/emnlp-2025/</a></p></li>
<li><p>[2408.01319] A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks - arXiv, <a class="reference external" href="https://arxiv.org/abs/2408.01319">https://arxiv.org/abs/2408.01319</a></p></li>
<li><p>survey on multimodal large language models | National Science Review - Oxford Academic, <a class="reference external" href="https://academic.oup.com/nsr/article/11/12/nwae403/7896414">https://academic.oup.com/nsr/article/11/12/nwae403/7896414</a></p></li>
<li><p>ICML 2024 NExT-GPT: Any-to-Any Multimodal LLM Oral - ICML 2025, <a class="reference external" href="https://icml.cc/virtual/2024/oral/35529">https://icml.cc/virtual/2024/oral/35529</a></p></li>
<li><p>Apple Machine Learning Research at NeurIPS 2024, <a class="reference external" href="https://machinelearning.apple.com/research/neurips-2024">https://machinelearning.apple.com/research/neurips-2024</a></p></li>
<li><p>Code and models for ICML 2024 paper, NExT-GPT: Any-to-Any Multimodal Large Language Model - GitHub, <a class="github reference external" href="https://github.com/NExT-GPT/NExT-GPT">NExT-GPT/NExT-GPT</a></p></li>
<li><p>NExT-GPT: Any-to-Any Multimodal LLM - GitHub, <a class="reference external" href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/wu24e/wu24e.pdf">https://raw.githubusercontent.com/mlresearch/v235/main/assets/wu24e/wu24e.pdf</a></p></li>
<li><p>NExT-GPT: Any-to-Any Multimodal LLM - Proceedings of Machine Learning Research, <a class="reference external" href="https://proceedings.mlr.press/v235/wu24e.html">https://proceedings.mlr.press/v235/wu24e.html</a></p></li>
<li><p>[2309.05519] NExT-GPT: Any-to-Any Multimodal LLM - arXiv, <a class="reference external" href="https://arxiv.org/abs/2309.05519">https://arxiv.org/abs/2309.05519</a></p></li>
<li><p>A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges - arXiv, <a class="reference external" href="https://arxiv.org/html/2501.02189v5">https://arxiv.org/html/2501.02189v5</a></p></li>
<li><p>Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions - arXiv, <a class="reference external" href="https://arxiv.org/html/2404.07214v3">https://arxiv.org/html/2404.07214v3</a></p></li>
<li><p>A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges - arXiv, <a class="reference external" href="https://arxiv.org/html/2501.02189v6">https://arxiv.org/html/2501.02189v6</a></p></li>
<li><p>Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM, <a class="reference external" href="https://arxiv.org/html/2505.18110v2">https://arxiv.org/html/2505.18110v2</a></p></li>
<li><p>Findings of the Association for Computational Linguistics: EMNLP 2025 - ACL Anthology, <a class="reference external" href="https://aclanthology.org/volumes/2025.findings-emnlp/">https://aclanthology.org/volumes/2025.findings-emnlp/</a></p></li>
<li><p>Grounded-VideoLLM: Sharpening Fine-grained Temporal …, <a class="reference external" href="https://aclanthology.org/2025.findings-emnlp.50/">https://aclanthology.org/2025.findings-emnlp.50/</a></p></li>
<li><p>Findings of the Association for Computational Linguistics: EMNLP 2025 - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.findings-emnlp.0.pdf">https://aclanthology.org/2025.findings-emnlp.0.pdf</a></p></li>
<li><p>Zhiyang Xu - OpenReview, <a class="reference external" href="https://openreview.net/profile?id=~Zhiyang_Xu1">https://openreview.net/profile?id=~Zhiyang_Xu1</a></p></li>
<li><p>Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.findings-emnlp.50.pdf">https://aclanthology.org/2025.findings-emnlp.50.pdf</a></p></li>
<li><p>Parrot or pilot? how llms ‘think’ when the geometry clicks | by BuildShift - Level Up Coding, <a class="reference external" href="https://levelup.gitconnected.com/parrot-or-pilot-how-llms-think-when-the-geometry-clicks-1ca58d307b8e">https://levelup.gitconnected.com/parrot-or-pilot-how-llms-think-when-the-geometry-clicks-1ca58d307b8e</a></p></li>
<li><p>Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning - arXiv, <a class="reference external" href="https://arxiv.org/html/2409.17270v2">https://arxiv.org/html/2409.17270v2</a></p></li>
<li><p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models - arXiv, <a class="reference external" href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a></p></li>
<li><p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models, <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf</a></p></li>
<li><p>What is Tree Of Thoughts Prompting? - IBM, <a class="reference external" href="https://www.ibm.com/think/topics/tree-of-thoughts">https://www.ibm.com/think/topics/tree-of-thoughts</a></p></li>
<li><p>From Chains to Trees: Revolutionizing AI Reasoning with Tree-of-Thought Prompting” | by Jacky Hsiao | Medium, <a class="reference external" href="https://medium.com/&#64;jacky0305/from-chains-to-trees-revolutionizing-ai-reasoning-with-tree-of-thought-prompting-ff0afb566dce">https://medium.com/&#64;jacky0305/from-chains-to-trees-revolutionizing-ai-reasoning-with-tree-of-thought-prompting-ff0afb566dce</a></p></li>
<li><p>Tree of Thoughts (ToT) - Prompt Engineering Guide, <a class="reference external" href="https://www.promptingguide.ai/techniques/tot">https://www.promptingguide.ai/techniques/tot</a></p></li>
<li><p>ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving - arXiv, <a class="reference external" href="https://arxiv.org/html/2505.12717v1">https://arxiv.org/html/2505.12717v1</a></p></li>
<li><p>Top AI Research Papers of 2025: From Chain-of-Thought Flaws to Fine-Tuned AI Agents, <a class="reference external" href="https://www.aryaxai.com/article/top-ai-research-papers-of-2025-from-chain-of-thought-flaws-to-fine-tuned-ai-agents">https://www.aryaxai.com/article/top-ai-research-papers-of-2025-from-chain-of-thought-flaws-to-fine-tuned-ai-agents</a></p></li>
<li><p>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity - Apple Machine Learning Research, <a class="reference external" href="https://machinelearning.apple.com/research/illusion-of-thinking">https://machinelearning.apple.com/research/illusion-of-thinking</a></p></li>
<li><p>New Research Challenges Apple’s “AI Can’t Really Reason” Study - Finds Mixed Results : r/OpenAI - Reddit, <a class="reference external" href="https://www.reddit.com/r/OpenAI/comments/1lqjw0n/new_research_challenges_apples_ai_cant_really/">https://www.reddit.com/r/OpenAI/comments/1lqjw0n/new_research_challenges_apples_ai_cant_really/</a></p></li>
<li><p>Technical Report: The Decreasing Value of Chain of Thought in …, <a class="reference external" href="https://gail.wharton.upenn.edu/research-and-insights/tech-report-chain-of-thought/">https://gail.wharton.upenn.edu/research-and-insights/tech-report-chain-of-thought/</a></p></li>
<li><p>Chain of Thought Prompting: Enhance AI Reasoning &amp; LLMs - Future AGI, <a class="reference external" href="https://futureagi.com/blogs/chain-of-thought-prompting-ai-2025">https://futureagi.com/blogs/chain-of-thought-prompting-ai-2025</a></p></li>
<li><p>Learning to Plan &amp; Reason for Evaluation with Thinking-LLM-as-a-Judge - arXiv, <a class="reference external" href="https://arxiv.org/html/2501.18099v2">https://arxiv.org/html/2501.18099v2</a></p></li>
<li><p>ryokamoi/llm-self-correction-papers: List of papers on Self-Correction of LLMs. - GitHub, <a class="github reference external" href="https://github.com/ryokamoi/llm-self-correction-papers">ryokamoi/llm-self-correction-papers</a></p></li>
<li><p>Learning to Plan &amp; Reason for Evaluation with Thinking-LLM … - arXiv, <a class="reference external" href="https://arxiv.org/abs/2501.18099">https://arxiv.org/abs/2501.18099</a></p></li>
<li><p>NeurIPS 2024 Spotlight Posters, <a class="reference external" href="https://neurips.cc/virtual/2024/events/spotlight-posters-2024">https://neurips.cc/virtual/2024/events/spotlight-posters-2024</a></p></li>
<li><p>Datasets Benchmarks 2024 - NeurIPS 2025, <a class="reference external" href="https://neurips.cc/virtual/2024/events/datasets-benchmarks-2024">https://neurips.cc/virtual/2024/events/datasets-benchmarks-2024</a></p></li>
<li><p>A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications - arXiv, <a class="reference external" href="https://arxiv.org/html/2410.15595v3">https://arxiv.org/html/2410.15595v3</a></p></li>
<li><p>Safety Alignment Makes Your Large Reasoning Models Less Reasonable - arXiv, <a class="reference external" href="https://arxiv.org/html/2503.00555v1">https://arxiv.org/html/2503.00555v1</a></p></li>
<li><p>Safety Tax: Safety Alignment Makes Your Large Reasoning … - arXiv, <a class="reference external" href="https://arxiv.org/abs/2503.00555">https://arxiv.org/abs/2503.00555</a></p></li>
<li><p>[2507.19672] Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges - arXiv, <a class="reference external" href="https://arxiv.org/abs/2507.19672">https://arxiv.org/abs/2507.19672</a></p></li>
<li><p>NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment, <a class="reference external" href="https://arxiv.org/html/2505.22327v1">https://arxiv.org/html/2505.22327v1</a></p></li>
<li><p>Security Concerns for Large Language Models: A Survey - arXiv, <a class="reference external" href="https://arxiv.org/html/2505.18889v1">https://arxiv.org/html/2505.18889v1</a></p></li>
<li><p>Red Teaming AI Red Teaming - arXiv, <a class="reference external" href="https://arxiv.org/html/2507.05538v1">https://arxiv.org/html/2507.05538v1</a></p></li>
<li><p>MART: Improving LLM Safety with Multi-round Automatic Red …, <a class="reference external" href="https://aclanthology.org/2024.naacl-long.107/">https://aclanthology.org/2024.naacl-long.107/</a></p></li>
<li><p>From Promise to Peril: Rethinking Cybersecurity Red and Blue Teaming in the Age of LLMs, <a class="reference external" href="https://arxiv.org/html/2506.13434v1">https://arxiv.org/html/2506.13434v1</a></p></li>
<li><p>What Can Generative AI Red-Teaming Learn from Cyber Red-Teaming? - Software Engineering Institute, <a class="reference external" href="https://www.sei.cmu.edu/documents/6301/What_Can_Generative_AI_Red-Teaming_Learn_from_Cyber_Red-Teaming.pdf">https://www.sei.cmu.edu/documents/6301/What_Can_Generative_AI_Red-Teaming_Learn_from_Cyber_Red-Teaming.pdf</a></p></li>
<li><p>From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.findings-emnlp.1244.pdf">https://aclanthology.org/2025.findings-emnlp.1244.pdf</a></p></li>
<li><p>From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.findings-emnlp.1244/">https://aclanthology.org/2025.findings-emnlp.1244/</a></p></li>
<li><p>On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks - arXiv, <a class="reference external" href="https://arxiv.org/html/2507.06489v1">https://arxiv.org/html/2507.06489v1</a></p></li>
<li><p>Libr-AI/OpenRedTeaming: Papers about red teaming LLMs and Multimodal models., <a class="github reference external" href="https://github.com/Libr-AI/OpenRedTeaming">Libr-AI/OpenRedTeaming</a></p></li>
<li><p>What Is Self-Supervised Learning? - IBM, <a class="reference external" href="https://www.ibm.com/think/topics/self-supervised-learning">https://www.ibm.com/think/topics/self-supervised-learning</a></p></li>
<li><p>Consequential Advancements of Self-Supervised Learning (SSL) in Deep Learning Contexts - MDPI, <a class="reference external" href="https://www.mdpi.com/2227-7390/12/5/758">https://www.mdpi.com/2227-7390/12/5/758</a></p></li>
<li><p>5th Workshop on Self-Supervised Learning: Theory and Practice - NeurIPS 2025, <a class="reference external" href="https://neurips.cc/virtual/2024/workshop/84703">https://neurips.cc/virtual/2024/workshop/84703</a></p></li>
<li><p>Synthetic Data in the Era of LLMs, <a class="reference external" href="https://synth-data-acl.github.io/">https://synth-data-acl.github.io/</a></p></li>
<li><p>Synthetic Data Generation Using Large Language Models: Advances in Text and Code - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2503.14023">https://arxiv.org/pdf/2503.14023</a></p></li>
<li><p>[D] Reviewed several ACL papers on data resources and feel that LLMs are undermining this field : r/MachineLearning - Reddit, <a class="reference external" href="https://www.reddit.com/r/MachineLearning/comments/1jihs98/d_reviewed_several_acl_papers_on_data_resources/">https://www.reddit.com/r/MachineLearning/comments/1jihs98/d_reviewed_several_acl_papers_on_data_resources/</a></p></li>
<li><p>Synthetic Data Generation Using Large Language Models … - arXiv, <a class="reference external" href="https://arxiv.org/abs/2503.14023">https://arxiv.org/abs/2503.14023</a></p></li>
<li><p>Synthetic Data Generation Using Large Language Models: Advances in Text and Code, <a class="reference external" href="https://arxiv.org/html/2503.14023v2">https://arxiv.org/html/2503.14023v2</a></p></li>
<li><p>Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls - arXiv, <a class="reference external" href="https://arxiv.org/html/2510.01631v1">https://arxiv.org/html/2510.01631v1</a></p></li>
<li><p>FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs - arXiv, <a class="reference external" href="https://arxiv.org/html/2507.15839v1">https://arxiv.org/html/2507.15839v1</a></p></li>
<li><p>NeurIPS 2025 Workshop on AI in the Synthetic Data Age: Challenges and Solutions - DIGITAL SIGNAL PROCESSING AT RICE UNIVERSITY, <a class="reference external" href="https://dsp.rice.edu/neurips-2025-workshop-on-ai-in-the-synthetic-data-age-challenges-and-solutions/">https://dsp.rice.edu/neurips-2025-workshop-on-ai-in-the-synthetic-data-age-challenges-and-solutions/</a></p></li>
<li><p>Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use - arXiv, <a class="reference external" href="https://arxiv.org/html/2504.04736v1">https://arxiv.org/html/2504.04736v1</a></p></li>
<li><p>NeurIPS 2025 Papers with Code &amp; Data, <a class="reference external" href="https://www.paperdigest.org/2025/11/neurips-2025-papers-with-code-data/">https://www.paperdigest.org/2025/11/neurips-2025-papers-with-code-data/</a></p></li>
<li><p>Updates to Apple’s On-Device and Server Foundation Language Models, <a class="reference external" href="https://machinelearning.apple.com/research/apple-foundation-models-2025-updates">https://machinelearning.apple.com/research/apple-foundation-models-2025-updates</a></p></li>
<li><p>Are We There Yet? A Measurement Study of Efficiency for LLM Applications on Mobile Devices - arXiv, <a class="reference external" href="https://arxiv.org/html/2504.00002v1">https://arxiv.org/html/2504.00002v1</a></p></li>
<li><p>How efficient is LLM-generated code? A rigorous &amp; high-standard benchmark - arXiv, <a class="reference external" href="https://arxiv.org/html/2406.06647v4">https://arxiv.org/html/2406.06647v4</a></p></li>
<li><p>Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need - arXiv, <a class="reference external" href="https://arxiv.org/html/2507.14397v1">https://arxiv.org/html/2507.14397v1</a></p></li>
<li><p>[2505.13840] EfficientLLM: Efficiency in Large Language Models - arXiv, <a class="reference external" href="https://arxiv.org/abs/2505.13840">https://arxiv.org/abs/2505.13840</a></p></li>
<li><p>Natural Language Processing in 2025: Technologies, Trends &amp; Business Impact - Aezion, <a class="reference external" href="https://www.aezion.com/blogs/natural-language-processing/">https://www.aezion.com/blogs/natural-language-processing/</a></p></li>
<li><p>Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications - arXiv, <a class="reference external" href="https://arxiv.org/html/2504.09909v2">https://arxiv.org/html/2504.09909v2</a></p></li>
<li><p>[2504.09909] Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications - arXiv, <a class="reference external" href="https://arxiv.org/abs/2504.09909">https://arxiv.org/abs/2504.09909</a></p></li>
<li><p>Quantum Natural Language Processing: Challenges and Opportunities - MDPI, <a class="reference external" href="https://www.mdpi.com/2076-3417/12/11/5651">https://www.mdpi.com/2076-3417/12/11/5651</a></p></li>
<li><p>Quantum natural language processing and its applications in bioinformatics: a comprehensive review of methodologies, concepts, and future directions - Frontiers, <a class="reference external" href="https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1464122/full">https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1464122/full</a></p></li>
<li><p>Design and analysis of quantum machine learning: a survey - Taylor &amp; Francis Online, <a class="reference external" href="https://www.tandfonline.com/doi/full/10.1080/09540091.2024.2312121">https://www.tandfonline.com/doi/full/10.1080/09540091.2024.2312121</a></p></li>
<li><p>Comparative Study of Traditional Machine Learning and Quantum Computing in Natural Language Processing: A Case Study on Sentiment Analysis - IEEE Xplore, <a class="reference external" href="https://ieeexplore.ieee.org/document/10791272/">https://ieeexplore.ieee.org/document/10791272/</a></p></li>
<li><p>Quantinuum Announces Updates to Quantum Natural Language Processing Toolkit λambeq, Enhancing Accessibility, <a class="reference external" href="https://www.quantinuum.com/press-releases/quantinuum-announces-updates-to-quantum-natural-language-processing-toolkit-lambeq-enhancing-accessibility">https://www.quantinuum.com/press-releases/quantinuum-announces-updates-to-quantum-natural-language-processing-toolkit-lambeq-enhancing-accessibility</a></p></li>
<li><p>Natural Language, AI, and Quantum Computing in 2024 - arXiv, <a class="reference external" href="https://arxiv.org/html/2403.19758v1">https://arxiv.org/html/2403.19758v1</a></p></li>
<li><p>A Survey on Quantum Machine Learning: Basics, Current Trends, Challenges, Opportunities, and the Road Ahead - arXiv, <a class="reference external" href="https://arxiv.org/html/2310.10315v3">https://arxiv.org/html/2310.10315v3</a></p></li>
<li><p>A Survey on Quantum Machine Learning: Basics, Current Trends, Challenges, Opportunities, and the Road Ahead - arXiv, <a class="reference external" href="https://arxiv.org/html/2310.10315v4">https://arxiv.org/html/2310.10315v4</a></p></li>
<li><p>[2510.15972] Quantum NLP models on Natural Language Inference - arXiv, <a class="reference external" href="https://www.arxiv.org/abs/2510.15972">https://www.arxiv.org/abs/2510.15972</a></p></li>
<li><p>Main Technical Track: Call for Papers - AAAI - The Association for the Advancement of Artificial Intelligence, <a class="reference external" href="https://aaai.org/conference/aaai/aaai-26/main-technical-track-call/">https://aaai.org/conference/aaai/aaai-26/main-technical-track-call/</a></p></li>
<li><p>Natural Language Processing Projects 2026-27, <a class="reference external" href="https://www.kcl.ac.uk/nmes/assets/informatics-pdfs-2026-27/natural-language-processing-projects-2026-27.pdf">https://www.kcl.ac.uk/nmes/assets/informatics-pdfs-2026-27/natural-language-processing-projects-2026-27.pdf</a></p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week14"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="ko"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week13/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 13: 온톨로지와 AI</p>
      </div>
    </a>
    <a class="right-next"
       href="../workshops/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">LLM From Scratch 워크숍</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.0 서론: 스케일링 이후 시대</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1 배경 설정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.2 2025년 연구 현황</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">1.3 시장 및 산업 맥락 (2025)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.0 1부: 아키텍처 혁명 (트랜스포머를 넘어서)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">2.1 문제: 트랜스포머의 병목</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssm-mamba">2.2 심화: 상태 공간 모델(SSM)과 Mamba의 부상</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">2.2.1 개념적 개요</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-gu-dao-2023-2024">2.2.2 주요 논문 리뷰: “Mamba: 선택적 상태 공간을 사용한 선형 시간 시퀀스 모델링” (Gu &amp; Dao, 2023/2024)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe">2.3 심화: 스케일링 패러다임으로서의 전문가 혼합(MoE)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">2.3.1 개념적 개요</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mome-neurips-2024">2.3.2 주요 논문 리뷰: “MoME: 범용 멀티모달 대규모 언어 모델을 위한 멀티모달 전문가 혼합” (NeurIPS 2024)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">2.4 분기된 아키텍처 미래</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vs-ssm-mamba-vs-moe">2.5 아키텍처 비교: 트랜스포머 vs. SSM (Mamba) vs. MoE</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">3.0 2부: 새로운 능력 전선: 에이전트 AI</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.1 생성 모델에서 자율 에이전트로</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mas">3.2 심화: 다중 에이전트 시스템(MAS)과 창발적 행동</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-laboratory-llm-schmidgall-et-al-2025">3.3 주요 논문 리뷰: “Agent Laboratory: 연구 보조원으로서의 LLM 에이전트 사용” (Schmidgall et al., 2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-vs">3.4 2025년 에이전트 AI 논쟁: 자율성 vs. 제어</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3.5 평가 위기: 에이전트를 어떻게 벤치마크할 것인가?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">4.0 3부: 새로운 영역: 진정한 멀티모달리티</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#any-to-any-mllm">4.1 융합 인코더를 넘어서: “Any-to-Any” MLLM으로</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-gpt-any-to-any-llm-wu-et-al-icml-2024">4.2 주요 논문 리뷰: “NExT-GPT: Any-to-Any 멀티모달 LLM” (Wu et al., ICML 2024)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">4.3 심화: 비디오-언어 전선</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grounded-videollm-wang-et-al-emnlp-2025">4.4 주요 논문 리뷰: “Grounded-VideoLLM: 비디오 대규모 언어 모델에서 세밀한 시간적 그라운딩 강화” (Wang et al., EMNLP 2025)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">5.0 4부: 큰 논쟁: 추론, 신뢰성, 안전</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">5.1 추론 논쟁 (2025): 앵무새인가 사고자인가?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-meincke-et-al-2025">5.2 주요 보고서 리뷰: “프롬프팅에서 Chain of Thought의 감소하는 가치” (Meincke et al., 2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-as-a-judge">5.3 감독 자동화: LLM-as-a-Judge</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evalplanner-thinking-llm-as-a-judge-saha-et-al-2025">5.4 주요 논문 리뷰: “EvalPlanner: Thinking-LLM-as-a-Judge를 위한 선호도 최적화 알고리즘” (Saha et al., 2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vs">5.5 정렬 트레이드오프: 안전 vs. 능력</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">5.6 주요 논문 리뷰: “안전 세금: 안전 정렬이 대규모 추론 모델을 덜 합리적으로 만든다” (2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">5.7 사전 방어: 레드 팀의 공식화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mart-llm-zhu-et-al-naacl-2024">5.8 주요 논문 리뷰: “MART: 다중 라운드 자동 레드 팀으로 LLM 안전 개선” (Zhu et al., NAACL 2024)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">6.0 5부: 데이터 엔진: 자기 지도 학습과 합성 생성</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssl">6.1 자기 지도 학습(SSL)의 역할</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm">6.2 2025년 동향: 데이터 생성기로서의 LLM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-2025">6.3 조사 리뷰: “LLM 기반 합성 데이터 생성에 대한 조사” (2025)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">6.4 자기 소비 루프</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">7.0 6부: 결론 강의: 2026년 및 그 이후의 전선</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">7.1 효율성과 보편성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qnlp">7.2 양자 도약: QNLP 소개</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">7.3 최종 요약: 2026년을 위한 미해결 연구 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
