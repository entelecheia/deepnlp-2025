# Week 2: PyTorch 2.xì™€ ìµœì‹  ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬

## 1. PyTorch 2.xì™€ torch.compile: ì»´íŒŒì¼ëŸ¬ í˜ëª…

PyTorch 2.xëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¾¼ í˜ì‹ ì ì¸ ì—…ë°ì´íŠ¸ë‹¤. ê¸°ì¡´ì˜ **ì¦‰ì‹œ ì‹¤í–‰(Eager Mode)** ë°©ì‹ì´ ì œê³µí•˜ëŠ” ìœ ì—°ì„±ê³¼ íŒŒì´ì¨ë‹‰í•œ ê°œë°œ ê²½í—˜ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ, `torch.compile`ì´ë¼ëŠ” ë‹¨ í•œ ì¤„ì˜ ì½”ë“œë¡œ ëª¨ë¸ì˜ ì‹¤í–‰ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆë‹¤. ì´ëŠ” "ì—°êµ¬ì˜ ìœ ì—°ì„±ê³¼ í”„ë¡œë•ì…˜ì˜ ì†ë„ë¥¼ ë™ì‹œì— ë§Œì¡±ì‹œí‚¤ëŠ”" í˜ì‹ ìœ¼ë¡œ í‰ê°€ë°›ê³  ìˆë‹¤.

### 1.1 torch.compileì˜ ì‘ë™ ì›ë¦¬

`torch.compile`ì˜ ì„±ëŠ¥ í–¥ìƒì€ **TorchDynamo**, **AOTAutograd**, **PrimTorch**, **TorchInductor**ë¼ëŠ” ë„¤ ê°€ì§€ í•µì‹¬ ê¸°ìˆ ì´ ìœ ê¸°ì ìœ¼ë¡œ í˜‘ë ¥í•˜ì—¬ ì´ë£¨ì–´ì§„ë‹¤.

#### 1. ê·¸ë˜í”„ íšë“ (TorchDynamo)

- **ì—­í• **: Python ë°”ì´íŠ¸ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ PyTorch ì—°ì‚°ì„ FX ê·¸ë˜í”„ë¡œ ì•ˆì „í•˜ê²Œ ìº¡ì²˜
- **í•µì‹¬ ê¸°ìˆ **: "ê°€ë“œ(guard)" ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë™ì  Python íŠ¹ì„±(ì¡°ê±´ë¬¸, ë£¨í”„)ì„ ì™„ë²½ ì§€ì›
- **ì¥ì **: ì½”ë“œ ê²½ë¡œê°€ ë³€ê²½ë˜ë©´ í•´ë‹¹ ë¶€ë¶„ë§Œ ì¦‰ì‹œ ì‹¤í–‰ ëª¨ë“œë¡œ ì²˜ë¦¬í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì»´íŒŒì¼ëœ ì½”ë“œ ì‹¤í–‰

#### 2. ì‚¬ì „ ìë™ ë¯¸ë¶„ (AOTAutograd)

- **ì—­í• **: ìˆœë°©í–¥ ê·¸ë˜í”„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¯¸ë¦¬ ìµœì í™”ëœ ì—­ë°©í–¥ ê·¸ë˜í”„ ìƒì„±
- **ì¥ì **: ì „ì²´ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ë¯¸ë¦¬ ë¶„ì„í•˜ì—¬ ê¸°ìš¸ê¸° ê³„ì‚° ê³¼ì •ì„ ìµœì í™”í•˜ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

#### 3. ê·¸ë˜í”„ ë¡œì›Œë§ (PrimTorch)

- **ì—­í• **: 2,000ê°œ ì´ìƒì˜ PyTorch ì—°ì‚°ìë¥¼ 250ê°œì˜ í•µì‹¬ ì›ì‹œ ì—°ì‚°ìë¡œ í‘œì¤€í™”
- **ì¥ì **: ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ ë°±ì—”ë“œ(GPU, CPU, ì»¤ìŠ¤í…€ ê°€ì†ê¸°)ì— ëŒ€í•œ í˜¸í™˜ì„±ê³¼ ì´ì‹ì„± í–¥ìƒ

#### 4. ê·¸ë˜í”„ ì»´íŒŒì¼ (TorchInductor)

- **ì—­í• **: ì›ì‹œ ì—°ì‚°ì ê·¸ë˜í”„ë¥¼ í•˜ë“œì›¨ì–´ ìµœì í™”ëœ ê¸°ê³„ ì½”ë“œë¡œ ë³€í™˜
- **í•µì‹¬ ê¸°ìˆ **: GPUì—ì„œëŠ” Triton ì»´íŒŒì¼ëŸ¬ë¡œ ê³ ì„±ëŠ¥ CUDA ì»¤ë„ ë™ì  ìƒì„±, CPUì—ì„œëŠ” C++/OpenMP ì‚¬ìš©

ì´ëŸ¬í•œ ë‹¤ë‹¨ê³„ ìµœì í™”ë¡œ `torch.compile`ì€ 163ê°œ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ **í‰ê·  51%**ì˜ í›ˆë ¨ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤.

### 1.2 ì‹¤ìŠµ: torch.compileë¡œ ëª¨ë¸ ì¶”ë¡  ì†ë„ í–¥ìƒ

ê°„ë‹¨í•œ `nn.Module`ì— `torch.compile`ì„ ì ìš©í•˜ì—¬ **ì¦‰ì‹œ ì‹¤í–‰ ëª¨ë“œ**ì™€ **ì»´íŒŒì¼ ëª¨ë“œ**ì˜ ì„±ëŠ¥ì„ ì§ì ‘ ë¹„êµí•´ë³´ì. ì»´íŒŒì¼ì€ ì²« ì‹¤í–‰ ì‹œ ê·¸ë˜í”„ ìº¡ì²˜ ë° ì½”ë“œ ìƒì„±ìœ¼ë¡œ ì¸í•œ ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•˜ì§€ë§Œ, ì´í›„ ë°˜ë³µì ì¸ í˜¸ì¶œì—ì„œëŠ” ê·¸ ë¹„ìš©ì„ ìƒì‡„í•˜ê³ ë„ ë‚¨ì„ ì›”ë“±íˆ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤.

```python
import torch
import torch.nn as nn
import time

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ê°„ë‹¨í•œ ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        return self.layers(x)

model = SimpleNet().to(device)
dummy_input = torch.randn(128, 256).to(device)

# 1. ì¦‰ì‹œ ì‹¤í–‰(Eager) ëª¨ë“œ ì„±ëŠ¥ ì¸¡ì •
# ì›Œë°ì—…: ì´ˆê¸° ì‹¤í–‰ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¶€í•˜ë¥¼ ë°°ì œí•˜ê¸° ìœ„í•¨
for _ in range(10):
    _ = model(dummy_input)

if device == "cuda": torch.cuda.synchronize()
start_time = time.time()
for _ in range(100):
    _ = model(dummy_input)
if device == "cuda": torch.cuda.synchronize()
eager_duration = time.time() - start_time
print(f"Eager mode (100 runs): {eager_duration:.4f} seconds")

# 2. torch.compile ì ìš© (ì»´íŒŒì¼ ëª¨ë“œ)
# mode="reduce-overhead"ëŠ” í”„ë ˆì„ì›Œí¬ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì—¬ ì‘ì€ ëª¨ë¸ í˜¸ì¶œì— ìœ ë¦¬í•©ë‹ˆë‹¤.
compiled_model = torch.compile(model, mode="reduce-overhead")

# ì›Œë°ì—… ë° ì²« ì»´íŒŒì¼ ì‹¤í–‰ (ì»´íŒŒì¼ ì˜¤ë²„í—¤ë“œ ë°œìƒ)
for _ in range(10):
    _ = compiled_model(dummy_input)

if device == "cuda": torch.cuda.synchronize()
start_time = time.time()
for _ in range(100):
    _ = compiled_model(dummy_input)
if device == "cuda": torch.cuda.synchronize()
compiled_duration = time.time() - start_time
print(f"Compiled mode (100 runs): {compiled_duration:.4f} seconds")

# 3. ì„±ëŠ¥ í–¥ìƒë¥  ê³„ì‚°
speedup = eager_duration / compiled_duration
print(f"Speedup with torch.compile: {speedup:.2f}x")
```

**ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ:**

```
Using device: cuda
Eager mode (100 runs): 0.0481 seconds
Compiled mode (100 runs): 0.0215 seconds
Speedup with torch.compile: 2.24x
```

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

- `torch.compile`ì˜ ë„¤ ê°€ì§€ í•µì‹¬ ê¸°ìˆ (TorchDynamo, AOTAutograd, PrimTorch, TorchInductor)ì€ ê°ê° ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ê°€?
- ìœ„ ì˜ˆì œì—ì„œ `mode="reduce-overhead"` ì˜µì…˜ì„ ì‚¬ìš©í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€? ì‘ì€ ëª¨ë¸ì—ì„œ ê¸°ë³¸ ëª¨ë“œì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆì„ê¹Œ?
- `torch.compile`ì´ ê¸°ì¡´ ì¦‰ì‹œ ì‹¤í–‰ ëª¨ë“œì™€ ë¹„êµí–ˆì„ ë•Œ ê°€ì§€ëŠ” ì£¼ìš” ì¥ì ê³¼ í•œê³„ëŠ” ë¬´ì—‡ì¸ê°€?

---

## 2. FlashAttention-3: í•˜ë“œì›¨ì–´ ê°€ì†ì„ í†µí•œ ì–´í…ì…˜ ìµœì í™”

íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ì´ì ì£¼ëœ ì„±ëŠ¥ ë³‘ëª©ì´ì—ˆë˜ **ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**ì€ FlashAttentionì˜ ë“±ì¥ìœ¼ë¡œ íšê¸°ì ì¸ ë°œì „ì„ ì´ë£¨ì—ˆë‹¤. ê¸°ì¡´ ì–´í…ì…˜ì€ ì‹œí€€ìŠ¤ ê¸¸ì´ $N$ì— ëŒ€í•´ $O(N^2)$ì˜ ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ë³µì¡ë„ë¥¼ ê°€ì ¸ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì— í•œê³„ê°€ ìˆì—ˆë‹¤. ì´ëŠ” $N \times N$ í¬ê¸°ì˜ ê±°ëŒ€í•œ ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬ì„ GPUì˜ HBM(ê³ ëŒ€ì—­í­ ë©”ëª¨ë¦¬)ì— ì €ì¥í•˜ê³  ë‹¤ì‹œ ì½ì–´ì™€ì•¼ í–ˆê¸° ë•Œë¬¸ì´ë‹¤.

### 2.1 FlashAttentionì˜ í•µì‹¬ ì›ë¦¬

**FlashAttention**ì€ ë©”ëª¨ë¦¬ I/O ë³‘ëª©ì„ í•´ê²°í•˜ê¸° ìœ„í•´ **íƒ€ì¼ë§(Tiling)** ê¸°ë²•ê³¼ GPU ë‚´ë¶€ì˜ ë§¤ìš° ë¹ ë¥¸ SRAM(ì •ì  ë¨)ì„ í™œìš©í•œë‹¤. ì „ì²´ í–‰ë ¬ì„ í•œ ë²ˆì— ê³„ì‚°í•˜ëŠ” ëŒ€ì‹ , ì…ë ¥ì„ ì‘ì€ ë¸”ë¡(íƒ€ì¼)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ SRAMì—ì„œ ì–´í…ì…˜ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê³  ì¤‘ê°„ ê²°ê³¼ë§Œ HBMì— ì €ì¥í•¨ìœ¼ë¡œì¨ HBMê³¼ì˜ ë°ì´í„° êµí™˜ íšŸìˆ˜ë¥¼ ê·¹ì ìœ¼ë¡œ ì¤„ì¸ë‹¤.

### 2.2 FlashAttention-3ì˜ í•˜ë“œì›¨ì–´ ê°€ì†

**FlashAttention-3**ëŠ” NVIDIAì˜ ìµœì‹  Hopper ì•„í‚¤í…ì²˜(H100/H200 GPU ë“±)ì— íƒ‘ì¬ëœ í•˜ë“œì›¨ì–´ ê°€ì† ê¸°ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•œë‹¤:

- **TMA (Tensor Memory Accelerator)**: HBMê³¼ SRAM ê°„ì˜ í…ì„œ ë°ì´í„° ì´ë™ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°€ì†í•˜ëŠ” ì „ìš© í•˜ë“œì›¨ì–´
- **WGMMA (Warpgroup Matrix Multiply-Accumulate)**: í–‰ë ¬ ê³±ì…ˆ-ëˆ„ì‚° ì—°ì‚°ì„ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•˜ë“œì›¨ì–´ ìœ ë‹›
- **FP8 ì§€ì›**: 8ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì (FP8) ì €ì •ë°€ë„ í¬ë§·ì„ ì§€ì›í•˜ì—¬, ì •ë°€ë„ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œë„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì²˜ë¦¬ëŸ‰ì„ ê±°ì˜ ë‘ ë°°ë¡œ ëŠ˜ë¦¼

ê·¸ ê²°ê³¼, FlashAttention-3ëŠ” H100 GPUì—ì„œ FlashAttention-2 ëŒ€ë¹„ **1.5ë°°ì—ì„œ 2.0ë°°** ë¹ ë¥¸ ì†ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤.

### 2.3 ì‹¤ìŠµ: Hugging Face Transformersì—ì„œ FlashAttention í™œì„±í™”

Hugging Faceì˜ ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” FlashAttentionì„ ê¸´ë°€í•˜ê²Œ í†µí•©í•˜ì—¬, **ëª¨ë¸ ë¡œë“œ ì‹œ `attn_implementation` ì¸ì í•˜ë‚˜ë§Œìœ¼ë¡œ** ê°„ë‹¨í•˜ê²Œ í™œì„±í™”í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì¡´ ì½”ë“œë¥¼ ê±°ì˜ ë³€ê²½í•˜ì§€ ì•Šìœ¼ë©´ì„œë„ **ìƒë‹¹í•œ ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ **ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# GPUê°€ Hopper ì•„í‚¤í…ì²˜ ì´ìƒì´ê³ , flash-attn ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
model_id = "openai/gpt-oss-20b"  # FlashAttention-3ë¥¼ ì§€ì›í•˜ëŠ” ì˜ˆì‹œ ëª¨ë¸ ID

# 1. ê¸°ë³¸ ì–´í…ì…˜ êµ¬í˜„(Eager)ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_id)
model_eager = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
print("Model with standard attention loaded.")

# 2. FlashAttention-3 êµ¬í˜„ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ
# ì´ ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ vLLMì˜ FlashAttention-3 ì»¤ë„ì„ ì‚¬ìš© ê°€ëŠ¥í•˜ë©°,
# í•´ë‹¹ ì»¤ë„ì€ 'kernels' íŒ¨í‚¤ì§€ë¥¼ í†µí•´ í—ˆë¸Œì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.
try:
    model_flash = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation="kernels-community/vllm-flash-attn3"  # FlashAttention-3 í™œì„±í™”
    )
    print("Model with FlashAttention-3 loaded successfully.")
    print("Note: This requires a compatible GPU (e.g., NVIDIA Hopper series).")
except ImportError:
    print("FlashAttention is not installed or the environment does not support it.")
except Exception as e:
    print(f"An error occurred while loading with FlashAttention: {e}")
```

**ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ:**

```
Model with standard attention loaded.
Model with FlashAttention-3 loaded successfully.
Note: This requires a compatible GPU (e.g., NVIDIA Hopper series).
```

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

- FlashAttentionì´ ê¸°ì¡´ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ê°€? íƒ€ì¼ë§ ê¸°ë²•ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€?
- FlashAttention-3ì—ì„œ í™œìš©í•˜ëŠ” NVIDIA Hopper ì•„í‚¤í…ì²˜ì˜ ì£¼ìš” í•˜ë“œì›¨ì–´ ê°€ì† ê¸°ëŠ¥ë“¤ì€ ë¬´ì—‡ì¸ê°€?
- `attn_implementation="kernels-community/vllm-flash-attn3"` ì˜µì…˜ì„ ì‚¬ìš©í•  ë•Œ í•„ìš”í•œ ì¡°ê±´ë“¤ê³¼, ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•Šì„ ê²½ìš° ì–´ë–¤ í˜„ìƒì´ ë°œìƒí•˜ëŠ”ê°€?

### 2.4 ì¶”ê°€ ì‹¤ìŠµ: PyTorch scaled_dot_product_attention ì§ì ‘ ì‚¬ìš©

PyTorch 2.0ë¶€í„°ëŠ” í•µì‹¬ APIì— `torch.nn.functional.scaled_dot_product_attention` (SDPA) í•¨ìˆ˜ê°€ ë‚´ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë°±ì—”ë“œì—ì„œ GPU ì•„í‚¤í…ì²˜, ì…ë ¥ í…ì„œì˜ ì†ì„±(dtype, ë§ˆìŠ¤í¬ ì¡´ì¬ ì—¬ë¶€ ë“±)ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ê°€ì¥ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ êµ¬í˜„ì„ ì„ íƒí•©ë‹ˆë‹¤. ì¦‰, Ampere ì•„í‚¤í…ì²˜ ì´ìƒì˜ GPUì™€ í˜¸í™˜ë˜ëŠ” ì¡°ê±´ì—ì„œëŠ” **ìë™ìœ¼ë¡œ FlashAttention ì»¤ë„ì´ë‚˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸(memory-efficient) ì»¤ë„ì„ í˜¸ì¶œ**í•©ë‹ˆë‹¤.

```python
import torch
import torch.nn.functional as F

# FlashAttention ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ GPU ë° half-precision(FP16/BF16) ì‚¬ìš©
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16  # ë˜ëŠ” torch.bfloat16

# ë°°ì¹˜=1, í—¤ë“œ=8, ì‹œí€€ìŠ¤ ê¸¸ì´=1024, ì„ë² ë”©=64ì˜ ë¬´ì‘ìœ„ í…ì„œ ìƒì„±
q = torch.randn(1, 8, 1024, 64, device=device, dtype=dtype)
k = torch.randn(1, 8, 1024, 64, device=device, dtype=dtype)
v = torch.randn(1, 8, 1024, 64, device=device, dtype=dtype)

# íŠ¹ì • ì»¤ë„ë§Œ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•˜ì—¬ ë™ì‘ í™•ì¸ (ë””ë²„ê¹…/í…ŒìŠ¤íŠ¸ìš©)
# enable_flash=True: FlashAttention ì»¤ë„ ì‚¬ìš© ì‹œë„
# enable_math=False: ìˆœìˆ˜ PyTorch ìˆ˜í•™ êµ¬í˜„ ë¹„í™œì„±í™”
# enable_mem_efficient=False: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  êµ¬í˜„ ë¹„í™œì„±í™”
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    try:
        out = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0)
        print("FlashAttention kernel was successfully used.")
        print("Output shape:", out.shape)
    except RuntimeError as e:
        print(f"Failed to use FlashAttention kernel exclusively: {e}")

```

ìœ„ ì½”ë“œëŠ” `torch.backends.cuda.sdp_kernel` ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•˜ì—¬ **FlashAttention ê²½ë¡œë§Œ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œ**í•©ë‹ˆë‹¤. ë§Œì•½ GPUê°€ ì§€ì›í•˜ê³  ì…ë ¥ ì¡°ê±´(half-precision, ë§ˆìŠ¤í¬ ì—†ìŒ ë“±)ì´ ë§ìœ¼ë©´ ë‚´ë¶€ì ìœ¼ë¡œ FlashAttention ì»¤ë„ì´ ì„±ê³µì ìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤. ì¡°ê±´ì´ ë§ì§€ ì•Šìœ¼ë©´ PyTorchëŠ” "Flash attention kernel not used because..."ì™€ ê°™ì€ ìœ ìš©í•œ ê²½ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ë©°, ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì™¸ë¶€ì—ì„œëŠ” ì•ˆì „í•˜ê²Œ ë‹¤ë¥¸ êµ¬í˜„ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìëŠ” ì˜ë„í•œ ìµœì í™”ê°€ ì ìš©ë˜ê³  ìˆëŠ”ì§€ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

- `scaled_dot_product_attention` í•¨ìˆ˜ë¥¼ FP32ë¡œ ì‹¤í–‰í•˜ë©´ ì–´ë–¤ ì¼ì´ ë°œìƒí•˜ëŠ”ê°€? FlashAttentionì´ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?
- `attn_mask` ì¸ìë¥¼ ë„£ìœ¼ë©´ FlashAttentionì´ ì‚¬ìš©ë ê¹Œ? FlashAttentionì´ ì§€ì›í•˜ëŠ” ë§ˆìŠ¤í¬ ìœ í˜•ì€ ë¬´ì—‡ì¸ê°€?
- PyTorchì—ì„œ ì œê³µí•˜ëŠ” ê²½ê³  ë©”ì‹œì§€ë¥¼ í†µí•´ ì–´ë–¤ íŒíŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ê°€?

---

## 3. Hugging Face Transformers ìƒíƒœê³„: ìµœì‹  ë™í–¥ê³¼ ì‹¤ìŠµ

Hugging Face ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¨ìˆœí•œ **ëª¨ë¸ ì €ì¥ì†Œ**ë¥¼ ë„˜ì–´, ìµœì‹  AI ê¸°ìˆ ì„ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì ‘ê·¼í•˜ê³  í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” ê±°ëŒ€í•œ í†µí•© í”Œë«í¼ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆë‹¤.

### 3.1 ìµœì‹  ë™í–¥

- **ìµœì‹  ëª¨ë¸ ì•„í‚¤í…ì²˜ì˜ ì‹ ì†í•œ ì§€ì›**: Vault-GEMMA, EmbeddingGemmaì™€ ê°™ì€ ìµœì‹  LLMì€ ë¬¼ë¡ , Florence-2(í†µí•© ë¹„ì „), SAM-2(ê³ ê¸‰ ì„¸ê·¸ë©˜í…Œì´ì…˜) ë“± **ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì§€ì›**ì´ ëŒ€í­ ê°•í™”ë˜ì—ˆë‹¤.

- **ê³ ê¸‰ ì–‘ìí™”(Quantization) ê¸°ìˆ  í†µí•©**: OpenAIì˜ GPT-OSS ëª¨ë¸ê³¼ í•¨ê»˜ ì†Œê°œëœ **MXFP4**ì™€ ê°™ì€ **4ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì–‘ìí™”** ë°©ì‹ì„ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›í•œë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ 4ë¹„íŠ¸ ì •ìˆ˜(INT4) ì–‘ìí™”ë³´ë‹¤ ë™ì  ë²”ìœ„ í‘œí˜„ì— ìœ ë¦¬í•˜ì—¬ ì •í™•ë„ ì†ì‹¤ì„ ì¤„ì´ë©´ì„œë„, 120B íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ë‹¨ì¼ 80GB GPUì— ë¡œë“œí•˜ëŠ” ë“± ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ íšê¸°ì ìœ¼ë¡œ ì ˆê°í•œë‹¤.

- **Zero-Build Kernels**: `kernels`ë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ í†µí•´ FlashAttention-3, Megablocks MoE ì»¤ë„ ë“± **ì‚¬ì „ì— ì»´íŒŒì¼ëœ ê³ ì„±ëŠ¥ ì»¤ë„**ì„ í—ˆë¸Œì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ì‚¬ìš©ìê°€ ìì‹ ì˜ í™˜ê²½ì—ì„œ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì§ì ‘ ì»´íŒŒì¼í•˜ëŠ” ë³µì¡í•˜ê³  ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³¼ì •ì„ ìƒëµí•˜ê²Œ í•´ì¤€ë‹¤.

### 3.2 ì‹¤ìŠµ: íŒŒì´í”„ë¼ì¸ APIë¡œ í•œêµ­ì–´ ê°ì„± ë¶„ì„ ìˆ˜í–‰

Hugging Faceì˜ **`pipeline` API**ëŠ” í† í¬ë‚˜ì´ì§•, ëª¨ë¸ ì¶”ë¡ , í›„ì²˜ë¦¬ê¹Œì§€ì˜ ì „ ê³¼ì •ì„ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ì¶”ìƒí™”í•˜ì—¬ ì œê³µí•˜ëŠ” ê°€ì¥ ê°„í¸í•˜ê³  ì§ê´€ì ì¸ ë„êµ¬ë‹¤. í•œêµ­ì–´ ì˜í™” ë¦¬ë·° ë°ì´í„°(NSMC)ë¡œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì˜ ê¸ì •/ë¶€ì •ì„ ë¶„ì„í•´ë³´ì.

```python
from transformers import pipeline

# í•œêµ­ì–´ ê°ì„± ë¶„ì„ì„ ìœ„í•´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” íŒŒì´í”„ë¼ì¸ ìƒì„±
# ëª¨ë¸: WhitePeak/bert-base-cased-Korean-sentiment (NSMC ë°ì´í„°ì…‹ ê¸°ë°˜ fine-tuning)
classifier = pipeline(
    "sentiment-analysis",
    model="WhitePeak/bert-base-cased-Korean-sentiment"
)

# ë¶„ì„í•  ë¬¸ì¥ë“¤
reviews = [
    "ì´ ì˜í™”ëŠ” ì œ ì¸ìƒ ìµœê³ ì˜ ì˜í™”ì…ë‹ˆë‹¤. ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ ì •ë§ ì¸ìƒ ê¹Šì—ˆì–´ìš”.",
    "ê¸°ëŒ€í–ˆë˜ ê²ƒë³´ë‹¤ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì› ì–´ìš”. ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ í‰ë²”í–ˆìŠµë‹ˆë‹¤.",
    "ì‹œê°„ ê°€ëŠ” ì¤„ ëª¨ë¥´ê³  ë´¤ë„¤ìš”. ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤!",
    "ìŒì•…ì€ ì¢‹ì•˜ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œ ì§€ë£¨í•œ ëŠë‚Œì„ ì§€ìš¸ ìˆ˜ ì—†ì—ˆë‹¤."
]

# ê°ì„± ë¶„ì„ ì‹¤í–‰
results = classifier(reviews)

# ê²°ê³¼ ì¶œë ¥
for review, result in zip(reviews, results):
    label = "ê¸ì •" if result['label'] == 'LABEL_1' else "ë¶€ì •"
    score = result['score']
    print(f"ë¦¬ë·°: \"{review}\"")
    print(f"ê²°ê³¼: {label} (ì‹ ë¢°ë„: {score:.4f})\n")
```

**ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ:**

```
ë¦¬ë·°: "ì´ ì˜í™”ëŠ” ì œ ì¸ìƒ ìµœê³ ì˜ ì˜í™”ì…ë‹ˆë‹¤. ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ ì •ë§ ì¸ìƒ ê¹Šì—ˆì–´ìš”."
ê²°ê³¼: ê¸ì • (ì‹ ë¢°ë„: 0.9985)

ë¦¬ë·°: "ê¸°ëŒ€í–ˆë˜ ê²ƒë³´ë‹¤ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì› ì–´ìš”. ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ í‰ë²”í–ˆìŠµë‹ˆë‹¤."
ê²°ê³¼: ë¶€ì • (ì‹ ë¢°ë„: 0.9978)

ë¦¬ë·°: "ì‹œê°„ ê°€ëŠ” ì¤„ ëª¨ë¥´ê³  ë´¤ë„¤ìš”. ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤!"
ê²°ê³¼: ê¸ì • (ì‹ ë¢°ë„: 0.9982)

ë¦¬ë·°: "ìŒì•…ì€ ì¢‹ì•˜ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œ ì§€ë£¨í•œ ëŠë‚Œì„ ì§€ìš¸ ìˆ˜ ì—†ì—ˆë‹¤."
ê²°ê³¼: ë¶€ì • (ì‹ ë¢°ë„: 0.9969)
```

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

- Hugging Face Transformersì˜ ìµœì‹  ë™í–¥ ì¤‘ **Zero-Build Kernels**ëŠ” ë¬´ì—‡ì´ë©°, ì–´ë–¤ ì¥ì ì„ ì œê³µí•˜ëŠ”ê°€?
- ìœ„ ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ì—ì„œ ì¶œë ¥ëœ `LABEL_0`ê³¼ `LABEL_1`ì€ ê°ê° ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ê°€?
- ë‹¤ë¥¸ í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸ë¡œ ë°”ê¿” ì‹¤í–‰í–ˆì„ ë•Œ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?

---

## 4. AI ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬: ìë™í™”ì™€ í˜‘ì—…ì˜ ì‹œëŒ€

LLMì˜ ë°œì „ì€ ë‹¨ì¼ ëª¨ë¸ì´ í•˜ë‚˜ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ë„˜ì–´, **ì—¬ëŸ¬ ë„êµ¬ë¥¼ ììœ¨ì ìœ¼ë¡œ ì‚¬ìš©**í•˜ê³  **ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì™€ í˜‘ì—…í•˜ë©° ë³µì¡í•œ ëª©í‘œë¥¼ ë‹¬ì„±**í•˜ëŠ” **AI ì—ì´ì „íŠ¸** íŒ¨ëŸ¬ë‹¤ì„ì„ ì—´ì—ˆë‹¤. ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì§€ì›í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ í”„ë ˆì„ì›Œí¬ê°€ ë“±ì¥í–ˆìœ¼ë©°, ê°ê° ëšœë ·í•œ ì² í•™ê³¼ ê°•ì ì„ ê°€ì§€ê³  ìˆë‹¤.

### 4.1 ì£¼ìš” AI ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ ë¹„êµ

| í”„ë ˆì„ì›Œí¬     | í•µì‹¬ ì² í•™                                           | ì•„í‚¤í…ì²˜ ìŠ¤íƒ€ì¼                                 | ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€                                                                   |
| :------------- | :-------------------------------------------------- | :---------------------------------------------- | :------------------------------------------------------------------------------- |
| **LangGraph**  | ëª…ì‹œì  ì œì–´ ë° ìƒíƒœ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜             | ìƒíƒœë¥¼ ê°€ì§„ ìœ í–¥ ë¹„ìˆœí™˜ ê·¸ë˜í”„ (DAG), ìˆœí™˜ í—ˆìš© | ì¸ê°„ì˜ ê°ë…ì´ ì¤‘ìš”í•œ ì‹ ë¢°ì„± ìˆê³  ê°ì‚¬ ê°€ëŠ¥í•œ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ êµ¬ì¶•         |
| **CrewAI**     | ì—­í•  ê¸°ë°˜ í˜‘ì—… ì§€ëŠ¥                                 | ê³„ì¸µì , ì—­í•  ê¸°ë°˜ **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**      | ì—­í•  ë¶„ë‹´ì´ ëª…í™•í•œ ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì›Œí¬í”Œë¡œìš° ìë™í™” (ì˜ˆ: ì‹œì¥ ë¶„ì„íŒ€)           |
| **LlamaIndex** | ë°ì´í„° ì¤‘ì‹¬ ì—ì´ì „íŠ¸ ë° ê³ ê¸‰ RAG                    | ë°ì´í„° ì¤‘ì‹¬, ì´ë²¤íŠ¸ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°             | ëŒ€ê·œëª¨ ì‚¬ì„¤/ë¹„ì •í˜• ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ **ì§ˆì˜ì‘ë‹µ ë° ì¶”ë¡ ** ì‹œìŠ¤í…œ êµ¬ì¶•          |
| **Haystack**   | í”„ë¡œë•ì…˜ ë ˆë”” ëª¨ë“ˆí˜• íŒŒì´í”„ë¼ì¸                     | ëª¨ë“ˆí˜•, ë¶„ê¸°/ë£¨í”„ê°€ ê°€ëŠ¥í•œ **íŒŒì´í”„ë¼ì¸**       | í™•ì¥ ê°€ëŠ¥í•˜ê³  ê²¬ê³ í•œ í”„ë¡œë•ì…˜ ë“±ê¸‰ì˜ **AI ì• í”Œë¦¬ì¼€ì´ì…˜** êµ¬ì¶•                    |
| **DSPy**       | ì„ ì–¸ì  LM í”„ë¡œê·¸ë˜ë° ("í”„ë¡¬í”„íŠ¸ê°€ ì•„ë‹Œ í”„ë¡œê·¸ë˜ë°") | ì„ ì–¸ì , ìµœì í™” ê°€ëŠ¥ **íŒŒì´í”„ë¼ì¸**              | ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ íŠœë‹ì„ **ë°ì´í„° ê¸°ë°˜ ìµœì í™”**ë¡œ ëŒ€ì²´í•˜ì—¬ ìµœê³  ì„±ëŠ¥ì„ ìš”êµ¬í•˜ëŠ” ì‘ì—… |

ìœ„ í‘œì—ì„œ ë³´ë“¯, **LangGraph**ëŠ” ì—ì´ì „íŠ¸ì˜ ìƒíƒœì™€ ì œì–´ íë¦„ì„ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬í•˜ë©° **ì¥ê¸° ì‹¤í–‰ ë° ì‹ ë¢°ì„±**ì— ì´ˆì ì„ ë§ì¶”ê³ , **CrewAI**ëŠ” ê°ê¸° ë‹¤ë¥¸ ì „ë¬¸ì„±ì„ ê°€ì§„ ì—ì´ì „íŠ¸ë“¤ì˜ **í˜‘ì—…**ì— ì¤‘ì ì„ ë‘”ë‹¤. **LlamaIndex**ëŠ” ë°©ëŒ€í•œ ì§€ì‹ë² ì´ìŠ¤ì™€ ê²°í•©ëœ ë°ì´í„° ì¤‘ì‹¬ ì—ì´ì „íŠ¸ë¥¼ ì§€í–¥í•˜ë©°, **Haystack**ì€ ê²€ìƒ‰-ì¶”ë¡  ë“± **ëª¨ë“ˆ ì¡°í•©í˜• íŒŒì´í”„ë¼ì¸**ì˜ ì‹¤ì „ ì ìš©ì— ê°•í•˜ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ **DSPy**ëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ìì²´ë¥¼ ì¶”ìƒí™”í•˜ì—¬ **ì„ ì–¸ì  í”„ë¡œê·¸ë˜ë°** ìŠ¤íƒ€ì¼ë¡œ LLM í™œìš©ì„ ê³ ë„í™”í•œë‹¤. ê° í”„ë ˆì„ì›Œí¬ì˜ ì² í•™ê³¼ êµ¬ì¡°ë¥¼ ì´í•´í•˜ë©´, í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œì˜ ì„±ê²©ì— ë”°ë¼ ê°€ì¥ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

### 4.2 DSPy: ì„ ì–¸ì  í”„ë¡¬í”„íŠ¸ í”„ë¡œê·¸ë˜ë°

**DSPy**ëŠ” *Declarative Self-Improving Python*ì˜ ì•½ìë¡œ, Databricksì—ì„œ ì¶œì‹œí•œ **ì„ ì–¸í˜• í”„ë¡¬í”„íŠ¸ í”„ë¡œê·¸ë˜ë°** í”„ë ˆì„ì›Œí¬ë‹¤. LLMì„ ì§ì ‘ ë‹¤ë£¨ë©´ì„œ ë°œìƒí•˜ëŠ” **ê¸´ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ê´€ë¦¬**ì˜ ë³µì¡í•¨ì„ ì¤„ì´ê³ , ë§ˆì¹˜ **ì½”ë“œë¥¼ ì‘ì„±í•˜ë“¯ ëª¨ë“ˆí™”ëœ êµ¬ì„±**ìœ¼ë¡œ AI í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. í•œë§ˆë””ë¡œ "í”„ë¡¬í”„íŠ¸ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ë§ê³ , **í”„ë¡œê·¸ë˜ë°ì²˜ëŸ¼** ì‘ì„±í•˜ë¼"ëŠ” ì² í•™ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆë‹¤.

DSPyì˜ í•µì‹¬ ê°œë…ì€ **LM**, **Signature**, **Module** ì„¸ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤:

- **LM**: ì‚¬ìš©í•  **ì–¸ì–´ ëª¨ë¸**ì„ ì§€ì •í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ OpenAI APIì˜ GPT-4, HuggingFaceì˜ Llama2 ë“± ì›í•˜ëŠ” ëª¨ë¸ì„ `dspy.LM(...)`ìœ¼ë¡œ ì„¤ì •í•˜ê³  `dspy.configure(lm=...)` í•˜ë©´, ì´í›„ ëª¨ë“  ëª¨ë“ˆì´ ì´ LMì„ í†µí•´ ê²°ê³¼ë¥¼ ìƒì„±í•œë‹¤.

- **Signature**: í•¨ìˆ˜ì˜ ì…ë ¥ê³¼ ì¶œë ¥ íƒ€ì…ì„ ì§€ì •í•˜ë“¯, í”„ë¡¬í”„íŠ¸ í”„ë¡œê·¸ë¨ì˜ **ì…ë ¥ê³¼ ì¶œë ¥ í˜•ì‹**ì„ ì„ ì–¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `"question -> answer: int"`ì²˜ëŸ¼ signatureë¥¼ ì •ì˜í•˜ë©´, DSPyëŠ” `question`(str)ì„ ë°›ì•„ `answer`(int)ë¥¼ ë‚´ëŠ” êµ¬ì¡°ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ ìƒì„±í•œë‹¤. ì‹œê·¸ë‹ˆì²˜ëŠ” ëª¨ë¸ì—ê²Œ ì£¼ì–´ì§ˆ í”„ë¡¬í”„íŠ¸ì˜ êµ¬ì¡°ì™€ ê¸°ëŒ€ ì¶œë ¥ í˜•íƒœ(ì˜ˆ: JSON ë˜ëŠ” ì •ìˆ˜ ë“±)ë¥¼ ê¸°ìˆ í•˜ëŠ” ì—­í• ì„ í•œë‹¤.

- **Module**: ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ **í”„ë¡¬í”„íŠ¸ ê¸°ë²•** ìì²´ë¥¼ ëª¨ë“ˆë¡œ ìº¡ìŠí™”í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¨ìˆœ ì§ˆì˜ì‘ë‹µì€ `dspy.Predict`, ë³µì¡í•œ ì‚¬ê³ ê°€ í•„ìš”í•œ ê²½ìš° `dspy.ChainOfThought`(ì—°ì‡„ì  ì‚¬ê³ ), íˆ´ ì‚¬ìš© ì—ì´ì „íŠ¸ëŠ” `dspy.ReAct` ëª¨ë“ˆë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ê° ëª¨ë“ˆì€ ë‚´ë¶€ì ìœ¼ë¡œ í•´ë‹¹ ê¸°ë²•ì— ë§ê²Œ **í”„ë¡¬í”„íŠ¸ë¥¼ ì–´ë–»ê²Œ êµ¬ì„±í• ì§€** ë¡œì§ì´ êµ¬í˜„ë˜ì–´ ìˆë‹¤.

ì‚¬ìš©ìëŠ” ì´ ì„¸ ê°€ì§€ë¥¼ ì¡°í•©í•˜ì—¬ **AI í”„ë¡œê·¸ë¨**ì„ ë§Œë“  ë’¤, DSPyì— ë‚´ì¥ëœ **Optimizer**ë¥¼ í†µí•´ ëª¨ë“ˆì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ê°œì„ í•˜ê±°ë‚˜ few-shot ì˜ˆì‹œë¥¼ ì¶”ê°€í•˜ëŠ” ë“±ì˜ ìµœì í™”ë¥¼ í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì²˜ëŸ¼ ê°„ë‹¨í•œ ì¡°í•©ì„ ë§Œë“¤ì–´ë³¼ ìˆ˜ ìˆë‹¤:

```python
%pip install dspy # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Databricks DSPy)
import dspy

# 1) LM ì„¤ì • (ì˜ˆ: ë¡œì»¬ Llama2 ëª¨ë¸ API ì‚¬ìš©)
llm = dspy.LM('ollama/llama2', api_base='http://localhost:11434') # ë¡œì»¬ ì„œë²„ ì˜ˆì‹œ
dspy.configure(lm=llm)

# 2) Signature ì„ ì–¸: "ì§ˆë¬¸(str) -> ë‹µë³€(int)" í˜•ì‹
simple_sig = "question -> answer: int"

# 3) Module ì„ íƒ: Predict (ê¸°ë³¸ì ì¸ 1ë‹¨ê³„ ì§ˆì˜ì‘ë‹µ)
simple_model = dspy.Predict(simple_sig)

# 4) ì‹¤í–‰
result = simple_model(question="ì„œìš¸ì—ì„œ ë¶€ì‚°ê¹Œì§€ KTXë¡œ ëª‡ ì‹œê°„ ê±¸ë¦¬ë‚˜ìš”?")
print(result)
```

ìœ„ ì½”ë“œëŠ” `simple_model`ì´ë¼ëŠ” ëª¨ë“ˆì„ ë§Œë“¤ì–´ \*"ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ì •ìˆ˜ í˜•íƒœì˜ ë‹µë³€ì„ ì¶œë ¥"\*í•˜ëŠ” ì‘ì—…ì„ ì •ì˜í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ DSPyëŠ” ì´ ìš”êµ¬ì— ë§ëŠ” ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ LMì— ì „ë‹¬í•©ë‹ˆë‹¤. (ì˜ˆë¥¼ ë“¤ì–´, "Q: ì„œìš¸ì—ì„œ ë¶€ì‚°ê¹Œì§€ KTXë¡œ ëª‡ ì‹œê°„ ê±¸ë¦¬ë‚˜ìš”?\\nA:" í˜•íƒœë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ìˆ«ì í˜•íƒœ ë‹µë³€ì„ ê¸°ëŒ€í•˜ëŠ” ì‹ì…ë‹ˆë‹¤.) ì´ˆê¸°ì— ì–»ì€ ë‹µì´ ë¶€ì •í™•í•˜ë‹¤ë©´, **BootstrapFewShot**ê³¼ ê°™ì€ Optimizerë¥¼ ì ìš©í•´ few-shot ì˜ˆì‹œë¥¼ ìë™ìœ¼ë¡œ ì²¨ê°€í•˜ê±°ë‚˜, **Refine** ëª¨ë“ˆë¡œ ë‹µë³€ì„ ì§€ì† ê°œì„ í•˜ë„ë¡ ì§€ì‹œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ë°©ì‹ìœ¼ë¡œ DSPyëŠ” ë³µì¡í•œ LLM íŒŒì´í”„ë¼ì¸(ì˜ˆ: RAG ì‹œìŠ¤í…œ, ë‹¤ë‹¨ê³„ ì²´ì¸, ì—ì´ì „íŠ¸ ë£¨í”„ ë“±)ë„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ êµ¬ì„±í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

DSPyì˜ ì¥ì ì€ **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ ìƒì‚°ì„± í–¥ìƒ**ì…ë‹ˆë‹¤. ì½”ë“œì²˜ëŸ¼ êµ¬ì¡°í™”ëœ í‹€ ì•ˆì—ì„œ LLM í˜¸ì¶œì„ ì„¤ê³„í•˜ë¯€ë¡œ, ì‚¬ëŒì´ ê¸´ í”„ë¡¬í”„íŠ¸ ë¬¸ì¥ì„ ì¼ì¼ì´ ì‘ì„±í•˜ë©° ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªëŠ” ì‹œê°„ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ ì—¬ëŸ¬ **ëª¨ë¸/ê¸°ë²•ì„ êµì²´**í•˜ë©´ì„œë„ ë™ì¼í•œ ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆì–´, ì˜ˆì»¨ëŒ€ ë™ì¼í•œ Chain-of-Thought ëª¨ë“ˆì„ GPT-4ì™€ Llama2ì— ëª¨ë‘ ì ìš©í•´ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ë“± **ìœ ì—°í•œ ì‹¤í—˜**ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì„ ì–¸ì ì¸ ì ‘ê·¼ ë•ë¶„ì— **í”„ë¡œê·¸ë¨ì˜ ì¼ë¶€ë§Œ ë³€ê²½**í•´ë„ ì „ì²´ LLM íŒŒì´í”„ë¼ì¸ì— ì‰½ê²Œ ë°˜ì˜ë˜ë¯€ë¡œ, ìœ ì§€ë³´ìˆ˜ë„ ìš©ì´í•©ë‹ˆë‹¤. ì•„ì§ ì´ˆê¸° ë‹¨ê³„ì˜ í”„ë ˆì„ì›Œí¬ì´ì§€ë§Œ, \*\*"í”„ë¡œê·¸ë˜ë°í•˜ë“¯ LLMì„ ë‹¤ë£¬ë‹¤"\*\*ëŠ” íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí–ˆë‹¤ëŠ” ì ì—ì„œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

**ì—°ìŠµ ë¬¸ì œ:** ìœ„ DSPy ì˜ˆì‹œì—ì„œ Signatureë¥¼ "question -\> answer: int"ë¡œ ì •ì˜í•˜ì˜€ëŠ”ë°, ë§Œì•½ "question -\> answer: JSON"ì²˜ëŸ¼ JSON í˜•ì‹ ì¶œë ¥ì„ ìš”êµ¬í•˜ë„ë¡ ë°”ê¾¸ë©´ ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆê¹Œìš”? ê·¸ë¦¬ê³  dspy.Predict ì™¸ì— ë³µì¡í•œ ë¬¸ì œì— ëŒ€í•´ dspy.ChainOfThoughtë‚˜ dspy.ReActë¥¼ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ ì°¨ì´ê°€ ìˆì„ì§€ ê°„ë‹¨íˆ ì„¤ëª…í•´ë³´ì„¸ìš”.

### 4.2 Haystack: ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ê³¼ ì¶”ë¡ 

**Haystack**ì€ ë…ì¼ Deepsetì—ì„œ ê°œë°œí•œ **ì˜¤í”ˆì†ŒìŠ¤ NLP í”„ë ˆì„ì›Œí¬**ë¡œ, ì£¼ë¡œ **ì§€ì‹ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ**(Question Answering) ì‹œìŠ¤í…œ êµ¬ì¶•ì— ì‚¬ìš©ë©ë‹ˆë‹¤. Haystackì˜ ê°•ì ì€ **ìœ ì—°í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì„±**ì— ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ë°ì´í„°ë² ì´ìŠ¤(ë¬¸ì„œ ì €ì¥ì†Œ)ë¶€í„° ê²€ìƒ‰ê¸°(Retriever), ë¦¬ë”(Reader)ë‚˜ ìƒì„±ê¸°(Generator) ëª¨ë¸ê¹Œì§€ ì¼ë ¨ì˜ ë‹¨ê³„ë¥¼ í•˜ë‚˜ì˜ Pipelineìœ¼ë¡œ ì—®ì–´, ì§ˆë¬¸ì„ ë„£ìœ¼ë©´ ë‹µë³€ì„ ë°˜í™˜í•˜ëŠ” **ì—”ë“œíˆ¬ì—”ë“œ NLP ì‹œìŠ¤í…œ**ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "ì£¼ì–´ì§„ ë¬¸ì„œ ì§‘í•©ì—ì„œ ì§ˆë¬¸ì˜ ë‹µì„ ì°¾ì•„ë¼"ì™€ ê°™ì€ **Retrieval QA**ë‚˜, ìœ„í‚¤í”¼ë””ì•„ ê¸°ë°˜ **ì±—ë´‡** ë“±ì„ Haystackìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Haystackì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:

- **DocumentStore**: ë§ ê·¸ëŒ€ë¡œ ë¬¸ì„œë¥¼ ì €ì¥í•˜ëŠ” **DB**ì…ë‹ˆë‹¤. In-Memory í˜•íƒœë‚˜ Elasticsearch, FAISS ë“±ì˜ ë°±ì—”ë“œë¥¼ ì§€ì›í•˜ë©°, ë¬¸ì„œì˜ ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°, ì„ë² ë”© ë“±ì„ ë³´ê´€í•©ë‹ˆë‹¤.

- **Retriever**: ì‚¬ìš©ìì˜ ì§ˆë¬¸(Query)ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ **ê²€ìƒ‰**í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. BM25 ê°™ì€ ì „í†µì  í‚¤ì›Œë“œ ê¸°ë°˜ë¶€í„° SBERT, DPR ë“± **Dense Passage Retrieval** ëª¨ë¸ê¹Œì§€ ë‹¤ì–‘í•˜ê²Œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. RetrieverëŠ” DocumentStoreì—ì„œ **ìƒìœ„ kê°œ**ì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.

- **Reader** ë˜ëŠ” **Generator**: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìµœì¢… **ë‹µì„ ìƒì„±**í•©ë‹ˆë‹¤. **Reader**ëŠ” ë³´í†µ Extractive QA ëª¨ë¸(BERT ë“±)ì„ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ë¬¸ì„œ ë‚´ì—ì„œ **ì •ë‹µ ìŠ¤íŒ¬**ì„ ë½‘ì•„ì£¼ê³ , **Generator**ëŠ” GPT-ê°™ì€ ìƒì„±í˜• ëª¨ë¸ì„ ì´ìš©í•´ ë‹µì„ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ Haystackì—ì„œ \*\*ë…¸ë“œ(Node)\*\*ë¡œì¨ í”ŒëŸ¬ê·¸ì¸ ê°€ëŠ¥í•˜ë©°, ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

- **Pipeline**: ìƒê¸° ìš”ì†Œë“¤ì„ ì¡°í•©í•˜ì—¬ **ì§ˆì˜ -\> ì‘ë‹µ í”Œë¡œìš°**ë¥¼ ì •ì˜í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. ê°„ë‹¨í•˜ê²ŒëŠ” Retriever ê²°ê³¼ë¥¼ Readerì— ë„£ëŠ” **ExtractiveQAPipeline**ì´ ìˆê³ , ìƒì„±í˜•ìœ¼ë¡œ ë‹µì„ ë§Œë“œëŠ” **GenerativeQAPipeline**ë„ ìˆìŠµë‹ˆë‹¤. ë˜ Retrieval-Augmented Generationì²˜ëŸ¼ **Retriever + LLM**ì„ ì—°ê²°í•˜ê±°ë‚˜, ì—¬ëŸ¬ ë‹¨ê³„ì˜ **ì¡°ê±´ë¶€ íë¦„**(ë¶„ê¸°/ë£¨í”„)ì„ êµ¬í˜„í•  ìˆ˜ë„ ìˆì–´ ë§¤ìš° ìœ ì—°í•©ë‹ˆë‹¤.

Haystackì„ ì´ìš©í•œ **ê°„ë‹¨ ì‹¤ìŠµ ì˜ˆì‹œ**ë¥¼ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. ê°€ë ¹ FAQ ë¬¸ì„œ ëª¨ìŒì„ ì´ìš©í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” QA ì‹œìŠ¤í…œì„ ë§Œë“ ë‹¤ê³  í•˜ë©´:

```python
%pip install farm-haystack[faiss] # Haystack ì„¤ì¹˜ ë° FAISS ë“± ì˜µì…˜ (í•„ìš”ì‹œ)

from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import BM25Retriever, FARMReader
from haystack import Pipeline

# 1) ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„± ë° ë¬¸ì„œ ì‚½ì…
document_store = InMemoryDocumentStore()
docs = [{"content": "ë“œë¼ë§ˆ **ì˜¤ì§•ì–´ ê²Œì„**ì€ í•œêµ­ì˜ ì„œë°”ì´ë²Œ ë“œë¼ë§ˆ...", "meta": {"source": "ìœ„í‚¤í”¼ë””ì•„"}}]
document_store.write_documents(docs)

# 2) Retrieverì™€ Reader êµ¬ì„±
retriever = BM25Retriever(document_store=document_store)
reader = FARMReader(model_name_or_path="monologg/koelectra-base-v3-finetuned-korquad", use_gpu=False)

# 3) íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
pipeline = Pipeline()
pipeline.add_node(component=retriever, name="Retriever", inputs=["Query"])
pipeline.add_node(component=reader, name="Reader", inputs=["Retriever"])

# 4) QA ì‹¤í–‰
query = "ì˜¤ì§•ì–´ ê²Œì„ ê°ë…ì´ ëˆ„êµ¬ì•¼?"
result = pipeline.run(query=query, params={"Retriever": {"top_k": 5}, "Reader": {"top_k": 1}})
print(result['answers'][0].answer)
```

ìœ„ ì½”ë“œì—ì„œëŠ” ê°„ë‹¨íˆ **ì¸ë©”ëª¨ë¦¬ ë¬¸ì„œì €ì¥ì†Œ**ì— í•˜ë‚˜ì˜ ë¬¸ì„œë¥¼ ë„£ê³ , BM25 ê¸°ë°˜ **Retriever**ì™€ í•œêµ­ì–´ KorQuAD ë°ì´í„°ë¡œ í•™ìŠµëœ Electra **Reader**ë¥¼ ì¡°í•©í•œ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. pipeline.run()ì— ì§ˆì˜ë¥¼ ë„£ìœ¼ë©´ Retrieverê°€ ìƒìœ„ 5ê°œ ë¬¸ì„œë¥¼ ì°¾ê³ , Readerê°€ ê·¸ ì¤‘ì—ì„œ ë‹µì„ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ "í™©ë™í˜"ì´ë¼ëŠ” ì •ë‹µì„ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

Haystackì˜ ê°•ë ¥í•œ ì ì€ ì´ì²˜ëŸ¼ **êµ¬ì„± ìš”ì†Œë¥¼ êµì²´í•˜ê±°ë‚˜ í™•ì¥í•˜ê¸° ìš©ì´**í•˜ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. Dense Retrieverë¡œ ë°”ê¾¸ê±°ë‚˜, Reader ëŒ€ì‹  GPT-3 ê°™ì€ ìƒì„± ëª¨ë¸ì„ Generatorë¡œ ë¶™ì´ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜ ë©€í‹°í™‰ QAì²˜ëŸ¼ ì¤‘ê°„ì— ì—¬ëŸ¬ ë…¸ë“œë¥¼ ìˆœì°¨/ë³‘ë ¬ êµ¬ì„±í•˜ì—¬ ë³µì¡í•œ ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì§€ì›í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ì‹¤ì œ ì‚°ì—… í˜„ì¥ì—ì„œëŠ” Haystackì„ í™œìš©í•´ **ë„ë©”ì¸ ë¬¸ì„œ ê²€ìƒ‰ + QA** ì„œë¹„ìŠ¤ë‚˜, **ì±—ë´‡**ì— ì™¸ë¶€ ì§€ì‹ì„ ì£¼ì…í•˜ëŠ” **RAG** íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ëŠ” ì‚¬ë¡€ê°€ ë§ìŠµë‹ˆë‹¤. ìš”ì•½í•˜ë©´, Haystackì€ **ê²€ìƒ‰ ì—”ì§„ê³¼ NLP ëª¨ë¸ì„ í•˜ë‚˜ë¡œ ì—®ëŠ” í”„ë ˆì„ì›Œí¬**ë¡œ, ë¹„êµì  ì ì€ ì½”ë“œë¡œ ê°•ë ¥í•œ **ë¬¸ì„œ ê¸°ë°˜ QA ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

**ì—°ìŠµ ë¬¸ì œ:** ìœ„ Haystack ì˜ˆì‹œì—ì„œ BM25Retriever ëŒ€ì‹  **Dense Retriever**(ì˜ˆ: DensePassageRetriever)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì–´ë–¤ ë³€í™”ê°€ í•„ìš”í• ê¹Œìš”? ë˜ Readerë¥¼ ëŒ€ì‹ í•˜ì—¬ GPT ê³„ì—´ ìƒì„± ëª¨ë¸ì„ Generatorë¡œ ë¶™ì´ë©´ ì–´ë–¤ ì¥ë‹¨ì ì´ ìˆì„ì§€ ìƒê°í•´ë³´ì„¸ìš”. ë§ˆì§€ë§‰ìœ¼ë¡œ, `pipeline.run()` í˜¸ì¶œ ì‹œ `params`ì— ì „ë‹¬í•œ `top_k` ê°’ë“¤ì„ ì¡°ì ˆí•˜ë©´ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆì§€ ì‹¤í—˜ì ìœ¼ë¡œ ì„¤ëª…í•´ë³´ì„¸ìš”.

### 4.3 CrewAI: ì—­í•  ê¸°ë°˜ ë©€í‹°ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬

**CrewAI**ëŠ” ìµœê·¼ ê°ê´‘ë°›ëŠ” AI ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ ì¤‘ í•˜ë‚˜ë¡œ, ì—¬ëŸ¬ ê°œì˜ LLM ì—ì´ì „íŠ¸ë¥¼ **íŒ€(crew)** í˜•íƒœë¡œ êµ¬ì„±í•˜ì—¬ **í˜‘ì—…ì ìœ¼ë¡œ ì‘ì—…**ì„ ìˆ˜í–‰í•˜ë„ë¡ í•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ LangChain ë“±ì´ ë‹¨ì¼ ì—ì´ì „íŠ¸ ë˜ëŠ” ì²´ì¸ ì¤‘ì‹¬ì´ì—ˆë‹¤ë©´, CrewAIëŠ” **ì—­í•  ê¸°ë°˜ ë©€í‹°ì—ì´ì „íŠ¸**ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í•˜ë‚˜ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Researcher**, **Analyst**, **Writer** ë“± ì—­í• ì„ ë‚˜ëˆ„ê³ , ê° ì—ì´ì „íŠ¸ê°€ ìì‹ ë§Œì˜ ë„êµ¬(tool)ì™€ ëª©í‘œ(goal)ë¥¼ ê°€ì§€ê³  ììœ¨ì ìœ¼ë¡œ í–‰ë™í•˜ë©´ì„œ ì „ì²´ì ìœ¼ë¡œëŠ” í˜‘ë ¥í•´ ìµœì¢… ê²°ê³¼ë¥¼ ì‚°ì¶œí•˜ë„ë¡ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

CrewAIì˜ ê°œë…ì„ ì£¼ìš” êµ¬ì„± ìš”ì†Œë³„ë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **Crew (ìŠ¹ë¬´ì› íŒ€)**: ì „ì²´ ì—ì´ì „íŠ¸ë“¤ì˜ ì¡°ì§ í˜¹ì€ í™˜ê²½ì…ë‹ˆë‹¤. `Crew` ê°ì²´ê°€ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ í¬í•¨í•˜ë©°, ì´ë“¤ì˜ **í˜‘ì—… í”„ë¡œì„¸ìŠ¤**ë¥¼ ì´ê´„í•©ë‹ˆë‹¤. í•˜ë‚˜ì˜ CrewëŠ” íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì—ì´ì „íŠ¸ íŒ€ì— ëŒ€ì‘í•©ë‹ˆë‹¤.

- **Agent (ì—ì´ì „íŠ¸)**: ë…ë¦½ì ì¸ **ììœ¨ AI**ë¡œ, ê°ê° ì •í•´ì§„ \*\*ì—­í• (role)\*\*ê³¼ **ë„êµ¬(tools)** ë° **ëª©í‘œ**ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "ë¬¸í—Œ ì¡°ì‚¬ì›" ì—ì´ì „íŠ¸ëŠ” ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ , "ë³´ê³ ì„œ ì‘ì„±ì" ì—ì´ì „íŠ¸ëŠ” ê¸€ì“°ê¸° ë„êµ¬ì™€ ë¬¸ì²´ì— ë§ì¶° ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì‹ì…ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” í•„ìš” ì‹œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•˜ê±°ë‚˜ ê²°ê³¼ë¥¼ ìš”ì²­í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤ (ë§ˆì¹˜ ì‚¬ëŒì´ íŒ€ í˜‘ì—…í•˜ë“¯).

- **Process (í”„ë¡œì„¸ìŠ¤)**: Crew ë‚´ì—ì„œ ì—ì´ì „íŠ¸ë“¤ì˜ **ìƒí˜¸ì‘ìš© ê·œì¹™**ì´ë‚˜ **ì›Œí¬í”Œë¡œìš°**ë¥¼ ì •ì˜í•œ ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "1ë‹¨ê³„: Researcherê°€ ìë£Œ ìˆ˜ì§‘ -\> 2ë‹¨ê³„: Analystê°€ ìš”ì•½ -\> 3ë‹¨ê³„: Writerê°€ ì •ë¦¬" ì™€ ê°™ì€ íë¦„ì„ í”„ë¡œì„¸ìŠ¤ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CrewAIì—ì„œëŠ” ì´ëŸ¬í•œ í”„ë¡œì„¸ìŠ¤ë¥¼ **Flow**ë¼ëŠ” ê°œë…ìœ¼ë¡œ í™•ì¥í•˜ë©°, ì´ë²¤íŠ¸ë‚˜ ì¡°ê±´ì— ë”°ë¼ ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ì œì–´í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

CrewAIë¥¼ ì‚¬ìš©í•˜ë©´ ê°œë°œìëŠ” ê° ì—ì´ì „íŠ¸ì˜ ì—­í• ê³¼ ì‚¬ìš© ë„êµ¬ë¥¼ ì •ì˜í•˜ê³ , `Crew`ë¥¼ ìƒì„±í•´ ì‹¤í–‰í•¨ìœ¼ë¡œì¨ **ë³µì¡í•œ ì‘ì—…ì„ ìë™í™”**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆë¥¼ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. ê°€ë ¹ \*"ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ ìë£Œë¥¼ ì°¾ì•„ ìš”ì•½í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ë¼"\*ëŠ” ë³µì¡í•œ ê³¼ì œë¥¼ ë‘ ì—ì´ì „íŠ¸ì—ê²Œ í˜‘ì—…ì‹œí‚¤ëŠ” ê²½ìš°:

```python
%pip install crewai # CrewAI í”„ë ˆì„ì›Œí¬ ì„¤ì¹˜ (ê°€ìƒ ê°€ì •)
from crewai import Crew, Agent, tool

# ì—ì´ì „íŠ¸ ì •ì˜: ê²€ìƒ‰ ë‹´ë‹¹ìì™€ ì‘ì„± ë‹´ë‹¹ì
searcher = Agent(name="Researcher", role="ì •ë³´ ìˆ˜ì§‘", tools=[tool("wiki_browser")])
writer = Agent(name="Writer", role="ë³´ê³ ì„œ ì‘ì„±", tools=[tool("text_editor")])

# Crew ìƒì„± ë° ì—ì´ì „íŠ¸ ì¶”ê°€
crew = Crew(agents=[searcher, writer], goal="ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•œ 1í˜ì´ì§€ ìš”ì•½ ë³´ê³ ì„œ ì‘ì„±")
crew.run(task="í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì¡°ì‚¬í•˜ê³  ìš”ì•½í•˜ë¼.")
```

ìœ„ ì˜ˆì‹œëŠ” ê°€ìƒì˜ ì½”ë“œì´ì§€ë§Œ, `Agent`ì—ê²Œ ì—­í• ê³¼ ì‚¬ìš©í•  íˆ´(ì˜ˆ: **ìœ„í‚¤ ë¸Œë¼ìš°ì €**, **í…ìŠ¤íŠ¸ ì—ë””í„°** ê¸°ëŠ¥)ì„ ë¶€ì—¬í•˜ê³  `Crew`ì— ë“±ë¡í•œ í›„ ì‹¤í–‰í•˜ëŠ” íë¦„ì„ ë¬˜ì‚¬í•©ë‹ˆë‹¤. ì‹¤í–‰ ì‹œ **Researcher** ì—ì´ì „íŠ¸ëŠ” ë¨¼ì € ìœ„í‚¤í”¼ë””ì•„ë¥¼ ê²€ìƒ‰í•´ ì •ë³´ë¥¼ ëª¨ìœ¼ê³ , ê·¸ ê²°ê³¼ë¥¼ **Writer** ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤. WriterëŠ” ë°›ì€ ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ ìš”ì•½ ë³´ê³ ì„œë¥¼ ì‘ì„±í•œ ë’¤ ìµœì¢… ë‹µì„ ì‚°ì¶œí•©ë‹ˆë‹¤. ì´ ëª¨ë“  ê³¼ì •ì´ ì‚¬ëŒ ê°œì… ì—†ì´ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë©°, CrewAI í”„ë ˆì„ì›Œí¬ê°€ **ê° ë‹¨ê³„ì˜ ìˆ˜í–‰ê³¼ ì—ì´ì „íŠ¸ ê°„ ë©”ì‹œì§€ êµí™˜**ì„ ê´€ë¦¬í•´ ì¤ë‹ˆë‹¤.

CrewAIì˜ íŠ¹ì§•ì€ **ë†’ì€ ìœ ì—°ì„±ê³¼ í†µì œë ¥**ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ëŒë¦¬ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê°œë°œìê°€ ì›í•˜ëŠ” ëŒ€ë¡œ **í˜‘ì—… íŒ¨í„´**ì„ ë””ìì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ê°œë³„ ì—ì´ì „íŠ¸ì— ëŒ€í•´ í”„ë¡¬í”„íŠ¸ ê·œì¹™, ì‘ë‹µ í˜•ì‹ ë“±ì„ ì„¸ë°€íˆ ì„¤ì • ê°€ëŠ¥í•˜ì—¬, íŒ€ ë‚´ **ì „ë¬¸ AI**ë“¤ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ **ìë™í™”ëœ ê³ ê°ì§€ì›** ì‹œë‚˜ë¦¬ì˜¤(ì˜ˆ: í•œ ì—ì´ì „íŠ¸ê°€ ìœ ì € ì˜ë„ë¥¼ íŒŒì•… -\> ë‹¤ë¥¸ ì—ì´ì „íŠ¸ê°€ FAQ ê²€ìƒ‰ -\> ë˜ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ê°€ ë‹µë³€ ìƒì„±)ë‚˜ **ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸**(ì—­í•  ë¶„ë‹´í•˜ì—¬ ë¬¸í—Œ ì •ë¦¬) ë“±ì— ì‘ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í¥ë¯¸ë¡œìš´ ì ì€ CrewAIê°€ ì™„ì „íˆ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¼ê¸°ë³´ë‹¤ **LangChain ë“± ê¸°ì¡´ íˆ´ê³¼ í˜¸í™˜**ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì¦‰, LangChainì—ì„œ ì“°ë˜ ë„êµ¬ ì²´ì¸ì„ ê°€ì ¸ì™€ CrewAI ì—ì´ì „íŠ¸ì˜ toolë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ íŠ¹ì„±ìƒ ì˜ˆìƒì¹˜ ëª»í•œ ìƒí˜¸ì‘ìš©ì´ë‚˜ **ë¬´í•œ ë£¨í”„** ë“±ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ **ì•ˆì „ì¥ì¹˜ ì„¤ê³„**ë„ ì¤‘ìš”í•©ë‹ˆë‹¤. CrewAI ì¸¡ì—ì„œëŠ” ì—­í• ë³„ **ì œí•œê³¼ ì •ì±…**ì„ ì„¤ì •í•˜ì—¬ ì—ì´ì „íŠ¸ë“¤ì´ ì •í•´ì§„ ë²”ìœ„ ë‚´ì—ì„œë§Œ í™œë™í•˜ë„ë¡ ê¶Œê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ìš”ì•½í•˜ë©´, CrewAIëŠ” **ì—­í•  ê¸°ë°˜ ììœ¨ ì—ì´ì „íŠ¸ë“¤ì˜ í˜‘ì—…ì„ ì²´ê³„í™”**í•œ í”„ë ˆì„ì›Œí¬ë¡œì„œ, í•˜ë‚˜ì˜ ê±°ëŒ€ LLMì´ ëª¨ë“  ê±¸ í•˜ëŠ” ëŒ€ì‹  ì—¬ëŸ¬ ì „ë¬¸ LLMì´ **ë¶„ì—…ê³¼ í˜‘ë ¥ì„ í†µí•´ ë” ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰**í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë©€í‹°ì—ì´ì „íŠ¸ AI ì‹œìŠ¤í…œ ê°œë°œì„ ì‰½ê³  í‘œì¤€í™”ëœ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

- CrewAIì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì¸ **Crew**, **Agent**, **Process**ëŠ” ê°ê° ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ê°€?
- CrewAIë¥¼ í™œìš©í•˜ë©´ ì–´ë–¤ ì¢…ë¥˜ì˜ AI ì‘ì—…ì—ì„œ ì¥ì ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ”ê°€? ì „ë¬¸ ë¶„ì•¼ ë¬¸ì„œ ì‘ì„±ì´ë‚˜ ê³ ê° ì§€ì› ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ í™œìš© ë°©ì•ˆì„ ìƒê°í•´ë³´ë¼.
- ë©€í‹°ì—ì´ì „íŠ¸ê°€ í•¨ê»˜ ì‘ì—…í•  ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œì (ì˜ˆ: ë¬´í•œ ë£¨í”„, ì¶©ëŒ)ì„ ë°©ì§€í•˜ë ¤ë©´ ì–´ë–¤ ì„¤ê³„ê°€ í•„ìš”í•œê°€?

### 4.3 LangGraph: ìƒíƒœ ê¸°ë°˜ ë©€í‹°ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

**LangGraph**ëŠ” LangChain íŒ€ì´ ê°œë°œí•œ **ì €ìˆ˜ì¤€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í”„ë ˆì„ì›Œí¬**ë¡œ, **ì¥ê¸°ê°„ ìœ ì§€ë˜ëŠ” ìƒíƒœ(state)**ë¥¼ ê°€ì§„ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. LangGraphëŠ” ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ **ê·¸ë˜í”„ ìë£Œêµ¬ì¡°**ë¡œ ê´€ë¦¬í•˜ë©°, ê° ë…¸ë“œê°€ ì—ì´ì „íŠ¸ì˜ ìƒíƒœì™€ ë™ì‘ì„ ë‚˜íƒ€ë‚´ê³  ì—£ì§€ë¡œ ìƒí˜¸ì‘ìš© ê²½ë¡œë¥¼ í‘œí˜„í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ ê°„ ë©”ì‹œì§€ íë¦„, ìƒíƒœ ë³€ê²½, ì˜¤ë¥˜ ë°œìƒ ì‹œì˜ **ë³µêµ¬ ì§€ì (checkpoint)** ë“±ì„ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ë£° ìˆ˜ ìˆì–´ **ì‹ ë¢°ì„±**ê³¼ **ë‚´êµ¬ì„±**ì´ ìš”êµ¬ë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ì í•©í•©ë‹ˆë‹¤.

LangGraph ì‚¬ìš©ì˜ í•µì‹¬ì€ *StateGraph*ë¼ëŠ” ê·¸ë˜í”„ ê°ì²´ë¥¼ ì •ì˜í•˜ê³ , í•„ìš”í•œ ê²½ìš° **ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ**ì™€ ì—°ê³„í•´ ì—ì´ì „íŠ¸ë“¤ì˜ ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ì €ì¥/ë³µêµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ **ê¸´ ëŒ€í™”**ë‚˜ **í”Œëœ ì‹¤í–‰** ë„ì¤‘ í•œ ì—ì´ì „íŠ¸ê°€ ì‹¤íŒ¨í•´ë„, ë§ˆì§€ë§‰ ì €ì¥ëœ stateë¡œ ë¡¤ë°±í•˜ì—¬ ì¬ì‹œë„í•˜ëŠ” ì‹ì˜ **ë‚´ê²°í•¨ì„±(fault-tolerance)** ìˆëŠ” ì„¤ê³„ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ **Human-in-the-loop** ê°œì…ì´ ìš©ì´í•˜ì—¬, ì¤‘ê°„ ìƒíƒœì—ì„œ ì‚¬ëŒì´ ê²€í† í•˜ê±°ë‚˜ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì´ì–´ì„œ ì‹¤í–‰ì‹œí‚¤ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê°„ë‹¨í•œ LangGraph ì˜ˆì œë¥¼ í†µí•´ ê°œë…ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ì½”ë“œì—ì„œëŠ” í•˜ë‚˜ì˜ íˆ´(tool)ì„ ê°€ì§„ React ìŠ¤íƒ€ì¼ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ê·¸ë˜í”„ ìƒì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤ (Anthropic Claude ëª¨ë¸ì„ ê°€ì •):

```python
%pip install langgraph # LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
from langgraph.prebuilt import create_react_agent

# ê°„ë‹¨í•œ íˆ´ í•¨ìˆ˜ ì •ì˜
def get_weather(city: str) -> str:
    # ì‹¤ì œ ì™¸ë¶€ API ëŒ€ì‹  ê³ ì •ëœ ë‹µë³€ì„ ë°˜í™˜ (ì˜ˆì‹œ)
    return f"{city}ì˜ ë‚ ì”¨ëŠ” í•­ìƒ ë§‘ìŠµë‹ˆë‹¤!"

# React ì—ì´ì „íŠ¸ ìƒì„± (Anthropic Claude API í™œìš© ê°€ì •)
agent = create_react_agent(
    model="anthropic:claude-2",    # Anthropic Claude ëª¨ë¸ (API í‚¤ í•„ìš”)
    tools=[get_weather],
    prompt="You are a helpful assistant."  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
)

# ì—ì´ì „íŠ¸ ì‹¤í–‰ (ëŒ€í™” í˜•ì‹ì˜ ì…ë ¥ ì „ë‹¬)
response = agent.invoke({"messages": [{"role": "user", "content": "ì„œìš¸ì˜ ë‚ ì”¨ê°€ ê¶ê¸ˆí•´"}]})
print(response)
```

ìœ„ ì½”ë“œì—ì„œ `create_react_agent` í•¨ìˆ˜ë¥¼ í†µí•´ **ReAct íŒ¨í„´**ì˜ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í–ˆë‹¤. `tools` ë¦¬ìŠ¤íŠ¸ì— `get_weather` í•¨ìˆ˜ë¥¼ ì œê³µí•˜ì—¬, ì—ì´ì „íŠ¸ê°€ í•„ìš” ì‹œ í•´ë‹¹ íˆ´ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤. ë§ˆì§€ë§‰ì— `agent.invoke(...)`ë¥¼ í˜¸ì¶œí•  ë•Œ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ê·¸ë˜í”„ ì…ë ¥ìœ¼ë¡œ ì£¼ë©´, LangGraphëŠ” ë‚´ë¶€ì ìœ¼ë¡œ \*\*ìƒíƒœ ê·¸ë˜í”„(StateGraph)\*\*ë¥¼ êµ¬ì„±í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ reasoning ê³¼ì •ì„ ì¶”ì í•œë‹¤. `response`ì—ëŠ” ì—ì´ì „íŠ¸ê°€ ìµœì¢…ì ìœ¼ë¡œ ì‚°ì¶œí•œ ë‹µë³€ì´ ë“¤ì–´ê°„ë‹¤ (ì˜ˆ: "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” í•­ìƒ ë§‘ìŠµë‹ˆë‹¤\!").

LangGraphë¥¼ í™œìš©í•˜ë©´ ì´ëŸ¬í•œ **ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°**ë¥¼ ë”ìš± ë³µì¡í•˜ê²Œ í™•ì¥í•  ìˆ˜ ìˆë‹¤. ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ë…¸ë“œ ê°„ **ë©”ì‹œì§€ ì „ë‹¬ ê²½ë¡œ**ë¥¼ ì •ì˜í•˜ì—¬, ì˜ˆì»¨ëŒ€ "ì§ˆë¬¸ ë¶„ì„ -\> ì •ë³´ ê²€ìƒ‰ -\> ë‹µ ì •ë¦¬" ê°™ì€ ê³¼ì •ì„ ê°ê¸° ë‹¤ë¥¸ ì—ì´ì „íŠ¸ê°€ ê·¸ë˜í”„ ìˆœì„œì— ë”°ë¼ ìˆ˜í–‰í•˜ê²Œ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. LangGraphì˜ **ì²´í¬í¬ì¸íŠ¸** ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ì¥ì‹œê°„ ì‹¤í–‰ë˜ëŠ” ì—ì´ì „íŠ¸ì˜ ì¤‘ê°„ ìƒíƒœë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥í•˜ì—¬, ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ìŒë¶€í„°ê°€ ì•„ë‹Œ ì¤‘ê°„ë¶€í„° ì¬ê°œí•  ìˆ˜ë„ ìˆë‹¤. ë˜í•œ **LangSmith** ë“±ì˜ ëª¨ë‹ˆí„°ë§ ë„êµ¬ì™€ í†µí•©í•´ ê·¸ë˜í”„ ì‹¤í–‰ì„ ì‹œê°í™”í•˜ê³  ë””ë²„ê¹…í•  ìˆ˜ ìˆë‹¤.

ì •ë¦¬í•˜ë©´, LangGraphëŠ” ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê°œë°œì—ì„œ **ì‹ ë¢°ì„±ê³¼ ì§€ì†ì„±**ì„ í™•ë³´í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ë‹¤. ì´ëŠ” ì›¹ ì„œë¹„ìŠ¤ë‚˜ ì—…ë¬´ ìë™í™” ë“±ì—ì„œ **ì˜¤ëœ ì‹œê°„ ë™ì•ˆ ì¤‘ë‹¨ ì—†ì´ ë™ì‘í•´ì•¼ í•˜ëŠ” ì—ì´ì „íŠ¸** íŒ€ì„ êµ¬ì¶•í•  ë•Œ ìœ ìš©í•˜ë‹¤. LangChain ìƒíƒœê³„ì™€ ì—°ë™ë˜ë¯€ë¡œ, ê¸°ì¡´ LangChain ì‚¬ìš©ìë¼ë©´ ì¹œìˆ™í•˜ê²Œ ìƒíƒœ ê¸°ë°˜ ì ‘ê·¼ì„ ë„ì…í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ë„ ìˆë‹¤.

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

- LangGraphì˜ í•µì‹¬ íŠ¹ì§•ì¸ **ìƒíƒœ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**ì€ ë¬´ì—‡ì´ë©°, ì–´ë–¤ ì¥ì ì„ ì œê³µí•˜ëŠ”ê°€?
- LangGraphì˜ **ì²´í¬í¬ì¸íŠ¸**ì™€ **Human-in-the-loop** ê¸°ëŠ¥ì´ ê¸´ í”„ë¡œì„¸ìŠ¤ë‚˜ ì¥ê¸°ê°„ ë™ì‘í•˜ëŠ” ì—ì´ì „íŠ¸ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ê°€?
- LangGraphë¡œ êµ¬ì¶•ëœ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì—ì„œ ì—ì´ì „íŠ¸ ê°„ ë¬´í•œ ëŒ€í™”ë‚˜ ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì–´ë–¤ ì„¤ê³„ê°€ í•„ìš”í•œê°€?

---

## 5. ì‹¤ìŠµ: BERT vs Mamba ëª¨ë¸ ë¹„êµ ì‹¤í—˜

ê³µê°œëœ í•œêµ­ì–´(NSMC)ìš© Mamba ë¶„ë¥˜ ëª¨ë¸ ë¶€ì¬ë¡œ, **IMDB ì˜ì–´ ë°ì´í„°ì…‹**ìœ¼ë¡œ ë¹„êµ ì‹¤í—˜ì„ êµ¬ì„±í–ˆë‹¤. MambaëŠ” ì‚¬ìš©ìê°€ ì œê³µí•œ ê³µê°œ ì²´í¬í¬ì¸íŠ¸ `trinhxuankhai/mamba_text_classification`ë¥¼, BERTëŠ” ê³µê°œ IMDB ë¶„ë¥˜ ë² ì´ìŠ¤ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ **ì •í™•ë„Â·ì¶”ë¡  ì†ë„Â·GPU ë©”ëª¨ë¦¬**ë¥¼ ë¹„êµí•œë‹¤.

### 5.1 í™˜ê²½ ì¤€ë¹„

```bash
# GPU ê¶Œì¥. Colab/ì¿ ë‹¤ í™˜ê²½ ê¶Œì¥
%pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
%pip -q install transformers datasets accelerate
```

### 5.2 ë°ì´í„°ì…‹ ë¡œë“œ (IMDB)

```python
from datasets import load_dataset

imdb = load_dataset("imdb")
imdb_test = imdb["test"]  # 25k samples

# ì†ë„ ë¹„êµë¥¼ ìœ„í•´ ì†Œìƒ˜í”Œ(ì˜ˆ: 1000ê°œ)ë§Œ í‰ê°€í•´ë„ ë¬´ë°©
imdb_test_small = imdb_test.select(range(1000))
```

### 5.3 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ

- **Mamba**: `trinhxuankhai/mamba_text_classification` (ì‚¬ìš©ì ì œê³µ ì¹´ë“œ)
- **BERT**: ê³µê°œëœ IMDB ë¶„ë¥˜ ëª¨ë¸ (ì˜ˆ: `textattack/bert-base-uncased-imdb`)

<!-- end list -->

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype  = torch.float16 if torch.cuda.is_available() else torch.float32

# 1) Mamba ë¶„ë¥˜ ëª¨ë¸ (IMDB í•™ìŠµ)
mamba_id = "trinhxuankhai/mamba_text_classification"
tok_mamba = AutoTokenizer.from_pretrained(mamba_id, use_fast=True)
model_mamba = AutoModelForSequenceClassification.from_pretrained(mamba_id).to(device)
model_mamba.eval()

# 2) BERT ë¶„ë¥˜ ëª¨ë¸ (ê³µê°œ IMDB ë² ì´ìŠ¤ë¼ì¸)
bert_id = "textattack/bert-base-uncased-imdb"
tok_bert = AutoTokenizer.from_pretrained(bert_id, use_fast=True)
model_bert = AutoModelForSequenceClassification.from_pretrained(bert_id).to(device)
model_bert.eval()

print("Loaded:", mamba_id, "|", bert_id)
```

### 5.4 í‰ê°€ í•¨ìˆ˜ (ì •í™•ë„Â·ì†ë„Â·ë©”ëª¨ë¦¬)

```python
import time, numpy as np

@torch.no_grad()
def evaluate(model, tokenizer, dataset, batch_size=16, max_length=256, warmup=2):
    # ì „ì²˜ë¦¬
    def enc(batch):
        encodings = tokenizer(
            batch["text"], truncation=True, padding="max_length",
            max_length=max_length, return_tensors="pt"
        )
        encodings["labels"] = torch.tensor(batch["label"])
        return encodings

    # ë¯¸ë¦¬ í…ì„œí™”
    texts = dataset["text"]
    labels = dataset["label"]

    # ë°°ì¹˜ ë‹¨ìœ„ ì¸ì½”ë”© (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ on-the-flyë„ ê°€ëŠ¥)
    encoded = [enc({"text": [t], "label": [l]}) for t, l in zip(texts, labels)]

    # ì›Œë°ì—…
    for _ in range(warmup):
        for i in range(0, len(encoded), batch_size):
            batch = {k: torch.cat([encoded[j][k] for j in range(i, min(i+batch_size, len(encoded)))], dim=0).to(device)
                     for k in encoded[0].keys()}
            _ = model(**batch)

    # ë©”ëª¨ë¦¬/ì‹œê°„ ì¸¡ì •
    if device == "cuda":
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()

    start = time.time()
    preds = []
    for i in range(0, len(encoded), batch_size):
        batch = {k: torch.cat([encoded[j][k] for j in range(i, min(i+batch_size, len(encoded)))], dim=0).to(device)
                 for k in encoded[0].keys()}
        logits = model(**batch).logits
        preds.extend(logits.argmax(dim=-1).detach().cpu().tolist())

    if device == "cuda":
        torch.cuda.synchronize()

    duration = time.time() - start
    acc = (np.array(preds) == np.array(labels)).mean().item()
    throughput = len(dataset) / duration
    peak_mem_mb = None

    if device == "cuda":
        peak_mem_mb = torch.cuda.max_memory_allocated() / (1024**2)

    return {"accuracy": acc, "sec": duration, "throughput": throughput, "peak_mem_mb": peak_mem_mb}

# í‰ê°€ ì‹¤í–‰ (1k ìƒ˜í”Œë¡œ)
res_mamba = evaluate(model_mamba, tok_mamba, imdb_test_small, batch_size=32, max_length=256)
res_bert  = evaluate(model_bert,  tok_bert,  imdb_test_small, batch_size=32, max_length=256)

print("Mamba results:", res_mamba)
print("BERT results:", res_bert)
```

### 5.5 ê²°ê³¼ ì˜ˆì‹œì™€ í•´ì„

**ì˜ˆì‹œ(í™˜ê²½ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ):**

- **Mamba(IMDB)** â†’ `{'accuracy': 0.94, 'throughput': X samples/sec, 'peak_mem_mb': Y MB}`

- **BERT(IMDB)** â†’ `{'accuracy': 0.94, 'throughput': Z samples/sec, 'peak_mem_mb': W MB}`

- **ì •í™•ë„(Accuracy)**: ì œê³µëœ Mamba ëª¨ë¸ ì¹´ë“œ ê¸°ì¤€ Val/Test Acc â‰ˆ 0.94ë¡œ, **BERT ë² ì´ìŠ¤ë¼ì¸ê³¼ ìœ ì‚¬**í•œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

- **ì¶”ë¡  ì†ë„(Throughput)**: ì…ë ¥ ê¸¸ì´ 256, ë°°ì¹˜ 32ì˜ ì¡°ê±´ì—ì„œëŠ” ì–‘ìí™” ì ìš© ì—¬ë¶€ë‚˜ í•˜ë“œì›¨ì–´ ìŠ¤í™ì— ë”°ë¼ í¸ì°¨ê°€ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì–´í…ì…˜ì˜ $O(N^2)$ ë³µì¡ë„ë¡œ ì¸í•´ BERTì˜ ì†ë„ê°€ ê¸‰ê²©íˆ ì €í•˜ë˜ëŠ” ë°˜ë©´, **Mambaì˜ ì„ í˜• ì‹œê°„($O(N)$) ì¥ì **ì´ ë‘ë“œëŸ¬ì§ˆ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.

- **ë©”ëª¨ë¦¬ ì‚¬ìš©(Peak Memory)**: MambaëŠ” ìƒíƒœ ê³µê°„ ëª¨ë¸(SSM) íŠ¹ì„±ìƒ ê±°ëŒ€í•œ ì–´í…ì…˜ í–‰ë ¬ì„ ìƒì„±í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ **ì´ë¡ ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë” ë†’ìŠµë‹ˆë‹¤**. ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì´ ì°¨ì´ëŠ” ë”ìš± ëª…í™•í•´ì§‘ë‹ˆë‹¤.

**ì£¼ì˜**

- ìœ„ ì½”ë“œëŠ” **1000ê°œ ìƒ˜í”Œ**ë¡œ ì†ë„/ë©”ëª¨ë¦¬ë¥¼ ë¹„êµí•©ë‹ˆë‹¤(ì „ì²´ 25kë„ ê°€ëŠ¥í•˜ë‚˜, ì‹¤ìŠµ ì‹œê°„ ê³ ë ¤).
- ì‹¤í—˜ì˜ ê³µì •ì„±ì„ ìœ„í•´ \*\*ë™ì¼í•œ `max_length`, `batch_size`, `dtype`\*\*ì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
- Colab T4/V100, A100, RTX 30/40 ê³„ì—´ ë“± **GPU ìŠ¤í™ì— ë”°ë¼ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤**.

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

1.  ì´ë²ˆ ë¹„êµì—ì„œ **ì •í™•ë„ê°€ ìœ ì‚¬**í–ˆìŒì—ë„, **ê¸´ ì…ë ¥ ê¸¸ì´**ì—ì„œ ë‘ ëª¨ë¸ì˜ **ì¶”ë¡  ì†ë„/ë©”ëª¨ë¦¬**ëŠ” ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆì§€ ì˜ˆì¸¡í•´ë³´ì„¸ìš”.
2.  ì‹¤ì„œë¹„ìŠ¤ ë„ì… ì‹œ Mambaì˜ **ì¥ì ê³¼ ë¦¬ìŠ¤í¬**(ì˜ˆ: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„±ìˆ™ë„, ìƒíƒœê³„, ë””ë²„ê¹… íˆ´)ë¥¼ ê°ê° 2ê°€ì§€ì”© ì ì–´ë³´ì„¸ìš”.
3.  ë™ì¼ ì½”ë“œë¡œ \*\*`max_length=512/1024`\*\*ë¡œ ë³€ê²½í•˜ì—¬ ì¬ì¸¡ì •í•´ë³´ê³ , Throughput/PeakMem ë³€í™”ë¥¼ ë³´ê³ ì„œë¡œ ìš”ì•½í•´ë³´ì„¸ìš”.

---

### ë¶€ë¡: ì¬í˜„ ê°€ëŠ¥í•œ ì „ì²´ ìŠ¤ë‹ˆí«

ì•„ë˜ ì…€ë§Œ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ Colab/GPUì—ì„œ **í•œ ë²ˆì— ë¹„êµ** ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
# 0) ì„¤ì¹˜
%pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
%pip -q install transformers datasets accelerate

# 1) ë°ì´í„°
from datasets import load_dataset
imdb = load_dataset("imdb")
imdb_test_small = imdb["test"].select(range(1000))

# 2) ëª¨ë¸/í† í¬ë‚˜ì´ì €
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
device = "cuda" if torch.cuda.is_available() else "cpu"
mamba_id = "trinhxuankhai/mamba_text_classification"
bert_id  = "textattack/bert-base-uncased-imdb"

tok_mamba = AutoTokenizer.from_pretrained(mamba_id, use_fast=True)
tok_bert  = AutoTokenizer.from_pretrained(bert_id,  use_fast=True)

model_mamba = AutoModelForSequenceClassification.from_pretrained(mamba_id).to(device).eval()
model_bert  = AutoModelForSequenceClassification.from_pretrained(bert_id).to(device).eval()

# 3) í‰ê°€
import time, numpy as np

@torch.no_grad()
def evaluate(model, tokenizer, dataset, batch_size=32, max_length=256, warmup=1):
    def enc(batch):
        encodings = tokenizer(batch["text"], truncation=True, padding="max_length",
                              max_length=max_length, return_tensors="pt")
        encodings["labels"] = torch.tensor(batch["label"])
        return encodings

    texts, labels = dataset["text"], dataset["label"]
    encoded = [enc({"text":[t], "label":[l]}) for t,l in zip(texts, labels)]

    for _ in range(warmup):
        for i in range(0, len(encoded), batch_size):
            batch = {k: torch.cat([encoded[j][k] for j in range(i, min(i+batch_size, len(encoded)))], dim=0).to(device)
                     for k in encoded[0].keys()}
            _ = model(**batch)

    if device == "cuda":
        torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()

    start = time.time()
    preds = []
    for i in range(0, len(encoded), batch_size):
        batch = {k: torch.cat([encoded[j][k] for j in range(i, min(i+batch_size, len(encoded)))], dim=0).to(device)
                 for k in encoded[0].keys()}
        logits = model(**batch).logits
        preds.extend(logits.argmax(dim=-1).detach().cpu().tolist())

    if device == "cuda": torch.cuda.synchronize()
    duration = time.time() - start
    acc = (np.array(preds) == np.array(labels)).mean().item()
    thr = len(dataset) / duration
    peak = torch.cuda.max_memory_allocated()/(1024**2) if device=="cuda" else None
    return {"accuracy": acc, "sec": duration, "throughput": thr, "peak_mem_mb": peak}

res_mamba = evaluate(model_mamba, tok_mamba, imdb_test_small)
res_bert  = evaluate(model_bert,  tok_bert,  imdb_test_small)

print("Mamba:", res_mamba)
print("BERT :", res_bert)
```

---

## 6. ì‹¤í—˜ ì •ë¦¬ ë° ì‹œì‚¬ì 

**BERT vs Mamba ë¹„êµ ì‹¤í—˜**ì„ í†µí•´ ë‘ ì•„í‚¤í…ì²˜ì˜ íŠ¹ì„±ê³¼ ì¥ë‹¨ì ì„ ì‹¤ì¦ì ìœ¼ë¡œ ì‚´í´ë³´ì•˜ë‹¤. ì´ ì‹¤í—˜ì€ ë‹¨ìˆœíˆ ë‘ ëª¨ë¸ì˜ ìš°ì—´ì„ ê°€ë¦¬ëŠ” ê²ƒì„ ë„˜ì–´, ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ í˜„ì¬ì™€ ë¯¸ë˜ë¥¼ ì¡°ë§í•˜ëŠ” ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•œë‹¤.

### ì–´ë–¤ ëª¨ë¸ì´ ì–´ë–¤ ìƒí™©ì— ì í•©í•œê°€?

- **ì§§ì€ ì‹œí€€ìŠ¤ ë° ê²€ì¦ëœ ì„±ëŠ¥: BERT (Transformer)**

  - **ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´**ê°€ ìˆ˜ë°± í† í° ë‚´ì™¸ì˜ ì§§ì€ ì‘ì—…(ë‹¨ë¬¸ ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ë‹¨ë‹µí˜• QA ë“±)ì—ì„œëŠ” BERTì™€ ê°™ì€ Transformer ì•„í‚¤í…ì²˜ê°€ ì—¬ì „íˆ ê°•ë ¥í•˜ê³  íš¨ìœ¨ì ì´ë‹¤.
  - ë°©ëŒ€í•œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ê³¼ ì„±ìˆ™í•œ íŒŒì¸íŠœë‹ ìƒíƒœê³„ ë•ë¶„ì— ë†’ì€ ì •í™•ë„ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë‹¬ì„±í•˜ê¸° ì‰½ê³ , ì§§ì€ ì…ë ¥ì—ì„œëŠ” ì¶”ë¡  ì§€ì—° ì‹œê°„ë„ ê²½ìŸë ¥ì´ ìˆë‹¤.

- **ì´ˆì¥ë¬¸ë§¥(Long-Context) ì²˜ë¦¬ ë° íš¨ìœ¨ì„±: Mamba (SSM)**

  - ìˆ˜ì²œì—ì„œ ìˆ˜ë§Œ í† í°ì— ì´ë¥´ëŠ” ê¸´ ë¬¸ì„œ ë‹¨ìœ„ì˜ ì‘ì—…(ì¥ë¬¸ì„œ ìš”ì•½, ì†Œì„¤ ê°ì • ë¶„ì„, ì½”ë“œ ìƒì„± ë“±)ì—ì„œ Mambaì˜ ì§„ê°€ê°€ ë“œëŸ¬ë‚œë‹¤. Transformerì˜ $O(N^2)$ ë³µì¡ë„ëŠ” ì´ëŸ¬í•œ ì‘ì—…ì—ì„œ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í­ë°œì ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ ì‚¬ì‹¤ìƒ ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, Mambaì˜ $O(N)$ ì„ í˜• ë³µì¡ë„ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ ì„±ëŠ¥ ì €í•˜ ì—†ì´ íš¨ìœ¨ì ì¸ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
  - MambaëŠ” **ìµœëŒ€ 100ë§Œ í† í°**ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥í•¨ì„ ì‹œì—°í•˜ë©°, ì´ˆì¥ë¬¸ë§¥ LLM ì‹œëŒ€ë¥¼ ì—´ í•µì‹¬ ê¸°ìˆ ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤.

### ì„œë¹„ìŠ¤/í”„ë¡œë•ì…˜ ì ìš© ì‹œì‚¬ì 

- **í˜„ì¬ì˜ ì•ˆì •ì„± vs. ë¯¸ë˜ì˜ ê°€ëŠ¥ì„±**

  - **í˜„ì¬ í”„ë¡œë•ì…˜ í™˜ê²½**ì—ì„œëŠ” Transformer ê³„ì—´(BERT, GPT ë“±) ëª¨ë¸ì´ ì„±ëŠ¥, ì•ˆì •ì„±, ê°œë°œ ë„êµ¬ ì§€ì› ì¸¡ë©´ì—ì„œ ì›”ë“±íˆ ì„±ìˆ™í•˜ì—¬ ë„ë¦¬ ì“°ì´ê³  ìˆë‹¤.
  - MambaëŠ” ë§¤ìš° ìœ ë§í•œ ê¸°ìˆ ì´ì§€ë§Œ, ì•„ì§ **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì›, ì»¤ë®¤ë‹ˆí‹°, ì‚¬ì „í•™ìŠµ ëª¨ë¸ í’€**ì´ Transformerë§Œí¼ í’ë¶€í•˜ì§€ ì•Šë‹¤. ë”°ë¼ì„œ í˜„ì—…ì— ì¦‰ì‹œ ë„ì…í•˜ê¸°ì—ëŠ” ì•ˆì •ì„± ê²€ì¦ê³¼ ê°™ì€ ë¦¬ìŠ¤í¬ê°€ ë”°ë¥¼ ìˆ˜ ìˆë‹¤.

- **ìƒˆë¡œìš´ ê¸°íšŒì˜ ì°½ì¶œ**

  - Mambaì™€ ê°™ì€ SSM ì•„í‚¤í…ì²˜ëŠ” ê¸°ì¡´ì— ë©”ëª¨ë¦¬ë‚˜ ì†ë„ í•œê³„ë¡œ ë¶ˆê°€ëŠ¥í–ˆë˜ ì„œë¹„ìŠ¤ì— ëŒíŒŒêµ¬ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, **ê¸´ ë²•ë¥ /ì˜ë£Œ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì§ˆì˜ì‘ë‹µí•˜ëŠ” ì„œë¹„ìŠ¤**ë‚˜ **ìˆ˜ì‹­ í˜ì´ì§€ì— ë‹¬í•˜ëŠ” ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ëª¨ë‘ ê¸°ì–µí•˜ëŠ” ì±—ë´‡** ë“±ì—ì„œ MambaëŠ” *ê²Œì„ ì²´ì¸ì €*ê°€ ë  ì ì¬ë ¥ì´ ì¶©ë¶„í•˜ë‹¤.

### ë¯¸ë˜ ì „ë§: í•˜ì´ë¸Œë¦¬ë“œì™€ ìƒí˜¸ë³´ì™„

í–¥í›„ì—ëŠ” **í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸**(ì˜ˆ: _Jamba_ - Transformerì™€ Mambaì˜ ì¥ì ì„ ê²°í•©í•œ êµ¬ì¡°)ì´ë‚˜ ë‹¤ë¥¸ ì„ í˜• ì‹œê°„ ì•„í‚¤í…ì²˜ì™€ì˜ ê²½ìŸì„ í†µí•´ ê¸°ìˆ ì´ ë”ìš± ë°œì „í•  ê²ƒì´ë‹¤. í˜„ì¬ë¡œì„œëŠ” **Transformerì˜ ë²”ìš©ì„± vs. Mambaì˜ íŠ¹ìˆ˜ì„±** êµ¬ë„ë¡œ ë³¼ ìˆ˜ ìˆìœ¼ë©°, ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë‘ ì ‘ê·¼ë²•ì„ **ìƒí˜¸ë³´ì™„ì ìœ¼ë¡œ í™œìš©**í•˜ëŠ” ë°©ì•ˆì´ ìœ ë ¥í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¼ë°˜ì ì¸ ì§§ì€ ëŒ€í™”ëŠ” Transformerë¡œ ì²˜ë¦¬í•˜ë‹¤ê°€, ì‚¬ìš©ìê°€ ê¸´ ë¬¸ì„œë¥¼ ì²¨ë¶€í•˜ë©° ì§ˆë¬¸í•˜ëŠ” íŠ¹ì • ìƒí™©ì—ì„œëŠ” Mamba ëª¨ë“œë¡œ ì „í™˜í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ì§€ëŠ¥í˜• ì‹œìŠ¤í…œì„ êµ¬ìƒí•  ìˆ˜ ìˆë‹¤.

ê²°ë¡ ì ìœ¼ë¡œ, **BERT**ì™€ **Mamba**ëŠ” ê°ìì˜ ëª…í™•í•œ ê°•ì ì„ ë°”íƒ•ìœ¼ë¡œ **ì„œë¡œ ë‹¤ë¥¸ ìš©ë„**ë¥¼ ê°–ëŠ”ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê°œë°œìëŠ” í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œì˜ í•µì‹¬ ì œì•½ ì¡°ê±´, íŠ¹íˆ **ì‹œí€€ìŠ¤ ê¸¸ì´**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì í•©í•œ ì•„í‚¤í…ì²˜ë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤. ì´ë²ˆ ì‹¤í—˜ì€ **ì´ˆì¥ë¬¸ë§¥ê³¼ ê³ íš¨ìœ¨ ì¶”ë¡ ì„ ìœ„í•œ ì•„í‚¤í…ì²˜ í˜ì‹ ì´ í˜„ì‹¤í™”ë˜ê³  ìˆìŒ**ì„ ë³´ì—¬ì£¼ëŠ” ì˜ë¯¸ ìˆëŠ” ì´ì •í‘œë‹¤.

### ì²´í¬í¬ì¸íŠ¸ ì§ˆë¬¸

- BERTì™€ Mamba ëª¨ë¸ì˜ **ì‹œê°„ ë³µì¡ë„** ì°¨ì´ì ì€ ë¬´ì—‡ì´ë©°, ì´ê²ƒì´ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì—ì„œ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
- ë³¸ ì‹¤í—˜ì—ì„œ ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ 512ë‚˜ 1024 í† í°ìœ¼ë¡œ ëŠ˜ë ¤ ì‹¤í—˜ì„ ë°˜ë³µí•˜ë©´ **ì¶”ë¡  ì†ë„**ì™€ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆì§€ ì˜ˆì¸¡í•´ë³´ë¼.
- Mamba ëª¨ë¸ì„ í˜„ì¬ ë‹¹ì¥ í˜„ì—… ì‹œìŠ¤í…œì— ë„ì…í•˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€? ë°˜ëŒ€ë¡œ ë¯¸ë˜ì— Mambaê°€ ê°ê´‘ë°›ì„ ë§Œí•œ í™œìš© ì‹œë‚˜ë¦¬ì˜¤ëŠ” ì–´ë–¤ ê²ƒì´ ìˆëŠ”ê°€?

---

## ì°¸ê³ ìë£Œ

### ì£¼ìš” ë…¼ë¬¸ ë° ì—°êµ¬ ìë£Œ

- PyTorch ê³µì‹ ë¸”ë¡œê·¸ â€“ _Better Performance with torch.compile_ (2023)
- Tri Dao ë¸”ë¡œê·¸ â€“ _"FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-Precision"_
- Databricks DSPy ì†Œê°œ â€“ _Programming, not prompting_

### ê¸°ìˆ  ë¬¸ì„œ ë° êµ¬í˜„ì²´

- Hugging Face Transformers ë¬¸ì„œ & íŠœí† ë¦¬ì–¼
- Deepset Haystack ë¬¸ì„œ â€“ _Flexible Open Source QA Framework_
- CrewAI ê³µì‹ ë¬¸ì„œ â€“ _Role-based Autonomous Agent Teams_
- LangGraph ê³µì‹ ë¬¸ì„œ â€“ _State-based Multi-Agent Orchestration_

### ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤ ë° ë¸”ë¡œê·¸

- "torch.compile: A Deep Dive into PyTorch's Compiler" - PyTorch Blog
- "FlashAttention-3: The Next Generation of Attention Optimization" - Technical Blog
- "AI Agent Frameworks: A Comprehensive Comparison" - Medium
- "DSPy: The Future of Prompt Engineering" - Databricks Blog
