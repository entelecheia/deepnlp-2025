
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 1: Transformer 및 차세대 아키텍처 &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week01/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A" href="qna.html" />
    <link rel="prev" title="딥러닝자연어처리 (131307379A)" href="../index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          한국어 <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    딥러닝자연어처리 (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Week 1: Transformer 및 차세대 아키텍처</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="qna.html">Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x와 최신 딥러닝 프레임워크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: 현대적 PEFT 기법을 활용한 효율적 파인튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: 고급 프롬프트 기법과 최적화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM 평가 패러다임과 벤치마크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: 멀티모달 NLP의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: 초장문맥 처리와 효율적 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: 핵심 복습 및 최신 동향</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week09/index.html">Week 9: 고급 RAG 아키텍처</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week10/index.html">Week 10: 정렬 기법의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week11/index.html">Week 11: 프로덕션 에이전트 시스템</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week12/index.html">Week 12: AI 규제와 책임 있는 AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week13/index.html">Week 13: 온톨로지와 AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workshops/index.html">LLM From Scratch 워크숍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workshops/week01.html">1주차 워크숍: LLM 개요 및 개발 환경 구축</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">프로젝트 운영 가이드라인</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">강의계획서</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/ko/week01/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek01/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week01/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 1: Transformer 및 차세대 아키텍처</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">1. Transformer 아키텍처의 기본 구조</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention 연산 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-selective-state-space-model">2. Mamba 아키텍처 – Selective State Space Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba">Mamba 구조 및 사용법 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv-rnn">3. RWKV 아키텍처 – RNN과 유사한 구조의 효율적 처리</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv">RWKV 모델 사용 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-moe-transformer-mamba">4. Jamba 아키텍처 – MoE 기반 Transformer+Mamba 하이브리드</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">소개</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">주요 특징</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba">Jamba의 모델 구조</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">대규모 컨텍스트 윈도우 및 비용-효율성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe-mixture-of-experts">MoE(Mixture of Experts) 활용</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">사용 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Jamba 모델 활용 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">5. 아키텍처별 성능 비교</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm">6. 최신 오픈소스 LLM 소개 및 특징</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">Llama 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixtral-87b">Mixtral 8×7B</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qwen2-72b">Qwen2-72B</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">7. 실습 지침</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">참고자료</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">주요 논문 및 연구 자료</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">기술 문서 및 구현체</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">온라인 리소스 및 블로그</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">벤치마크 및 평가 자료</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-1-transformer">
<h1>Week 1: Transformer 및 차세대 아키텍처<a class="headerlink" href="#week-1-transformer" title="Link to this heading">#</a></h1>
<section id="transformer">
<h2>1. Transformer 아키텍처의 기본 구조<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h2>
<p>Transformer는 <strong>Self-Attention</strong> 메커니즘을 기반으로 한 모델로, 입력 문장을 인코더-디코더(encoder/decoder) 구조로 처리한다. 인코더는 입력 시퀀스를 인코딩하여 컨텍스트(Context)를 생성하고, 디코더는 이 컨텍스트를 참고해 출력 시퀀스를 생성한다. Self-Attention(자가 어텐션)은 한 토큰이 시퀀스 내 다른 모든 토큰과의 관계(유사도)를 계산하여 자기 자신의 표현을 조정하는 방법이다. 이를 통해 <strong>병렬 처리</strong>가 가능해져 RNN 기반 모델보다 학습이 빠르고, 긴 의존 관계도 효과적으로 학습할 수 있다. 다만 <strong>각 토큰 쌍마다 연산</strong>이 필요하여 Attention 연산 비용이 <span class="math notranslate nohighlight">\(O(n^2)\)</span>로 늘어나므로 시퀀스 길이가 길어질수록 비효율적이다. 특히 <strong>추론(inference)</strong> 시에는 토큰을 한 개씩 생성하면서 이전 모든 토큰과의 어텐션을 계산해야 하므로 시퀀스 길이 <em>L</em>에 대해 약 <span class="math notranslate nohighlight">\(L^2\)</span>에 비례하는 연산이 요구된다. 이로 인해 <strong>긴 문맥 처리</strong> 시 속도가 느려지고 메모리 사용량도 선형적으로 증가하여 큰 한계로 작용한다.</p>
<p><em>Transformer 전체 구조.</em> 왼쪽은 인코더, 오른쪽은 디코더를 나타낸다. 각 인코더 블록은 <strong>Multi-Head Self-Attention</strong>과 피드포워드(FFN)로 구성되며, 디코더 블록은 여기에 <strong>마스킹된(masked) Self-Attention</strong>과 <strong>인코더-디코더 Attention</strong>(교차 어텐션)이 추가된다. 어텐션 모듈은 모든 이전 토큰들을 참조하여 현재 토큰의 <strong>컨텍스트</strong>를 형성한다. 트랜스포머는 순환구조가 없어 병렬화에 유리하고, <strong>멀티-헤드 어텐션</strong>으로 다양한 표현 공간에서 패턴을 학습한다. 그러나 어텐션 연산의 <strong>쿼드러플릭(Quadratic) 복잡도</strong> 때문에 <strong>메모리와 계산량이 시퀀스 길이에 따라 급증</strong>하는 단점이 있다.</p>
<section id="self-attention">
<h3>Self-Attention 연산 예시 코드<a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h3>
<p>아래 코드는 PyTorch를 사용하여 Self-Attention의 핵심 연산을 구현한 예시다 (단일 배치, 시퀀스 길이 <em>L</em>, 차원 <em>d</em>인 경우). 각 토큰의 쿼리 <span class="math notranslate nohighlight">\(Q\)</span>, 키 <span class="math notranslate nohighlight">\(K\)</span>, 밸류 <span class="math notranslate nohighlight">\(V\)</span> 벡터를 이용해 <strong>어텐션 가중치</strong>를 구한 뒤, 가중합으로 출력값을 계산한다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span><span class="o">,</span><span class="w"> </span><span class="nn">math</span>

<span class="n">L</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">64</span>  <span class="c1"># 시퀀스 길이 L=5, 임베딩 차원 d=64</span>
<span class="c1"># 예시 Q, K, V 행렬 (batch=1, seq_len=L, dim=d)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="c1"># 1. 어텐션 스코어 행렬 계산 (Q * K^T / sqrt(d))</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>  <span class="c1"># (1, L, L)</span>
<span class="c1"># 2. Softmax로 어텐션 가중치 확률 분포 얻기</span>
<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                 <span class="c1"># (1, L, L)</span>
<span class="c1"># 3. 가중치를 V에 곱하여 출력 계산</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>                       <span class="c1"># (1, L, d)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1, 5, 64])</span>
</pre></div>
</div>
<p><strong>코드 해설:</strong> 위 코드에서 scores는 각 토큰 <em>i</em>가 다른 토큰 <em>j</em>에 주의를 얼마나 기울이는지 나타내는 어텐션 점수 행렬이다. softmax를 통해 각 <em>i</em>에 대해 합이 1인 확률 분포로 정규화하면 attn_weights가 된다. 마지막으로 이 가중치를 각 <em>j</em>의 값 벡터 <span class="math notranslate nohighlight">\(V_j\)</span>에 곱해서 합산하면 각 위치 <em>i</em>에 대한 Self-Attention 출력이 계산된다.</p>
</section>
<section id="id1">
<h3>체크포인트 질문<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Transformer에서 <strong>Self-Attention</strong>이 가지는 주요 장점은 무엇인가? 또한 <strong>추론 시 병목 현상</strong>은 어떤 부분에서 발생하는가?</p></li>
<li><p>Transformer <strong>인코더-디코더 구조</strong>와 <strong>GPT 같은 디코더-Only 구조</strong>의 차이를 설명하라.</p></li>
<li><p>시퀀스 길이 <em>L</em>에 대해 Transformer의 <strong>시간 복잡도</strong>와 <strong>공간 복잡도</strong>는 각각 어떻게 스케일링되는가?</p></li>
</ul>
</section>
</section>
<section id="mamba-selective-state-space-model">
<h2>2. Mamba 아키텍처 – Selective State Space Model<a class="headerlink" href="#mamba-selective-state-space-model" title="Link to this heading">#</a></h2>
<p><strong>Mamba</strong>는 2024년에 제안된 새로운 시퀀스 모델로, <strong>Selective State Space Model</strong> (선택적 상태 공간 모델)을 활용하여 <strong>Transformer의 대안</strong>을 제시한다. Mamba의 핵심 아이디어는 <strong>연속시간 상태 공간 모델</strong>(SSM)을 언어 모델링에 적용하되, <strong>입력에 따라 동적으로 상태 전이를 제어</strong>하는 것이다. 이를 통해 각 시점에 <strong>과거 정보를 선별적으로 유지하거나 잊도록</strong> 만들어, <strong>토큰 간 내용 기반(content-based) 의존성</strong>을 표현한다.</p>
<p>Mamba는 <strong>순환신경망</strong>(RNN)처럼 동작하여 이전 <strong>hidden state</strong>(은닉 상태)를 갱신하면서 한 토큰씩 순차 처리하지만, <strong>효율적인 알고리즘</strong>을 통해 병렬화 제약을 극복했다. 어텐션이나 거대한 MLP 피드포워드 층 없이도 토큰들 간 정보를 주고받을 수 있도록 <strong>선택적 스캔 알고리즘</strong>과 <strong>하드웨어 친화적인 병렬화 기법</strong>을 도입하여 <strong>긴 시퀀스도 선형 시간에 처리</strong>할 수 있게 최적화했다. 그 결과 Mamba는 <strong>추론 시 5배 이상 높은 토큰 처리량</strong>을 보이며, <strong>시퀀스 길이에 선형적으로(scale linearly) 증가</strong>하는 성능을 달성했다. 실제로 <strong>Mamba-3B 모델</strong>은 동일 크기 Transformer보다 성능이 우수하고, <strong>2배 큰 Transformer와 맞먹는 성능</strong>을 보였다고 보고되었다.</p>
<p>Mamba의 <strong>블록 구조</strong>는 Transformer의 블록과 유사하게 여러 층 적층 형태로 이루어져 있다. 다만 내부 구성은 다음과 같다:</p>
<ul class="simple">
<li><p>입력 임베딩에 <strong>선형 확장</strong>(projection)을 적용해 차원을 늘린다.</p></li>
<li><p>토큰 간 독립 계산을 막기 위해 <strong>지역 컨볼루션</strong>(layer)을 통과시켜 인접 토큰 정보를 섞어준다.</p></li>
<li><p>그런 다음 <strong>Selective SSM 레이어</strong>가 적용되어, <strong>HiPPO</strong> 초기화된 상태 행렬 <span class="math notranslate nohighlight">\(A\)</span>를 기반으로 <strong>연속 상태 업데이트</strong>를 수행한다. 이 단계에서 <strong>선택적 상태 압축</strong> 알고리즘이 적용되어 중요한 정보는 유지하고 불필요한 정보는 잊게 된다.</p></li>
<li><p>마지막으로 <strong>정규화</strong>(norm)와 출력단 Softmax 등을 거쳐 토큰 출력이 산출된다.</p></li>
</ul>
<p>이러한 구성으로 <strong>Mamba 블록</strong>은 <strong>Self-Attention을 대체</strong>하면서도 <strong>MLP와 유사한 국소 연산</strong>을 포함하여 <strong>토큰 내 계산</strong>(computation within token)도 수행한다. 여러 개의 Mamba 블록을 쌓으면 Transformer와 동일하게 <strong>딥한 시퀀스 모델</strong>을 구축할 수 있다. Mamba는 이처럼 <strong>어텐션 병목을 제거</strong>함으로써 <strong>사실상 무한대에 가까운 컨텍스트 길이</strong>도 실용적으로 다룰 수 있고, <strong>학습 및 추론 속도 모두 크게 향상</strong>되었다.</p>
<p><strong>참고:</strong> “Selective” SSM에서는 상태공간의 <strong>계수</strong>(상태 전이 행렬 등)가 토큰 값의 함수로 동적으로 결정된다. 이는 기존 고정 SSM의 한계를 극복해 <strong>이산 토큰 데이터</strong>(자연어)에도 높은 성능을 내도록 한 비결이다.</p>
<section id="mamba">
<h3>Mamba 구조 및 사용법 예시 코드<a class="headerlink" href="#mamba" title="Link to this heading">#</a></h3>
<p>Mamba는 현재 파이썬 패키지로 제공되어 손쉽게 활용할 수 있다. 아래는 mamba-ssm 라이브러리를 통해 <strong>Mamba 블록</strong>을 생성하고 텐서 입력을 처리하는 예시다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="c1"># Mamba 패키지 임포트</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mamba_ssm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mamba</span>

<span class="n">batch</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span>  <span class="c1"># 예시: 배치=2, 시퀀스=64, 차원=16</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>   <span class="c1"># 임의 입력 텐서 (GPU사용)</span>

<span class="c1"># Mamba 블록 모델 생성</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Mamba</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>  <span class="c1"># 모델 차원 (임베딩 차원)</span>
    <span class="n">d_state</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>   <span class="c1"># SSM 상태 차원 (확장 정도)</span>
    <span class="n">d_conv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>     <span class="c1"># 국소 컨볼루션 크기</span>
    <span class="n">expand</span><span class="o">=</span><span class="mi">2</span>      <span class="c1"># 내부 채널 확장 비율</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Mamba 블록으로 입력 처리</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 출력 텐서 크기 확인 (batch, length, dim 이어야 함)</span>
</pre></div>
</div>
<p>위 코드에서 Mamba(…)로 <strong>하나의 Mamba 블록</strong>을 생성한다. 매개변수 d_model, d_state 등은 논문에서 제시된 권장 값을 사용하였다. model(x)를 호출하면 입력 x에 대해 <strong>선택적 상태 공간 연산</strong>이 수행되어 동일한 크기의 출력 y를 반환한다. (실제 언어 모델에서는 이러한 블록을 여러 층 쌓고, 출력에 LM 헤드(head)를 붙여 어휘 분포를 예측하게 된다.)</p>
<p><strong>실습 팁:</strong> Mamba는 GPU에서 동작하도록 구현되어 있으므로, 위 예시처럼 .to(“cuda”)로 텐서와 모델을 옮겨야 성능을 제대로 활용할 수 있다. 또한 pip install mamba-ssm으로 패키지를 설치하고, NVIDIA CUDA 11.6 이상 환경이 필요하다.</p>
</section>
<section id="id2">
<h3>체크포인트 질문<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Mamba가 <strong>Transformer 대비</strong> 갖는 가장 큰 장점은 무엇인가? Mamba는 어떻게 <strong>어텐션의 <span class="math notranslate nohighlight">\(O(n^2)\)</span> 병목</strong>을 피할 수 있었는지 설명하라.</p></li>
<li><p>Mamba의 Selective SSM에서 **”선택적”**인 동작은 무엇을 뜻하는가? 이로 인해 언어 모델에서 어떤 효과를 얻을 수 있었는가?</p></li>
<li><p>Mamba-3B 모델이 보여준 성능 관련 특징을 언급하라 (예: 동일 크기 Transformer와의 비교, 두배 크기 Transformer와의 비교 등).</p></li>
</ul>
</section>
</section>
<section id="rwkv-rnn">
<h2>3. RWKV 아키텍처 – RNN과 유사한 구조의 효율적 처리<a class="headerlink" href="#rwkv-rnn" title="Link to this heading">#</a></h2>
<p><strong>RWKV</strong>는 **RNN(Recurrent Neural Network)**과 <strong>Transformer</strong>의 장점을 결합한 <strong>혼합형 아키텍처</strong>다. 이름 <em>RWKV</em>는 네트워크의 4가지 주요 파라미터인 <strong>Receptance (R)</strong>, <strong>Weight (W)</strong>, <strong>Key (K)</strong>, **Value (V)**에서 유래하였으며, 각각 <strong>과거 정보 수용 게이트</strong>, <strong>지수적 시간 가중치</strong>, <strong>키</strong>, <strong>값 벡터</strong>의 역할을 한다. RWKV는 내부적으로 <strong>시간 축 처리</strong>와 <strong>채널(피드포워드) 처리</strong>를 교대로 수행하는 RNN 구조를 가지며, 각 단계에서 <strong>과거 상태를 지수 가중치로 감쇠시키며</strong> Key/Value 정보를 누적하는 방식으로 동작한다. 이를 통해 <strong>어텐션과 유사한 효과</strong>를 내면서도 매 토큰 처리 비용을 선형으로 유지한다.</p>
<p>RWKV의 <strong>큰 특징 두 가지</strong>는 다음과 같다:</p>
<ul class="simple">
<li><p><strong>Transformer처럼 병렬 학습 가능:</strong> 기존 RNN은 순차적(Time-step)으로만 학습 가능하여 병렬화에 어려움이 있었으나, RWKV는 <strong>어텐션 형태의 수식으로 변환</strong>하여 <strong>학습 시 병렬화</strong>를 달성했다. 즉, <strong>훈련 단계에서는 Transformer와 동일하게</strong> 전체 시퀀스를 한꺼번에 처리하고(특수한 선형 어텐션 형태를 사용), <strong>추론 단계에서는 RNN처럼</strong> 토큰을 하나씩 생성한다. 이를 통해 <strong>훈련 효율</strong>과 <strong>추론 효율</strong>을 모두 얻었다.</p></li>
<li><p><strong>무한에 가까운 컨텍스트:</strong> RWKV는 RNN 계열답게 <strong>하나의 고정된 은닉 상태</strong>로 문맥을 요약하며 계속 업데이트하므로, 이론적으로 <strong>문맥 길이의 제한이 없다</strong>. 새로운 토큰을 생성할 때 이전 토큰들의 정보를 거대한 KV캐시로 모두 보관할 필요 없이, <strong>이전 스텝의 state만 유지</strong>하면 된다. 따라서 메모리 사용량이 입력 길이에 거의 영향받지 않고, 긴 문맥도 다룰 수 있다 (물론 실제로는 훈련 시 사용된 문맥 길이 이상에 대해 일반화 성능이 제한될 수 있다).</p></li>
</ul>
<p>RWKV의 성능은 <strong>Transformer와 견줄 만큼 우수</strong>한 것으로 나타났다. 커뮤니티 주도로 공개된 RWKV 모델들은 <strong>최대 14억~14십억 파라미터 규모</strong>까지 개발되었고, 동일한 파라미터 규모의 GPT 계열 Transformer와 <strong>비슷한 언어모델 능력</strong>을 보였다. 특히 RWKV-14B 모델은 14B 파라미터 GPT와 유사한 <strong>척도 상 scaling law</strong>을 따르며 성능 향상을 보였고, 대규모(예: 175B)로 확장하는 연구도 진행되고 있다.</p>
<p><strong>메시지(정보) 처리 방식의 효율성</strong>도 RWKV의 강점이다. 각 레이어는 자신만의 은닉 상태를 갖고 토큰 단위로 갱신되는데, <strong>Time-mix</strong>라는 구조를 통해 **지수적 이동평균(EMA)**으로 과거 키/값의 누적을 유지하고, <strong>Channel-mix</strong>를 통해 FFN과 유사한 <strong>비선형 변환</strong>을 적용한다. 이러한 구성 덕분에 <strong>이전 토큰들의 영향력이 멀리까지 전달</strong>되며(긴 의존성 처리), 불필요하게 오래 지속되지 않도록 자연스럽게 **감쇠(decay)**된다. 또한 RWKV는 어텐션과 달리 <strong>토큰 수가 증가해도 토큰당 계산량은 일정</strong>하므로, 컨텍스트 길이가 길어져도 <strong>처리 속도 저하가 완만</strong>하다.</p>
<p><strong>알아두기:</strong> RWKV의 경우 <strong>학습 시</strong>에는 내부적으로 어텐션 형태로 동작하므로 <span class="math notranslate nohighlight">\(O(n^2)\)</span> 연산을 사용하지만 GPU 병렬화로 상쇄된다. <strong>추론 시</strong>에는 실제 RNN처럼 <span class="math notranslate nohighlight">\(O(n)\)</span>로 한 토큰씩 처리한다. 따라서 아주 긴 문맥을 다룰 때 Transformer 대비 <strong>메모리 및 시간 면에서 큰 이점</strong>을 가진다.</p>
<section id="rwkv">
<h3>RWKV 모델 사용 예시 코드<a class="headerlink" href="#rwkv" title="Link to this heading">#</a></h3>
<p>RWKV는 Hugging Face transformers 라이브러리에 통합되어 있어, 기존 GPT 모델 다루듯 사용할 수 있다. 예를 들어 공개된 <strong>RWKV-4 169M</strong> 모델을 불러와 텍스트 생성을 해보자:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># 1. 토크나이저와 모델 불러오기</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;RWKV/rwkv-4-169m-pile&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;RWKV/rwkv-4-169m-pile&quot;</span><span class="p">)</span>

<span class="c1"># 2. 입력 프롬프트 정의</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Once upon a time, &quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="c1"># 3. 텍스트 생성</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>
</div>
<p>위 코드는 “Once upon a time, “라는 프롬프트로 시작하는 텍스트를 RWKV 모델로 생성하는 예다. RWKV 계열은 모델 크기가 클 경우 GPU 메모리에 효율적이며, 필요에 따라 <a class="reference external" href="http://model.to">model.to</a>(‘cuda’)로 옮겨서 사용할 수도 있다. RWKV는 Autoregressive LM이므로 generate 함수 사용법이 GPT와 동일하다. 다만 매우 긴 프롬프트를 처리할 때 Transformer 대비 메모리 사용량이 적고 속도가 빠른 것을 확인할 수 있다.</p>
<p><strong>참고:</strong> RWKV는 <strong>커뮤니티 중심으로 발전</strong>한 오픈소스 LLM으로, 디스코드 등에서 활발히 개발되고 있다. 현재 RWKV-14B까지 공개되어 있으며, 고속 추론을 위한 rwkv.cpp 같은 경량화 구현도 나와 있다.</p>
</section>
<section id="id3">
<h3>체크포인트 질문<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>RWKV 아키텍처가 <strong>Transformer의 어떤 단점</strong>을 해결하기 위해 나왔는지 설명하라. 또한 <strong>Transformer와 RNN의 장점</strong>을 각각 어떤 방식으로 결합했는가?</p></li>
<li><p><strong>RWKV의 추론 방식</strong>은 Transformer와 어떻게 다르며, 이로 인해 얻는 이점은 무엇인가 (힌트: KV 캐시 vs 은닉 상태)?</p></li>
<li><p>RWKV의 이름이 뜻하는 바는 무엇이며, Time-mix와 Channel-mix의 역할은 무엇인지 간략히 정리하라.</p></li>
</ul>
</section>
</section>
<section id="jamba-moe-transformer-mamba">
<h2>4. Jamba 아키텍처 – MoE 기반 Transformer+Mamba 하이브리드<a class="headerlink" href="#jamba-moe-transformer-mamba" title="Link to this heading">#</a></h2>
<section id="id4">
<h3>소개<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p><img alt="Jamba 소개" src="../_images/jamba-1.jpeg" /></p>
<p><strong>Jamba</strong>는 <strong>Joint Attention + Mamba</strong>의 줄임말로, <strong>Transformer</strong>와 <strong>Mamba</strong>를 결합하고 거기에 <strong>MoE (Mixture-of-Experts)</strong> 기술을 적용한 <strong>하이브리드 아키텍처</strong>다. 이 모델은 2024년 AI21 Labs에 의해 발표되었으며, <strong>세계 최초의 상용 수준 하이브리드 SSM-Transformer 모델</strong>로 불린다. Jamba는 Mamba, 즉 새로운 구조의 SSM 아키텍처를 기반으로 한 첫 번째 생산 등급 모델로, 트랜스포머 아키텍처의 한계를 극복하려는 시도에서 출발했으나, Mamba만의 한계도 있었다. Jamba는 트랜스포머와 SSM의 장점을 모두 살려, 기존 모델들과 비교해 뛰어난 성능을 보여주는 동시에, 긴 문맥에서의 처리량을 3배 가까이 향상시키는 등 비용 효율성과 접근성을 우선시하는 모델이다.</p>
</section>
<section id="id5">
<h3>주요 특징<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p><img alt="Jamba 성능 비교" src="../_images/jamba-2.jpeg" /></p>
<ul class="simple">
<li><p>새로운 SSM-트랜스포머 하이브리드 아키텍처를 기반으로 구축된 최초의 프로덕션 수준의 맘바 기반 모델</p></li>
<li><p>Mixtral 8x7B 대비 긴 컨텍스트에서 3배의 처리량 제공</p></li>
<li><p>대규모 256K 컨텍스트 창에 대한 액세스의 대중화</p></li>
<li><p>단일 GPU에서 최대 140K 컨텍스트까지 지원하는 동급 유일 모델</p></li>
<li><p>Apache 2.0 라이선스로 모델 가중치를 공개한 공개 LLM (OpenLLM)</p></li>
<li><p>Hugging Face에서 사용 가능하며, NVIDIA API 카탈로그에도 곧 추가 예정</p></li>
</ul>
<p>Jamba는 비슷한 크기의 다른 모델들과 비교했을 때, 더 뛰어나거나 비슷한 성능을 보인다. 추론(reasoning) 관련 벤치마크들에서 좋은 결과를 보이고 있다.</p>
</section>
<section id="jamba">
<h3>Jamba의 모델 구조<a class="headerlink" href="#jamba" title="Link to this heading">#</a></h3>
<p><img alt="Jamba 모델 구조" src="../_images/jamba-3.jpeg" /></p>
<p>Jamba는 SSM과 트랜스포머 아키텍처의 장점을 결합한 세계 최초의 프로덕션급의 Mamba 기반 모델이다. 이 하이브리드 구조는 트랜스포머의 강력한 언어 이해 능력과 SSM의 효율적인 메모리 관리 및 처리 속도를 모두 활용한다. 결과적으로, Jamba는 기존의 언어 모델들이 가지고 있던 메모리 사용량 증가와 처리 속도 저하 문제를 크게 개선한다.</p>
</section>
<section id="id6">
<h3>대규모 컨텍스트 윈도우 및 비용-효율성<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p><img alt="컨텍스트 크기 비교" src="../_images/jamba-4.jpeg" /></p>
<p>Jamba는 256K의 컨텍스트 윈도우를 제공함으로써, 매우 긴 문서나 대화를 효율적으로 처리할 수 있다. 이는 AI 개발자들이 보다 복잡한 자연어 처리 작업을 수행할 수 있게 해주며, 기존 모델들이 다룰 수 없었던 긴 문맥의 이해에 기여한다.</p>
</section>
<section id="moe-mixture-of-experts">
<h3>MoE(Mixture of Experts) 활용<a class="headerlink" href="#moe-mixture-of-experts" title="Link to this heading">#</a></h3>
<p><img alt="Jamba 처리량" src="../_images/jamba-5.jpeg" /></p>
<p>Jamba는 MoE 레이어를 통해 추론 시 사용 가능한 52B 파라미터 중 단 12B만을 활용한다. 이로 인해 모델의 활성화 파라미터가 더 효율적으로 사용되며, 동일한 크기의 트랜스포머 전용 모델보다 더 나은 성능을 보인다.</p>
<p>Jamba 아키텍처의 주요 특징은 다음과 같다:</p>
<ul class="simple">
<li><p><strong>교대 구조의 블록</strong>: Jamba는 <strong>8개 층 중 1개는 Transformer (Attention)</strong>, 나머지 7개는 Mamba로 구성되는 <strong>1:7 비율</strong>로 레이어들을 섞어 쌓는다. 예를 들어 32층짜리 모델이라면 그 중 4개 층에만 어텐션이 있고, 나머지 28개 층은 Mamba로 되어 있다. 이러한 <strong>블록 구조 다이어그램</strong>은 아래와 같다 (Transformer 또는 Mamba 레이어 + MLP로 한 블록을 형성):</p>
<ul>
<li><p>[{Transformer Attention} + MLP] → [{Mamba SSM} + MLP] → [{Mamba SSM} + MLP] → … (이런 식으로 1개의 Attention 블록 후 7개의 Mamba 블록이 이어지는 패턴).</p></li>
</ul>
</li>
</ul>
<p>이 구조를 통해 <strong>글로벌한 내용 추출</strong>은 가끔 삽입된 어텐션 층이 담당하고, <strong>나머지 대부분의 상호작용</strong>은 Mamba 층이 효율적으로 처리한다. 결과적으로 <strong>전체 메모리 footprint</strong>는 KV 캐시를 적게 사용하여 크게 줄고, <strong>긴 문맥 처리시에도</strong> 소수의 어텐션만으로 충분한 성능을 내도록 설계되었다.</p>
<ul class="simple">
<li><p><strong>Mixture-of-Experts (전문가 혼합)</strong> 활용: Jamba는 Transformer의 MLP 부분 일부를 <strong>MoE로 교체</strong>하였다. 구체적으로는 <strong>매 2개 층마다 하나의 MoE 레이어</strong>를 넣고, 각 MoE 레이어에는 <strong>16개의 Expert MLP</strong>가 존재하며, 매 토큰마다 상위 2개의 Expert만 활성화(top-2 gating)된다. 이로써 **총 파라미터 수는 크게 증가(52B)**하지만, <strong>실제 추론 시 활성화되는 파라미터는 12B 수준</strong>으로 제한된다. 즉, **모델 용량(capacity)**은 키우면서도 <strong>계산비용은 억제</strong>하는 효과를 낸다. (Jamba 7B 기본 모델이 MoE를 통해 <em>활성 12B / 전체 52B</em>인 것이 그 예다.)</p></li>
<li><p><strong>긴 문맥과 높은 효율</strong>: Jamba는 <strong>256K 토큰</strong>이라는 매우 긴 컨텍스트 윈도우를 지원한다. 이는 Transformers 기반 공개 모델 중 가장 긴 수준이며, 실제로 <strong>128K 토큰 입력을 8-bit 압축으로 단일 80GB GPU에서 처리 가능</strong>한 것으로 보고되었다. 동등한 크기의 일반 Transformer(Mixtral-8×7B 등)는 이러한 긴 문맥을 단일 GPU에 올릴 수 없기 때문에, <strong>메모리 면에서 2배 이상의 이득</strong>을 보인다. 또한 <strong>긴 문맥에서의 토큰 처리 속도(throughput)가 매우 높아</strong>, 128K 토큰 입력 기준으로 <strong>동일급 Transformer 대비 3배 이상 빠른 생성 속도</strong>를 달성했다. 이는 긴 입력을 처리할 때 어텐션 연산이 일부 층에서만 일어나므로 전체 부담이 적기 때문이다. Jamba는 이러한 효율 향상을 이루면서도, <strong>성능은 Mixtral-8x7B (활성 39B)나 Llama2-70B와 견줄만한 수준</strong>을 유지한다.</p></li>
</ul>
<p>요약하면, <strong>Jamba는 Transformer의 일부를 Mamba로 치환</strong>하고, <strong>모델 용량은 MoE로 늘린</strong> 독창적인 구조다. 이로써 <strong>메모리 사용량과 추론 속도를 극적으로 개선</strong>하여 대용량 LLM을 보다 실제 애플리케이션에 적합하게 만들었다. Jamba는 출시와 함께 오픈 소스로 가중치를 공개(Apache 2.0)하여, 연구자들이 추가 튜닝 및 개선을 이어갈 수 있도록 하였다.</p>
<p><strong>비교:</strong> 선행 연구로 Mamba와 어텐션을 결합한 작은 하이브리드 모델 시도가 있었으나(예: H3, Hyena 등), Jamba처럼 <strong>수십억 파라미터 규모로 확장</strong>하고 <strong>MoE까지 통합</strong>한 것은 처음이다. 또한 Jamba는 실제 제품화 수준의 안정된 성능을 보여주는 첫 사례로 평가된다.</p>
</section>
<section id="id7">
<h3>사용 방법<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>Jamba 모델을 사용하기 위해서는 Hugging Face의 transformers 라이브러리가 필요하다. 다음은 Python을 사용하여 Jamba 모델을 불러오고, 간단한 텍스트 생성을 수행하는 예시 코드다. 이 코드를 실행하기 전에, transformers 라이브러리와 함께 mamba-ssm, causal-conv1d 라이브러리를 설치해야 한다. 이는 Jamba의 최적화된 Mamba 구현을 사용하기 위함이다.</p>
</section>
<section id="id8">
<h3>Jamba 모델 활용 예시 코드<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>Jamba는 AI21 Labs에서 공개한 모델로, Hugging Face Hub에도 체크포인트가 올라와 있다. transformers 라이브러리를 통해 다음과 같이 사용할 수 있다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># Jamba v0.1 모델 로드 (HF 모델 카드: ai21labs/Jamba-v0.1)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ai21labs/Jamba-v0.1&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ai21labs/Jamba-v0.1&quot;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;AI 시대의 새로운 언어 모델 구조는&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>위 코드에서 trust_remote_code=True 옵션은, Jamba의 커스텀 모델 구조를 불러오기 위해 필요할 수 있다 (AI21이 제공한 모델 정의를 신뢰하여 로드). Jamba 모델을 GPU에서 사용하려면 .to(‘cuda’) 호출로 옮겨야 하며, 7B 기반이라 하더라도 전체 52B 파라미터를 포함하므로 로딩 시 메모리 요구사항을 고려해야 한다.</p>
<p>Jamba의 컨텍스트 윈도우는 기본 256K로 설정되어 있지만, model.config.max_position_embeddings 등을 통해 현재 지원 컨텍스트를 확인할 수 있다. 긴 문맥 사용 시에도 Jamba는 <strong>적은 메모리</strong>로 <strong>빠른 추론</strong>이 가능하므로, 실제로 100K 토큰 이상의 장문 문서를 질의응답하거나 요약하는 등의 실험에 적합하다.</p>
</section>
<section id="id9">
<h3>체크포인트 질문<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Jamba 아키텍처에서 <strong>Transformer 층과 Mamba 층은 어떤 비율로 배치</strong>되는가? 이러한 설계가 <strong>메모리 및 속도 측면</strong>에 어떤 이점을 주는지 설명하라.</p></li>
<li><p>Jamba가 MoE를 도입한 이유는 무엇인가? <em>활성 파라미터</em>와 <em>총 파라미터</em>의 개념을 들어 설명하라.</p></li>
<li><p>Jamba 모델이 지원하는 최대 컨텍스트 길이는 얼마이며, 이것이 실용적으로 어떤 의미를 가지는지 (예: 적용 사례) 생각해보라.</p></li>
</ul>
</section>
</section>
<section id="id10">
<h2>5. 아키텍처별 성능 비교<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<p>앞서 살펴본 <strong>Transformer, Mamba, RWKV, Jamba</strong>의 특성과 성능을 주요 지표별로 비교하면 다음과 같다:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>아키텍처</p></th>
<th class="head text-left"><p>지원 컨텍스트 길이</p></th>
<th class="head text-left"><p>시간 복잡도 (추론 시)</p></th>
<th class="head text-left"><p>추론 속도(throughput)</p></th>
<th class="head text-left"><p>파라미터 효율성</p></th>
<th class="head text-left"><p>메모리 사용 특징</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Transformer (기존)</strong></p></td>
<td class="text-left"><p>보통 2K~4K (확장형은 최대 32K+)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n^2)\)</span> (토큰 쌍 모두 연산)<br/><em>(신규 토큰 생성당 <span class="math notranslate nohighlight">\(O(n)\)</span>)</em></p></td>
<td class="text-left"><p>기준 1× (동일 크기 대비)</p></td>
<td class="text-left"><p>- (성능 ~ 파라미터 수 비례)</p></td>
<td class="text-left"><p>KV캐시 메모리 O(<em>n</em>) (문맥 길이에 비례)<br/><em>긴 문맥시 GPU 메모리 한계</em></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Mamba (SSM)</strong></p></td>
<td class="text-left"><p>이론상 무제한 (실험서 1M 토큰)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n)\)</span> (선형 시간)</p></td>
<td class="text-left"><p>Transformer 대비 ~5× 빠름</p></td>
<td class="text-left"><p>높음: <em>3B로 6B Transformer 성능</em></p></td>
<td class="text-left"><p><strong>상태만 유지</strong>, 메모리 O(1) per token (토큰 길이에 영향 적음)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>RWKV (RNN)</strong></p></td>
<td class="text-left"><p>사실상 매우 김 (훈련 한계 내)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O(n)\)</span> (선형)</p></td>
<td class="text-left"><p>Transformer보다 빠름 (유사 SSM계)</p></td>
<td class="text-left"><p>높음: <em>14B로 GPT 13B급 성능</em></p></td>
<td class="text-left"><p><strong>은닉 상태</strong>만 유지, 메모리 매우 효율적</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Jamba (Hybrid)</strong></p></td>
<td class="text-left"><p>최대 256K</p></td>
<td class="text-left"><p>혼합: 일부 <span class="math notranslate nohighlight">\(O(n^2)\)</span> (4개층) + 다수 <span class="math notranslate nohighlight">\(O(n)\)</span></p></td>
<td class="text-left"><p>긴 문맥에서 ~3× 빠름 (vs Mixtral)</p></td>
<td class="text-left"><p>높음: <em>활성 12B / 총 52B</em></p></td>
<td class="text-left"><p>KV캐시 일부만 사용 -&gt; <strong>메모리 절약</strong><br/><em>128K 문맥 단일 80GB GPU 적재 가능</em></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>비교 해설:</strong> Transformer는 <strong>병렬 학습</strong>에 유리하나, 컨텍스트 길이가 늘어날수록 <strong>추론 시간이 선형 증가</strong>하고 메모리 사용이 커진다. Mamba와 RWKV는 <strong>순차 처리</strong> 방식이지만 각각 <strong>선택적 상태공간, RNN 공식</strong>으로 구현되어 <strong>추론 복잡도가 선형</strong>이며, <strong>맥락을 요약</strong>하여 들고 다니므로 긴 입력에도 <strong>일관된 속도</strong>를 낸다. 특히 Mamba는 <strong>하드웨어 친화 최적화</strong> 덕에 작은 모델에서도 Transformers 대비 <strong>5배 이상 토큰 처리 속도</strong>를 보였다. RWKV도 유사하게 <strong>맥락 길이에 민감하지 않은 성능</strong>을 보여 고속/저메모리 추론이 가능하다.</p>
<p>Jamba는 <strong>하이브리드 구조</strong>로, 동일 조건에서 <strong>Transformer 대비 메모리 1/2, 속도 3배</strong> 이상의 이점을 보인다. 이는 대부분의 층이 Mamba이기에 가능한 것으로, <strong>일부 어텐션 층</strong>만으로도 충분한 성능을 확보하면서 불필요한 계산을 줄였기 때문이다. MoE 적용으로 <strong>파라미터 효율</strong>도 높아져, <strong>활성 파라미터</strong> 기준으로 보면 Transformer보다 적은 연산으로도 비슷한 성능을 낸다.</p>
<p>결과적으로, <strong>메모리 여건이나 실시간 처리</strong>가 중요한 응용(예: 장문 입력, 제한된 GPU 메모리 환경)에서는 Mamba, RWKV, Jamba 같은 대안이 Transformer를 대체하거나 보완할 수 있다. 반면 <strong>최고의 정밀도</strong>가 요구되는 경우 아직까지는 Transformer 계열이 유리하나, Jamba 등의 성공으로 이러한 격차도 빠르게 좁혀지고 있다.</p>
</section>
<section id="llm">
<h2>6. 최신 오픈소스 LLM 소개 및 특징<a class="headerlink" href="#llm" title="Link to this heading">#</a></h2>
<p>이제 앞서 언급된 아키텍처들이 적용되거나 영감을 준, <strong>최신 공개 대형 언어모델(LLM)</strong> 몇 가지를 살펴보자. 여기서는 <strong>Llama 3</strong>, <strong>Mixtral 8×7B</strong>, <strong>Qwen2-72B</strong> 세 가지를 소개하고, 각 모델의 구조적 특징과 산업 적용 사례를 간략히 설명한다.</p>
<section id="llama-3">
<h3>Llama 3<a class="headerlink" href="#llama-3" title="Link to this heading">#</a></h3>
<p>Meta AI의 <strong>Llama 3</strong>는 2024년 공개된 Llama 시리즈의 최신 버전으로, <strong>8B</strong>, <strong>70B</strong>, 그리고 초거대 <strong>405B</strong> 파라미터 모델로 구성된 **모델 군(family)**이다. Llama2 대비 <strong>더 방대한 데이터</strong>로 학습되었고(특히 405B 모델은 15.6조 토큰, Llama2-70B의 50배 데이터), <strong>128K 토큰</strong>에 이르는 긴 문맥을 다룰 수 있도록 설계되었다. 긴 문맥 지원을 위해 <strong>8K 문맥으로 먼저 프리트레인</strong> 한 후 <strong>점진적으로 문맥 길이를 늘리는</strong> 방식을 채택했으며, 총 6단계에 걸쳐 8000억 토큰을 추가 학습하여 128K까지 확장했다. 이러한 <strong>컨텍스트 윈도우 확장 기법</strong>은 모델이 긴 입력에서도 안정적으로 추론하도록 돕는다.</p>
<p>Llama 3의 <strong>모델 구조</strong> 자체는 기본적으로 <strong>표준 Transformer 데코더</strong>다. Llama2와 마찬가지로 <strong>GPT 스타일의 Decoder-Only</strong> 구조를 사용하고, 활성화 함수로 SwiGLU, RoPE 위치임베딩 등의 기술이 적용되었다 (Llama2의 개선사항들이 계승). 성능 향상은 주로 <strong>데이터 품질 개선</strong>과 <strong>학습 스케일 증가</strong>에서 얻었으며, Llama 3.1(개량 버전)을 통해 <strong>멀티링구얼</strong>(다중언어) 지원과 <strong>툴 사용 능력</strong>까지 강화했다. 특히 405B 모델은 공개된 것 중 <strong>최대 규모의 오픈 LLM</strong>으로, 수백개의 GPU를 동원해 54일간 학습되었을 정도로 <strong>막대한 리소스</strong>가 투입되었다.</p>
<p>Llama 3는 공개 이후 다양한 분야에 활용되고 있다. 예를 들어 <strong>고객센터 챗봇</strong>, <strong>전문 지식 질의응답</strong> 등에 Llama2를 대체하여 쓰이고 있으며, 70B 모델은 상용 수준의 성능으로 언어이해/생성 작업에 쓰인다. <strong>토크나이저</strong>는 SentencePiece BPE를 사용하며, Llama2와 호환되어 기존 모델 자산(프롬프트, 토크나이저)을 이어서 활용하기에도 좋다. Hugging Face에서도 meta-llama/Meta-Llama-3-8B 등의 리포지토리로 공개되어 있어 쉽게 로드하여 사용할 수 있다.</p>
<p><strong>요약:</strong> Llama 3는 <strong>대규모 데이터</strong>와 <strong>긴 문맥 학습</strong>을 통해 성능을 극대화한 차세대 LLM이다. 구조적으로 혁신적 변화는 없지만, <strong>모델 크기/데이터의 스케일 업</strong>으로 얻은 <strong>탁월한 성능</strong>이 강점이며, 멀티링구얼/툴 사용 등 실용적 기능이 추가되었다.</p>
</section>
<section id="mixtral-87b">
<h3>Mixtral 8×7B<a class="headerlink" href="#mixtral-87b" title="Link to this heading">#</a></h3>
<p><strong>Mixtral 8×7B</strong>는 <strong>Mistral AI</strong>에서 2024년 발표한 <strong>Sparse Mixture-of-Experts (SMoE)</strong> 모델로, <strong>Mistral 7B</strong> 모델의 각 MLP 층을 <strong>8개의 Expert로 확장</strong>한 버전이다. 이름의 의미는 <em>“8명의 7B 전문가를 혼합”</em> 한 모델로 이해할 수 있다. Mixtral-8×7B의 <strong>전체 파라미터 수는 약 46.7B</strong>로 늘어나지만 (7B 모델 대비 약 6.7배), <strong>추론 시에는 토큰마다 39B 정도의 파라미터만 활성화되어</strong> 연산된다. 다시 말해, 한 토큰이 8개의 expert 중 상위 2개만 사용하도록 (Top-2) 게이팅하였기 때문에, <strong>실제 계산량은 14B급 모델 수준</strong>으로 유지하면서 <strong>모델 용량은 46B급 성능</strong>을 낼 수 있게 한 것이다.</p>
<p>Mixtral의 결과, <strong>성능은 기존 7B 모델을 크게 상회</strong>한다. 논문 보고에 따르면 <strong>Llama2-70B를 대부분의 벤치마크에서 능가</strong>하는 수준이며, OpenAI GPT3.5급과도 맞먹는 영역이 있다. 예를 들어, MMLU 등 학습되지 않은 지식 테스트에서 7B치고 뛰어난 성적을 기록했다. 이러한 효율은 <strong>MoE의 장점</strong>인 <em>“전문가 분업”</em> 덕분에, 각 expert가 <strong>서브태스크에 특화된 표현</strong>을 학습하고, 필요한 때만 동원되기 때문이다.</p>
<p>Mixtral 8×7B의 <strong>활용 예시</strong>로는, <strong>오픈소스 챗봇</strong>이나 <strong>임베딩 생성기</strong> 등이 있다. 한 예로 <strong>FriendliAI</strong>에서는 Mixtral-8×7B를 실시간 서비스에 투입하여, 동일 하드웨어에서 <strong>더 빠른 응답 시간과 높은 처리량</strong>을 달성했다고 보고했다. 또한 MLPerf Inference 벤치마크에 Mixtral이 등장하여, MoE 모델의 효율성을 입증하기도 했다.</p>
<p>Mixtral 모델은 Hugging Face에 공개되어 있으며, 다음과 같이 사용할 수 있다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="c1"># 모델 로드시 device_map이나 load_in_4bit 등의 옵션으로 메모리 최적화 가능</span>
</pre></div>
</div>
<p><strong>참고:</strong> Mixtral은 현재 <strong>transformers 라이브러리에서 완전 지원</strong>되지는 않아서, 위 예시처럼 불러오면 내부적으로는 MoE 구조가 일반 Linear로 풀려 로드된다. 추론 효율을 극대화하려면 <strong>vLLM</strong>이나 <strong>FlexGen</strong>과 같은 MoE 최적화 엔진을 사용하는 것이 권장된다. Hugging Face 모델 카드에서는 half-precision, bitsandbytes(4bit 양자화), FlashAttention 2 등 메모리 절감 세팅도 안내하고 있으니 참고하라.</p>
</section>
<section id="qwen2-72b">
<h3>Qwen2-72B<a class="headerlink" href="#qwen2-72b" title="Link to this heading">#</a></h3>
<p><strong>Qwen2-72B</strong>는 **알리바바(Alibaba)**에서 공개한 <strong>차세대 거대 모델</strong>로, 다국어 및 멀티모달 지원에 초점을 맞춘 <strong>72억×10^8^ 파라미터 (약 727억)</strong> 규모의 LLM이다. Qwen 시리즈는 원래 중국어/영어에 강점이 있는 모델로 알려져 있는데, <strong>Qwen2에서는 27개 이상의 추가 언어 데이터</strong>로 학습하여 <strong>글로벌한 언어 능력</strong>을 확보했다. 또한 <strong>프로그래밍 코드</strong>와 <strong>수학 문제 해결</strong> 능력이 크게 강화되어, 다양한 벤치마크에서 동급 최고 수준을 달성했다.</p>
<p>Qwen2-72B의 아키텍처는 <strong>Transformer Decoder</strong> 기반이며, 활성화 함수로 <strong>SwiGLU</strong>, <strong>Attention QKV bias</strong>, 그리고 <strong>Grouped Query Attention (GQA)</strong> 기법 등을 적용한 점이 특징이다. GQA는 대규모 모델의 속도를 높이기 위해 <strong>어텐션 헤드를 그룹으로 묶어 계산</strong>하는 방법으로, Qwen2에서는 모든 모델에 GQA를 도입해 <strong>추론 메모리와 속도를 최적화</strong>했다. 이와 함께 임베딩 층과 출력층 가중치를 공유하는 <strong>Embedding Tying</strong> 등 파라미터 효율을 높이는 기법도 사용되었다.</p>
<p><strong>멀티모달 지원</strong>이 Qwen2의 큰 강점이다. <strong>Qwen2-VL-72B</strong>라는 변형 모델은 <strong>이미지-텍스트-비디오</strong>를 모두 입력받아 이해할 수 있는 모델로, **Multimodal-ROPE (M-ROPE)**라는 <strong>다차원 위치 임베딩</strong>을 도입했다. M-ROPE는 1D 위치정보(텍스트 시퀀스), 2D 위치정보(이미지의 위치), 3D 위치정보(비디오의 시간 프레임까지)를 <strong>하나의 통합된 위치 임베딩</strong>으로 처리해, Qwen2-VL이 긴 동영상(20분 이상 분량)까지 이해하도록 한다. 실제로 Qwen2.5-VL (개선 버전)은 <strong>복잡한 영상 질의응답, OCR</strong> 등에서 GPT-4와 견줄만한 성능을 시현하여 주목받았다.</p>
<p><strong>성능 측면</strong>에서 Qwen2-72B는 공개 모델 중 최상위권에 속한다. 내부 평가 결과, 많은 언어이해 벤치마크에서 이전 세대인 Qwen-14B나 경쟁 모델들을 앞섰고, 코드 생성 관련 HumanEval 등에서도 64.6% 통과율로 공개모델 최고 수준을 기록했다. 또한 중국어 등 비영어권 평가에서도 높은 점수를 받아, 산업적 활용 가치가 높다.</p>
<p><strong>활용 예시:</strong> Qwen2-72B는 알리바바 클라우드 등을 통해 <strong>API로 제공</strong>되며, 기업들이 자사 검색엔진, 전자상거래 Q&amp;A 등에 다국어로 활용하고 있다. 오픈소스로 가중치가 공개되어 HuggingFace Qwen/Qwen2-72B 경로로 다운로드 가능하며, Transformers 4.37.0 이상에서 바로 로드하여 사용할 수 있다. 다만 72B 모델은 크기가 크므로 GPU 메모리 4~8장 이상이 필요하다. 멀티모달 모델인 Qwen2-VL-72B는 별도 체크포인트로 제공되며, VisionEncoder를 포함한 처리로직이 통합되어 있어 <strong>이미지+텍스트 입력 형식</strong>으로 사용할 수 있다 (예: processor = QwenImageProcessor(); model = QwenVLModel.from_pretrained(…) 형태).</p>
<p><strong>정리:</strong> Qwen2-72B는 <strong>기업 수준의 멀티모달 AI</strong>를 지향한 모델로, <strong>광범위한 언어 지원</strong>, <strong>강화된 추론 성능</strong>, <strong>멀티모달 처리를 위한 구조적 개선</strong>(M-ROPE, GQA 등)을 통해 현존 최고 공개 모델 중 하나로 평가된다. 산업 현장에서는 다국어 고객지원 봇, 실시간 영상 분석 등 다양한 분야에 적용 가능성을 보여주고 있다.</p>
</section>
</section>
<section id="id11">
<h2>7. 실습 지침<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<p>마지막으로, 위에서 소개한 개념들과 모델들을 직접 다뤄볼 수 있는 <strong>실습 가이드</strong>를 제시한다. 아래 단계에 따라 환경을 구축하고, Transformer와 Mamba 모델의 <strong>속도 비교 실험</strong>까지 수행하라.</p>
<ol class="arabic">
<li><p><strong>Conda 가상환경 구성:</strong>
실습을 위해 Anaconda 혹은 Miniconda를 사용해 새로운 파이썬 가상환경을 만든다. 터미널에서 다음을 실행하라:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>llm_env<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="w"> </span>-y
conda<span class="w"> </span>activate<span class="w"> </span>llm_env
</pre></div>
</div>
<p>이렇게 하면 Python 3.10 기반의 llm_env 환경이 생성 및 활성화된다.</p>
</li>
<li><p><strong>PyTorch 및 Hugging Face Transformers 설치:</strong>
PyTorch 프레임워크와 트랜스포머 라이브러리를 설치한다. CUDA 지원 GPU가 있다면 해당 버전에 맞는 PyTorch를 선택하라 (여기서는 예시로 CPU 버전 설치):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w">    </span><span class="c1"># PyTorch 설치</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="o">[</span>torch<span class="o">]</span><span class="w">             </span><span class="c1"># Hugging Face Transformers 설치</span>
</pre></div>
</div>
<p>설치 과정에서 버전에 유의하라. 예를 들어 CUDA 11 이상이 설치된 경우 pip install torch==2.0.1+cu118 등의 명령을 사용할 수도 있다. 설치가 끝났으면 python -c “import torch; import transformers; print(‘OK’)” 등으로 잘 설치됐는지 확인한다.</p>
</li>
<li><p><strong>Mamba 설치 및 사용법:</strong>
Mamba 아키텍처 실험을 위해 <strong>mamba-ssm</strong> 패키지를 설치한다. 해당 패키지는 선택적으로 GPU 상의 연산 최적화를 포함하므로, Linux/NVIDIA 환경이 필요하다. 설치 명령:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mamba-ssm<span class="o">[</span>causal-conv1d<span class="o">]</span><span class="w">  </span><span class="c1"># (선택적으로 conv1d 가속 포함)</span>
</pre></div>
</div>
<p>설치 후, 간단한 예제로 Mamba 블록을 실행해보자:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mamba_ssm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mamba</span>
<span class="n">mamba_block</span> <span class="o">=</span> <span class="n">Mamba</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">d_state</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_conv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># 32차원 모델</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># 시퀀스 길이 10의 임의 입력</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">mamba_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (1, 10, 32) 출력</span>
</pre></div>
</div>
<p>이 코드로 Mamba 블록이 정상 동작하는지 간단히 확인해볼 수 있다.</p>
</li>
<li><p><strong>Transformer vs Mamba 추론 시간 비교 실험:</strong>
동일한 입력에 대해 Transformer 모델과 Mamba 모델이 응답을 생성하는 속도를 비교해보자. 여기서는 간단히 GPT-2 (Transformer, 1.5억 파라미터급)와 앞서 만든 소형 Mamba 블록으로 성능을 비교한다. 실제론 동등한 파라미터 규모의 모델이 필요하지만, 개념 증명을 위해 작은 예시로 시연한다. 아래 코드를 실행하라:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mamba_ssm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mamba</span>

<span class="c1"># 1. 프롬프트 설정</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;인공지능이란 무엇인가?&quot;</span>

<span class="c1"># 2. Transformer 모델 (GPT-2 small) 로드</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">transformer_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># 3. 프롬프트 토크나이징 및 추론 (Transformer)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output_ids</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">transformer_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformer 출력:&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformer 추론 시간: </span><span class="si">{</span><span class="n">transformer_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">초&quot;</span><span class="p">)</span>

<span class="c1"># 4. Mamba 모델 생성 (동일 hidden size로)</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>  <span class="c1"># GPT2 hidden size (768)</span>
<span class="n">mamba_model</span> <span class="o">=</span> <span class="n">Mamba</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_state</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_conv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Mamba는 학습된 언어모델이 아니므로 임의 가중치 상태. 여기서는 속도 비교만 수행.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># GPT2가 생성한 50개 토큰에 해당하는 임의 텐서</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">mamba_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Mamba 블록으로 50토큰 처리</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">mamba_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mamba 50토큰 처리 시간: </span><span class="si">{</span><span class="n">mamba_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">초&quot;</span><span class="p">)</span>

<span class="c1"># 5. 결과 비교</span>
<span class="n">speedup</span> <span class="o">=</span> <span class="n">transformer_time</span> <span class="o">/</span> <span class="n">mamba_time</span> <span class="k">if</span> <span class="n">mamba_time</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;추론 속도 비교: Mamba 블록이 Transformer 대비 약 </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">배 빠르다.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>이 코드는 ① GPT-2로 50개 신규 토큰을 생성하는 시간과 ② Mamba 블록이 50토큰을 순차 처리하는 시간을 측정한다. 마지막에 두 값을 비교하여 <strong>Mamba의 추론 속도가 얼마나 빠른지</strong> 출력한다. 예시 환경에서는 Mamba 블록이 경량 모델이므로 Transformer보다 수 배 이상 빠르게 나타날 것이다.</p>
<p><strong>주의:</strong> 여기서는 Mamba를 <strong>학습된 LM으로 사용하는 것이 아니므로</strong>, 실제 생성 텍스트 비교는 불가능하다. 대신 동일 길이의 토큰을 처리하는 <strong>연산량 비교</strong>에 중점을 둔 것이다. 더 정확한 비교를 위해서는 <strong>Mamba로 사전학습한 언어모델</strong>(Mamba-3B 등)과 동일 규모의 Transformer를 비교해야 한다. 해당 모델들을 확보했다면 동일한 프롬프트로 generate를 호출하고 time.time()으로 걸린 시간을 측정해보는 식으로 실험해볼 수 있다.</p>
</li>
<li><p><strong>응용 및 확장</strong>:</p>
<ul class="simple">
<li><p><strong>여러 모델 테스트</strong>: 위에서 다룬 Llama3, Mixtral, Qwen2 등 모델들도 Hugging Face 경로나 공식 배포 경로를 통해 로드하여 generate 실습을 해볼 수 있다. 예를 들어 Llama3-70B 모델의 경우 meta-llama/Meta-Llama-3-70B를 AutoModelForCausalLM으로 불러올 수 있다 (단, 큰 모델은 하드웨어 요구량에 유의).</p></li>
<li><p><strong>긴 문맥 실험</strong>: Jamba나 Mamba 같은 모델의 긴 문맥 능력을 시험해보고자 한다면, 예를 들어 임의의 긴 더미 텍스트(예: 10만 토큰)를 프롬프트로 주고 정상 출력이 나오는지, 메모리 사용량은 어떤지를 관찰해볼 수 있다.</p></li>
<li><p><strong>프로파일링</strong>: 파이썬의 profile 모듈이나 PyTorch의 프로파일러를 사용하면 각 모델의 레이어별 연산 시간을 측정해볼 수 있다. 이를 통해 Mamba가 어디서 시간을 절약하는지, Transformer는 어떤 연산에 시간이 많이 드는지 더 깊이 분석 가능하다.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="id12">
<h2>참고자료<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<section id="id13">
<h3>주요 논문 및 연구 자료<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Vaswani, A., et al. (2017). “Attention is all you need.” Advances in neural information processing systems.</p></li>
<li><p>Gu, A., &amp; Dao, T. (2023). “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” arXiv preprint.</p></li>
<li><p>Peng, B., et al. (2023). “RWKV: Reinventing RNNs for the Transformer Era.” arXiv preprint.</p></li>
<li><p>Lieber, O., et al. (2024). “Jamba: A Hybrid Transformer-Mamba Language Model.” arXiv preprint.</p></li>
<li><p>Meta AI (2024). “Llama 3: Technical Report.” Meta Research.</p></li>
</ul>
</section>
<section id="id14">
<h3>기술 문서 및 구현체<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hugging Face Transformers Documentation: <a class="reference external" href="https://huggingface.co/docs/transformers">https://huggingface.co/docs/transformers</a></p></li>
<li><p>Mamba GitHub Repository: <a class="github reference external" href="https://github.com/state-spaces/mamba">state-spaces/mamba</a></p></li>
<li><p>RWKV Wiki: <a class="reference external" href="https://wiki.rwkv.com">https://wiki.rwkv.com</a></p></li>
<li><p>AI21 Labs Jamba Documentation</p></li>
<li><p>Mistral AI Mixtral Technical Documentation</p></li>
<li><p>Alibaba Qwen2 Model Card and Documentation</p></li>
</ul>
</section>
<section id="id15">
<h3>온라인 리소스 및 블로그<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“A Visual Guide to Mamba and State Space Models” - Newsletter by Maarten Grootendorst</p></li>
<li><p>“The RWKV language model: An RNN with the advantages of a transformer” - The Good Minima</p></li>
<li><p>“Mamba Explained” - The Gradient</p></li>
<li><p>“Introducing RWKV - An RNN with the advantages of a transformer” - Hugging Face Blog</p></li>
<li><p>“Introducing Jamba: AI21’s Groundbreaking SSM-Transformer Model” - AI21 Blog</p></li>
<li><p>“Takeaways From the Llama 3 Release Paper” - Medium/ailia-ai</p></li>
<li><p>“Qwen2 — Alibaba’s New Powerhouse Multimodal AI” - Research Graph Hub</p></li>
</ul>
</section>
<section id="id16">
<h3>벤치마크 및 평가 자료<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>MLCommons MLPerf Inference Benchmark Results</p></li>
<li><p>“Serving Performances of Mixtral 8x7B” - FriendliAI Blog</p></li>
<li><p>OpenAI Model Comparison Studies</p></li>
<li><p>Various model cards on Hugging Face Model Hub</p></li>
</ul>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week01"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="ko"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">딥러닝자연어처리 (131307379A)</p>
      </div>
    </a>
    <a class="right-next"
       href="qna.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">1. Transformer 아키텍처의 기본 구조</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention 연산 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba-selective-state-space-model">2. Mamba 아키텍처 – Selective State Space Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba">Mamba 구조 및 사용법 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv-rnn">3. RWKV 아키텍처 – RNN과 유사한 구조의 효율적 처리</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv">RWKV 모델 사용 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba-moe-transformer-mamba">4. Jamba 아키텍처 – MoE 기반 Transformer+Mamba 하이브리드</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">소개</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">주요 특징</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba">Jamba의 모델 구조</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">대규모 컨텍스트 윈도우 및 비용-효율성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe-mixture-of-experts">MoE(Mixture of Experts) 활용</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">사용 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Jamba 모델 활용 예시 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">5. 아키텍처별 성능 비교</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm">6. 최신 오픈소스 LLM 소개 및 특징</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">Llama 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixtral-87b">Mixtral 8×7B</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qwen2-72b">Qwen2-72B</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">7. 실습 지침</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">참고자료</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">주요 논문 및 연구 자료</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">기술 문서 및 구현체</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">온라인 리소스 및 블로그</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">벤치마크 및 평가 자료</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
