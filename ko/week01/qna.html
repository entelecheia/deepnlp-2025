
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week01/qna';</script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="강의계획서" href="../syllabus/index.html" />
    <link rel="prev" title="Week 1 - Transformer 및 차세대 아키텍처" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          한국어 <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    딥러닝자연어처리 (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Week 1 - Transformer 및 차세대 아키텍처</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">강의계획서</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/ko/week01/qna.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek01/qna.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week01/qna.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformer, Mamba, RWKV, Jamba 아키텍처 Q&A</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer 아키텍처</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba">Mamba 아키텍처</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv">RWKV 아키텍처</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba">Jamba 아키텍처</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformer-mamba-rwkv-jamba-q-a">
<h1>Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A<a class="headerlink" href="#transformer-mamba-rwkv-jamba-q-a" title="Link to this heading">#</a></h1>
<section id="transformer">
<h2>Transformer 아키텍처<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h2>
<p><strong>Q:</strong> Transformer에서 Self-Attention이 가지는 주요 장점은 무엇인가? 또한 추론 시 병목 현상은 어떤 부분에서 발생하는가?</p>
<p><strong>A:</strong> <strong>Transformer</strong>의 핵심인 <strong>Self-Attention(자가 어텐션)</strong> 메커니즘 덕분에 <strong>입력 시퀀스를 병렬 처리</strong>할 수 있다는 점이 가장 큰 장점이다. RNN과 달리 순환 구조가 없어 모든 토큰 간의 관계를 동시에 계산하므로 <strong>긴 의존 관계</strong>도 효율적으로 학습할 수 있고 학습 속도도 빠르다. 다만 <strong>추론(inference)</strong> 단계에서는 <strong>병목 현상</strong>이 발생하는데, 새로운 토큰을 생성할 때마다 <strong>이전 모든 토큰들과의 어텐션</strong>을 계산해야 하기 때문이다. 예를 들어 시퀀스 길이가 <span class="math notranslate nohighlight">\(L\)</span>이면, 현재 토큰을 낼 때 과거 <span class="math notranslate nohighlight">\(L\)</span>개 토큰에 대한 어텐션을 모두 구해야 하므로 토큰을 하나 생성하는 데 <span class="math notranslate nohighlight">\(O(L)\)</span>의 연산이 들고, 전체적으로 보면 생성 과정이 <strong>길이에 따라 느려지는 병목</strong>을 겪는다. 이로 인해 Transformer 모델은 문맥 길이가 길어질수록 <strong>추론 속도가 저하</strong>되고 <strong>메모리 사용량</strong>도 크게 늘어나는 한계가 있다.</p>
<p><strong>Q:</strong> Transformer 인코더-디코더 구조와 GPT 같은 디코더-Only 구조의 차이를 설명하라.</p>
<p><strong>A:</strong> <strong>Transformer 인코더-디코더 모델</strong>은 **인코더(encoder)**와 <strong>디코더(decoder)</strong> 두 부분으로 구성된다. 인코더는 입력 시퀀스를 받아 내부 <strong>컨텍스트 표현</strong>으로 변환하고, 디코더는 이 컨텍스트와 이전까지 생성된 토큰들을 참고하여 출력 시퀀스를 한 토큰씩 생성한다. 디코더 층에서는 자기 자신에 <strong>마스크드(masked) Self-Attention</strong>을 적용해 미래 토큰을 보지 못하게 하며, **인코더-디코더 Attention(교차 어텐션)**으로 인코더의 컨텍스트를 참조한다. 반면 <strong>GPT와 같은 디코더-Only 구조</strong>는 <strong>인코더가 없고</strong> 디코더 하나로만 이루어진 <strong>단일 스트림</strong> 구조다. 오직 이전 토큰들에 대한 <strong>Self-Attention</strong>만 사용하여 다음 토큰을 예측하며, 별도의 인코더 입력이나 교차 어텐션이 없다. 요약하면, 인코더-디코더 모델은 <strong>입력 시퀀스와 출력 시퀀스가 분리</strong>되어 상호작용(교차 어텐션)을 하는 구조이고, 디코더-Only 모델은 <strong>하나의 시퀀스</strong>에서 <strong>순차 생성</strong>만을 수행하는 구조다.</p>
<p><strong>Q:</strong> 시퀀스 길이 <span class="math notranslate nohighlight">\(L\)</span>에 대해 Transformer의 시간 복잡도와 공간 복잡도는 각각 어떻게 스케일링되는가?</p>
<p><strong>A:</strong> <strong>Transformer</strong>에서 Self-Attention 연산의 <strong>시간 복잡도</strong>는 시퀀스 길이에 대해 **<span class="math notranslate nohighlight">\(O(L^2)\)</span>**로 증가한다. 이는 모든 토큰 쌍마다 유사도를 계산하기 때문에 연산량이 토큰 수의 제곱에 비례하기 때문이다. 마찬가지로 <strong>메모리(공간) 복잡도</strong>도 어텐션 가중치 행렬 등을 저장해야 하므로 **<span class="math notranslate nohighlight">\(O(L^2)\)</span>**로 스케일된다. 예를 들어, 토큰 수 <span class="math notranslate nohighlight">\(L\)</span>이 두 배로 늘어나면 계산량과 메모리 사용량은 네 배 수준으로 늘어나므로, 매우 긴 시퀀스를 처리할 때 Transformer는 연산 비용과 메모리 측면에서 비효율적이다.</p>
</section>
<section id="mamba">
<h2>Mamba 아키텍처<a class="headerlink" href="#mamba" title="Link to this heading">#</a></h2>
<p><strong>Q:</strong> Mamba가 Transformer 대비 갖는 가장 큰 장점은 무엇인가? Mamba는 어떻게 어텐션의 <span class="math notranslate nohighlight">\(O(n^2)\)</span> 병목을 피할 수 있었는지 설명하라.</p>
<p><strong>A:</strong> <strong>Mamba</strong>는 2024년에 제안된 <strong>새로운 시퀀스 모델</strong>로, <strong>어텐션 없이도</strong> 긴 시퀀스를 효과적으로 처리할 수 있다는 점이 가장 큰 장점이다. <strong>선택적 상태 공간 모델(Selective SSM)</strong> 기반의 순환 구조를 사용하여 <strong>시퀀스 길이에 선형적으로(scale linear)</strong> 처리 시간이 증가하도록 설계되었기에, Transformer처럼 토큰 쌍마다 계산을 하지 않고도 긴 문맥을 다룰 수 있다. Mamba는 내부적으로 <strong>RNN처럼 한 토큰씩 은닉 상태를 갱신</strong>하지만, <strong>하드웨어 친화적인 병렬화 알고리즘</strong>을 도입하여 순차 처리의 병목을 해결했다. 그 결과 <strong>어텐션의 <span class="math notranslate nohighlight">\(O(n^2)\)</span> 연산을 피하면서</strong>도 토큰들 간 정보를 주고받을 수 있게 되었고, 실제로 <strong>추론 시 처리 속도가 Transformer 대비 5배 이상</strong> 높다는 보고가 있다. 요약하면, Mamba의 구조 덕분에 <strong>거의 무한에 가까운 길이의 시퀀스</strong>도 실용적으로 다룰 수 있고, 긴 문맥에서도 <strong>계산 효율과 메모리 사용 측면에서 우수</strong>하다.</p>
<p><strong>Q:</strong> Mamba의 Selective SSM에서 “선택적”인 동작은 무엇을 뜻하는가? 이로 인해 언어 모델에서 어떤 효과를 얻을 수 있었는가?</p>
<p><strong>A:</strong> <strong>Selective SSM</strong>에서 “<strong>선택적</strong>”이라는 것은 <strong>상태 공간 모델의 계수(예: 상태 전이 행렬)가 입력 토큰의 함수로 동적으로 결정</strong>된다는 뜻이다. 즉, 모든 시점에 동일한 방식으로 상태를 업데이트하는 것이 아니라, <strong>현재 토큰의 내용에 따라 이전 정보를 얼마나 유지하거나 잊을지 조절</strong>하는 것이다. 이는 마치 RNN의 **게이트(gate)**처럼 동작하여 <strong>중요한 정보는 오래 간직하고 불필요한 정보는 빠르게 망각</strong>하게 한다. 이러한 선택적 상태 제어 덕분에 Mamba는 <strong>토큰 간의 내용 기반 의존성</strong>을 효과적으로 표현할 수 있게 되었고, 고정된 SSM으로는 어려웠던 <strong>자연어와 같은 이산 토큰 데이터</strong>에서도 높은 성능을 발휘할 수 있었다.</p>
<p><strong>Q:</strong> Mamba-3B 모델이 보여준 성능 관련 특징을 언급하라 (예: 동일 크기 Transformer와의 비교, 두 배 큰 Transformer와의 비교 등).</p>
<p><strong>A:</strong> <strong>Mamba-3B</strong>는 파라미터 규모 3억 수준의 Mamba 모델로, <strong>동일한 크기의 Transformer보다 더 우수한 성능</strong>을 보였으며 <strong>파라미터 수가 두 배인 Transformer와도 맞먹는 성능</strong>을 달성했다고 보고되었다. 이는 Mamba 아키텍처의 효율성 덕분에 <strong>더 작은 모델로도 Transformer의 성능을 뛰어넘거나 대등하게</strong> 낼 수 있음을 시사한다. 다시 말해, Mamba-3B는 3B 규모의 Transformer보다 언어 모델링 능력이 좋았고, 6B 규모의 Transformer와 유사한 결과를 보임으로써 <strong>모델 크기 대비 뛰어난 성능 효율</strong>을 입증했다. 이러한 결과는 Mamba의 <strong>아키텍처 혁신</strong>이 실제 모델 성능 향상으로 이어졌음을 보여준다.</p>
</section>
<section id="rwkv">
<h2>RWKV 아키텍처<a class="headerlink" href="#rwkv" title="Link to this heading">#</a></h2>
<p><strong>Q:</strong> RWKV 아키텍처가 Transformer의 어떤 단점을 해결하기 위해 나왔는지 설명하라. 또한 Transformer와 RNN의 장점을 각각 어떤 방식으로 결합했는가?</p>
<p><strong>A:</strong> <strong>RWKV</strong>는 <strong>Transformer의 한계를 극복</strong>하기 위해 고안된 모델로, <strong>긴 문맥 처리와 높은 자원 소모</strong> 문제에 대한 대안으로 등장했다. Transformer는 어텐션 연산의 제약으로 <strong>컨텍스트 길이에 한계</strong>가 있고 대용량 GPU 자원이 필요하다는 단점이 있는데, RWKV는 <strong>RNN 계열</strong>의 아이디어를 도입하여 <strong>사실상 제약 없는 문맥 길이</strong>를 지원한다. <strong>Transformer의 장점</strong>인 <strong>병렬 학습</strong> 능력을 그대로 받아들여, 학습 시에는 전체 시퀀스를 한꺼번에 처리(특수한 형태의 어텐션 수식으로 변환)함으로써 GPU 효율을 확보했고, <strong>RNN의 장점</strong>인 <strong>순차 추론 효율</strong>을 결합하여 추론 시에는 토큰을 <strong>한 개씩 RNN처럼 생성</strong>하도록 만들었다. 요약하면, RWKV는 <strong>학습 단계에서는 Transformer처럼 빠르고</strong>, <strong>추론 단계에서는 RNN처럼 가볍게</strong> 동작하도록 함으로써 두 구조의 이점을 모두 취한 혼합형 아키텍처다.</p>
<p><strong>Q:</strong> RWKV의 추론 방식은 Transformer와 어떻게 다르며, 이로 인해 얻는 이점은 무엇인가? (힌트: KV 캐시 vs 은닉 상태)</p>
<p><strong>A:</strong> <strong>Transformer</strong>는 추론 시 이전 토큰들의 <strong>KV 캐시</strong>를 모두 저장해 두고, 매 생성 스텝마다 그 <strong>전체와 어텐션을 계산</strong>하는 방식을 취한다. 반면 <strong>RWKV</strong>에서는 각 레이어가 **자신의 은닉 상태(state)**를 가지고 있고, 새로운 토큰이 들어오면 <strong>이전 상태를 업데이트</strong>하는 방식으로 동작한다. 따라서 이전 모든 토큰 정보를 거대한 KV 캐시로 보관할 필요 없이 <strong>고정 크기의 은닉 상태만 유지</strong>하면 된다. 이 차이로 인해 얻는 가장 큰 이점은 <strong>메모리 효율과 속도</strong>다. RWKV는 문맥이 길어져도 메모리 사용량이 거의 증가하지 않고, <strong>토큰당 계산량이 일정</strong>하므로 (어텐션처럼 토큰 수에 따라 증가하지 않음) <strong>아주 긴 입력에서도 일관된 속도</strong>를 낸다. 즉, RWKV는 Transformer 대비 <strong>긴 문서 처리에 유리</strong>하며, <strong>저사양 장치</strong>에서도 대용량 LLM을 상대적으로 원활히 구동할 수 있게 해준다.</p>
<p><strong>Q:</strong> RWKV의 이름이 뜻하는 바는 무엇이며, Time-mix와 Channel-mix의 역할은 무엇인지 간략히 정리하라.</p>
<p><strong>A:</strong> <strong>RWKV</strong>는 <strong>Receptance, Weight, Key, Value</strong>의 약자로, 네트워크의 4가지 주요 파라미터 이름에서 유래했다. 여기서 **Receptance (R)**는 <strong>과거 정보를 받아들이는 게이트</strong> 역할을 하고, **Weight (W)**는 <strong>과거 정보에 부여하는 지수적 시간 가중치</strong> (시간이 지남에 따라 이전 영향력을 서서히 감소시키는 계수)이며, **Key (K)**와 **Value (V)**는 각각 키/값 벡터로 <strong>현재 토큰이 전달하는 정보</strong>를 나타낸다.</p>
<p>RWKV 아키텍처의 각 레이어는 두 단계로 나뉘는데, <strong>Time-mix</strong> 단계와 <strong>Channel-mix</strong> 단계가 그것이다. <strong>Time-mix</strong>는 현재 토큰의 입력을 이전 토큰들의 <strong>누적된 Key/Value 정보와 섞어</strong>주는 단계로, R과 W 게이트를 이용해 <strong>이전 상태를 감쇠(decay)시키며 새로운 정보를 통합</strong>한다. 이는 Transformer에서 <strong>어텐션이 시간축 정보를 통합하는 역할</strong>을 대신한다고 볼 수 있다.</p>
<p>다음으로 <strong>Channel-mix</strong>는 각 토큰의 <strong>채널(피처) 방향의 변환</strong>을 수행하는 단계로, 전형적인 **Feed-Forward Network(FFN)**처럼 <strong>토큰별 비선형 변환</strong>을 적용한다. 이 과정에서 이전 토큰의 출력 일부도 입력으로 활용하여 <strong>게이트를 통한 조정</strong>이 이뤄지며, Transformer의 FFN과 유사한 역할을 한다. 정리하면, RWKV의 Time-mix는 <strong>순차적인 정보 혼합</strong>(시간 축 처리)을, Channel-mix는 <strong>특성 차원의 혼합</strong>(채널 처리)을 담당하여 <strong>어텐션 없이도 토큰 간 의존성과 토큰 내부 변환을 모두 수행</strong>하도록 설계되어 있다.</p>
</section>
<section id="jamba">
<h2>Jamba 아키텍처<a class="headerlink" href="#jamba" title="Link to this heading">#</a></h2>
<p><strong>Q:</strong> Jamba 아키텍처에서 Transformer 층과 Mamba 층은 어떤 비율로 배치되는가? 이러한 설계가 메모리 및 속도 측면에 어떤 이점을 주는지 설명하라.</p>
<p><strong>A:</strong> <strong>Jamba</strong>는 <strong>Transformer 층과 Mamba 층을 혼합</strong>한 <strong>하이브리드 아키텍처</strong>다. 구체적으로 하나의 Transformer (Attention) 층 뒤에 <strong>여러 개의 Mamba 층</strong>이 따라오는 형태로 쌓이는데, <strong>“1 : 7”의 비율</strong>이 대표적인 구성이다. 예를 들어 32개의 레이어로 구성된 Jamba 모델이라면 그 중 <strong>4개 층만 어텐션</strong>을 사용하고, 나머지 <strong>28개 층은 Mamba</strong>로 이루어진다.</p>
<p>이렇게 <strong>드문드문 어텐션을 삽입</strong>하고 대부분을 Mamba로 채움으로써, <strong>전역적인 패턴 처리</strong>는 간헐적으로 등장하는 어텐션 층이 담당하고 <strong>나머지 상호작용은 효율적인 Mamba 층</strong>들이 처리하도록 한다. 이 설계는 <strong>메모리 사용량과 속도를 크게 개선</strong>하는데, 특히 어텐션 층이 적으므로 <strong>KV 캐시를 저장해야 할 층 수가 줄어들어 전체 메모리 풋프린트가 작아지고</strong>, <strong>긴 문맥을 처리할 때도</strong> 소수의 어텐션만 계산하면 되므로 <strong>Transformer 대비 훨씬 빠른 토큰 처리 속도</strong>를 얻을 수 있다. 실제로 보고된 바에 따르면, Jamba는 <strong>동일 규모의 일반 Transformer보다 메모리를 1/2 수준만 사용</strong>하면서 <strong>128K 토큰 길이 입력에 대해 3배 이상의 속도</strong>로 텍스트를 생성해냈다.</p>
<p><strong>Q:</strong> Jamba가 MoE를 도입한 이유는 무엇인가? 활성 파라미터와 총 파라미터의 개념을 들어 설명하라.</p>
<p><strong>A:</strong> <strong>Jamba</strong>는 모델 용량을 키우면서도 효율을 유지하기 위해 <strong>MoE (Mixture-of-Experts, 전문가 혼합)</strong> 기법을 도입했다. 구체적으로, 일부 Transformer의 <strong>MLP 층을 MoE 레이어로 대체</strong>하여 <strong>여러 개의 Expert 네트워크</strong>를 두고, <strong>각 토큰마다 상위 몇 개의 Expert만 활성화</strong>되도록 한다. 예를 들어 Jamba에서는 하나의 MoE 레이어에 16개의 Expert MLP가 있고, **토큰마다 가장 관련 높은 2개의 Expert만 가동(top-2 gating)**되도록 설계되었다.</p>
<p>이때 <strong>총 파라미터</strong>란 모든 Expert들을 포함한 전체 모델의 파라미터 수를 의미하고, <strong>활성 파라미터</strong>는 <strong>실제 한 번의 추론에서 활성화되어 계산에 사용되는 파라미터 수</strong>를 뜻한다. Jamba의 경우 MoE 도입으로 **총 파라미터 수는 매우 크게 증가(예: 52억 → 520억 등)**하지만, 매 토큰마다 **극히 일부(예: 상위 2개 Expert)**의 파라미터만 사용되므로 <strong>실제 활성 파라미터 규모는 제한</strong>된다. 예를 들어 Jamba 7B 모델은 MoE를 통해 <strong>총 약 520억 개의 파라미터</strong>를 가지지만, <strong>실제 활성화되는 것은 약 120억 개</strong>에 불과하다.</p>
<p>이렇게 함으로써 **모델의 총 용량(capacity)**은 크게 늘려 <strong>성능 향상</strong>을 도모하면서도, <strong>추론 시 계산량과 메모리 사용량은 활성 파라미터 수준으로 억제</strong>하여 효율을 유지할 수 있다. 요컨대, MoE 도입으로 Jamba는 <strong>“큰 모델의 똑똑함”을 가지되 “작은 모델의 비용”만 치르는 효과</strong>를 얻은 것이다.</p>
<p><strong>Q:</strong> Jamba가 지원하는 최대 컨텍스트(문맥) 길이는 얼마나 되며, 이렇게 긴 문맥을 처리하면서도 성능을 유지하는 비결은 무엇인가?</p>
<p><strong>A:</strong> <strong>Jamba</strong>는 무려 <strong>256K(25만 6천) 토큰</strong>에 달하는 초장문의 <strong>컨텍스트 윈도우</strong>를 지원한다. 이는 현재 공개된 Transformer 계열 모델 중 <strong>가장 긴 수준의 문맥 처리 능력</strong>이며, 이 덕분에 아주 긴 문서를 한 번에 입력하여 질의응답이나 요약을 수행하는 것이 가능하다.</p>
<p>이렇게 긴 문맥을 다루고도 성능을 유지할 수 있는 비결은 앞선 설계 요소들에 있다. 우선, <strong>어텐션 층 수를 최소화</strong>하고 대부분을 Mamba로 구성했기 때문에 <strong>긴 입력에 대해 어텐션 연산으로 인한 부담이 매우 적다</strong>. 또한 Mamba 층은 <strong>선형 시간</strong>에 동작하므로 문맥 길이가 늘어나도 계산 비용이 크게 늘지 않는다. 실제 실험에서 Jamba는 <strong>128K 토큰 입력을 하나의 80GB GPU에서 처리</strong>해냈으며, 동일 규모의 일반 Transformer는 메모리 한계로 이를 처리하지 못하는 반면 Jamba는 <strong>무리 없이 동작하면서도 출력 품질을 최신 LLM 수준으로 유지</strong>했다. 정리하면, Jamba의 아키텍처는 <strong>긴 문맥을 효율적으로 처리하도록 특화</strong>되어 있으며, 덕분에 <strong>긴 입력에서도 빠른 추론과 우수한 성능을 동시에 달성</strong>할 수 있다.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week01"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="ko"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 1 - Transformer 및 차세대 아키텍처</p>
      </div>
    </a>
    <a class="right-next"
       href="../syllabus/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">강의계획서</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer 아키텍처</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba">Mamba 아키텍처</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rwkv">RWKV 아키텍처</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jamba">Jamba 아키텍처</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
