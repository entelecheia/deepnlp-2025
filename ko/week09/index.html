
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 9: 고급 RAG 아키텍처 &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week09/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 10: 정렬 기법의 발전" href="../week10/index.html" />
    <link rel="prev" title="Week 8: 핵심 복습 및 최신 동향" href="../week08/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          한국어 <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    딥러닝자연어처리 (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer 및 차세대 아키텍처</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x와 최신 딥러닝 프레임워크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: 현대적 PEFT 기법을 활용한 효율적 파인튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: 고급 프롬프트 기법과 최적화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM 평가 패러다임과 벤치마크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: 멀티모달 NLP의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: 초장문맥 처리와 효율적 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: 핵심 복습 및 최신 동향</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 9: 고급 RAG 아키텍처</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week10/index.html">Week 10: 정렬 기법의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week11/index.html">Week 11: 프로덕션 에이전트 시스템</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week12/index.html">Week 12: AI 규제와 책임 있는 AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week13/index.html">Week 13: 온톨로지와 AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week14/index.html">Week 14: 2025년 NLP 현황: 확장 모델에서 능동 에이전트로</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workshops/index.html">LLM From Scratch 워크숍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workshops/week01.html">1주차 워크숍: LLM 개요 및 개발 환경 구축</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">프로젝트 운영 가이드라인</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">강의계획서</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/ko/week09/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek09/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week09/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 9: 고급 RAG 아키텍처</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag">1. RAG 진화의 필요성: 장기 기억과 다중 문맥 통합</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hipporag">2. HippoRAG: 생물학적 장기 기억 모방 아키텍처</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag-retrieval-augmented-generation">3. GraphRAG: 지식 그래프 기반 Retrieval-Augmented Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4. 하이브리드 검색: 키워드와 임베딩의 조합</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain-graphrag">5. LangChain을 활용한 GraphRAG 구현 실습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">6. 고급 RAG 적용 사례와 마무리</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">참고자료</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-9-rag">
<h1>Week 9: 고급 RAG 아키텍처<a class="headerlink" href="#week-9-rag" title="Link to this heading">#</a></h1>
<p>이번 주차에서는 <strong>Retrieval-Augmented Generation</strong>(RAG) 기법의 고급 아키텍처를 다룬다. 이전까지 학습한 <strong>벡터 RAG</strong> 방식은 문서를 벡터로 임베딩하고 쿼리에 유사한 벡터를 검색하여 LLM에 전달하는 구조였다. 이는 긴 문맥 한계를 극복하고 <strong>외부 지식베이스</strong>를 활용하는 강력한 방법이지만, 여전히 <strong>다중 문서 간 지식 통합</strong>이나 <strong>연결 관계 추론</strong> 등에서는 한계를 보인다. 이번 강의에서는 이러한 한계를 극복하기 위해 등장한 <strong>차세대 RAG 아키텍처</strong>들을 살펴보고, 이들의 <strong>구조적 특징</strong>과 <strong>실용적 구현</strong> 방법을 학습한다. 특히 <strong>HippoRAG</strong>, <strong>GraphRAG</strong>, <strong>하이브리드 검색</strong> 기법에 집중하며, 앞서 배운 <strong>LangChain</strong> 활용법 및 <strong>벡터 DB</strong>, <strong>FlashAttention</strong>, <strong>PEFT</strong> 등의 개념과 자연스럽게 연결지어 설명한다.</p>
<section id="rag">
<h2>1. RAG 진화의 필요성: 장기 기억과 다중 문맥 통합<a class="headerlink" href="#rag" title="Link to this heading">#</a></h2>
<p>기존 RAG 시스템(벡터 기반)은 사용자의 질문을 받아 관련 문서를 <strong>임베딩 유사도</strong>로 검색하고, 찾아온 몇 개의 문서를 LLM에 컨텍스트로 주어 답변을 생성하는 구조였다. 이런 <strong>벡터 RAG</strong> 접근은 LLM이 사전에 학습하지 않은 <strong>외부 지식</strong>을 동적으로 활용할 수 있게 해 주었지만, 다음과 같은 한계가 드러났다:</p>
<ul class="simple">
<li><p><strong>다중-홉 추론의 어려움</strong>: 답변에 필요한 정보가 <strong>여러 문서에 분산</strong>된 경우, 벡터 검색만으로 관련 조각들을 모두 찾기 어렵다. 기존 해결책으로 <strong>Iterative RAG</strong> (예: IRCoT 등)로 여러 번 질의-생성을 반복하는 방법이 사용되었으나, 비효율적이고 여전히 누락이 발생한다.</p></li>
<li><p><strong>연관 관계 맥락 부족</strong>: 벡터 임베딩은 의미 유사성은 잘 포착하지만, 문서들 간의 <strong>명시적 관계</strong> (예: A가 B의 부분, 원인-결과 관계 등)를 표현하지 못한다. 따라서 <strong>지식 그래프</strong>처럼 관계를 표현하거나, 키워드 기반 정확 매칭이 필요한 경우 대응이 어렵다.</p></li>
<li><p><strong>장기 메모리 관리 부재</strong>: 현재까지 RAG는 새 문서를 추가하면 <strong>기존 벡터 DB에 무한 축적</strong>하는 방식이어서, 시간이 지남에 따라 <strong>불필요한 정보 누적</strong>이나 <strong>노이즈 증가</strong> 문제가 있다. 인간의 기억처럼 오래된 정보를 잊거나 통합 정리(consolidation)하는 기능이 부족했다.</p></li>
</ul>
<p>이러한 이유로, RAG를 <strong>인간 두뇌의 장기 기억 체계</strong>에 좀더 가깝게 발전시키려는 시도가 이루어졌다. 즉, <strong>구조화된 인덱스</strong> (예: 지식 그래프), <strong>의도적 망각</strong> (오래된 정보 제거), <strong>지식 통합</strong> (요약 및 관계 추출) 등의 기능을 갖춘 <strong>진화된 RAG</strong> 아키텍처가 등장하고 있다. 이번 섹션들에서 다룰 HippoRAG, GraphRAG, 하이브리드 검색 등은 이러한 맥락에서 개발된 기법들이다. 또한 이러한 고급 RAG 구조는 <strong>FlashAttention</strong>과 같은 효율적 Transformer 기법(긴 문맥 처리)이나 <strong>PEFT</strong> 기반 미세조정(지식 주입)과 상호보완적으로 작용할 수 있다 – 예를 들어, <strong>RAG</strong>로 외부 지식을 가져오면서 <strong>FlashAttention</strong>으로 긴 컨텍스트를 효율 처리하거나, <strong>PEFT</strong>로 LLM을 미세조정하여 RAG 통합 지식을 더 잘 활용하도록 하는 식이다.</p>
<section id="id1">
<h3>체크포인트 질문<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>왜 기본 벡터 RAG 시스템만으로는 <strong>여러 문서에 걸친 지식 통합</strong> (멀티-홉 질의)에 한계가 있을까? 그 한계를 보완하기 위해 어떤 접근이 시도되어 왔나?</p></li>
<li><p>인간 두뇌의 장기 기억 기능과 비교할 때, 기존 RAG 구조에는 어떤 <strong>메모리 관리 기능</strong>이 부족한가? (예: 망각, 통합 등)</p></li>
<li><p>FlashAttention이나 PEFT와 같은 기법들은 RAG와 어떻게 조합될 수 있을까? 이러한 조합을 통해 얻을 수 있는 <strong>이점</strong>은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="hipporag">
<h2>2. HippoRAG: 생물학적 장기 기억 모방 아키텍처<a class="headerlink" href="#hipporag" title="Link to this heading">#</a></h2>
<p><strong>HippoRAG</strong>는 <strong>인간 해마</strong> (hippocampus)의 기억 형성 이론에서 영감을 받아 고안된 고급 RAG 프레임워크다. 이 접근법은 <strong>NeurIPS 2024</strong>에서 발표되었으며, <strong>해마-대뇌피질 간 상호작용</strong>을 모방하여 LLM의 장기 지식통합 능력을 향상시키는 것이 핵심이다. HippoRAG의 작동을 이해하기 위해 우선 영감이 된 <strong>해마 기반 기억 인덱싱 이론</strong>을 간략히 살펴보자:</p>
<p><img alt="해마 기반 기억 인덱싱 이론" src="../_images/hippocampus-memory-theory.jpeg" />
<em>그림 1: 인간 해마의 기억 형성 이론과 HippoRAG 아키텍처의 대응 관계</em></p>
<ul class="simple">
<li><p><strong>대뇌피질</strong> (Neocortex): 감각경험을 추상화하여 <strong>고차원 표현</strong>으로 저장하는 역할 – 인간 뇌에서는 실제 기억 내용이 여러 피질 영역에 분산 저장된다.</p></li>
<li><p><strong>해마</strong> (Hippocampus): <strong>기억의 인덱스</strong> 역할 – 개별 경험을 고유한 패턴으로 분리(<strong>pattern separation</strong>), 일부 단서로 전체 기억을 복원(<strong>pattern completion</strong>)하는 역할을 한다. 해마는 새로운 경험이 들어오면 자신만의 인덱스를 갱신하고, 기존 대뇌피질 기억을 덮어쓰지 않음으로써 **연속학습(continuously learning)**이 가능하게 해 준다.</p></li>
<li><p><strong>Para-hippocampal region</strong> (PHR): 대뇌피질과 해마를 연결하는 중계 – 유사한 개념들을 연결하여 해마로 보낼 때 <strong>유사성 링크</strong>를 형성해 준다.</p></li>
</ul>
<p>이 이론을 RAG에 적용한 것이 HippoRAG이다. HippoRAG는 <strong>오프라인 인덱싱 단계</strong>와 <strong>온라인 검색 단계</strong>의 두 단계로 동작한다:</p>
<p><img alt="HippoRAG 전체 아키텍처" src="../_images/hipporag-architecture.jpeg" />
<em>그림 2: HippoRAG의 전체 아키텍처 - 오프라인 인덱싱과 온라인 검색 단계</em></p>
<p><strong>(1) 오프라인 인덱싱 – 인공 해마 인덱스 구축:</strong> 원문 문서들을 <strong>지식 그래프</strong> (knowledge graph) 형태로 변환하여, 일종의 **“해마 인덱스”**를 미리 만들어 두는 과정이다. 구체적인 절차는 다음과 같다:</p>
<ul class="simple">
<li><p><strong>LLM 기반 OpenIE</strong>: 대용량 LLM(예: GPT-4 등)을 활용하여 각 문서에서 <strong>주요 사실</strong> (triple)을 추출한다. 예컨대 문장 “Thomas 교수는 Stanford에서 Alzheimer 연구를 한다”에서 (Thomas, researches, Alzheimer’s), (Stanford, employs, Thomas) 같은 삼원소(triple)를 추출한다. 이는 인간 대뇌피질이 경험을 처리하여 요소로 분해하는 과정에 대응된다. 이러한 <strong>Open Information Extraction</strong>을 통해 문서 내용을 <strong>밀집 벡터</strong>가 아닌 <strong>구조화된 정보 조각</strong>으로 저장함으로써, 각 사실들이 서로 섞이지 않고 **구분 저장(pattern separation)**된다.</p></li>
<li><p><strong>지식 그래프 구성</strong>: 추출된 모든 triple들의 <strong>주어/객체</strong>를 노드로, <strong>관계</strong>를 엣지로 하여 <strong>스키마 없는 지식 그래프</strong>를 형성한다. 이 그래프는 모든 문서의 지식을 통합한 <strong>연결망 형태의 인덱스</strong>가 된다.</p></li>
<li><p><strong>시놉시스 링크 추가</strong> (PHR 역할): PHR에 해당하는 부분으로, <strong>동의어나 유사 개념</strong>을 연결하는 추가 엣지를 부여한다. 이를 위해 각 노드(개념)에 대해 <strong>임베딩 유사도</strong>를 계산하고, 코사인 유사도가 임계치 이상인 노드 쌍 사이에 <strong>유의어 엣지</strong>를 추가한다. 예컨대 “Alzheimer’s”와 “Alzheimer’s disease” 노드 간에 연결을 형성하여, 검색 시 한쪽을 보면 다른 쪽도 활성화되도록 만든다. 이는 해마로 전달되기 전 PHR이 유사 기억들을 연계해주는 기능과 유사하다.</p></li>
<li><p><strong>노드-문서 맵핑 저장</strong>: 그래프의 각 노드(개념)가 어떤 원문 단락에 등장했는지 기록한 매핑표를 저장한다. 이는 이후 검색 결과를 다시 실제 원문 문단으로 환원하기 위한 정보다.</p></li>
</ul>
<p>이렇게 구축된 <strong>지식 그래프 인덱스</strong>는 인간 뇌의 해마가 구축한 <strong>연합 기억망</strong>에 해당한다. 이 인덱스는 전통적인 벡터 DB보다 <strong>메모리 효율적</strong>일 수 있는데, 벡터로 전체 문서를 저장하는 대신 <strong>핵심 개념 노드들만 저장</strong>하므로 저장 공간을 크게 절약한다 (연구에 따르면 약 <strong>25%</strong> 저장 공간 절감 효과 보고). 또한 개념 간 관계가 명시적으로 연결되어 있으므로, <strong>연결 중심의 추론</strong>에 강점을 보인다.</p>
<p><img alt="지식 그래프 인덱스 구축" src="../_images/knowledge-graph-index.jpeg" />
<em>그림 3: 문서에서 지식 그래프 인덱스 구축 과정</em></p>
<p><strong>(2) 온라인 검색 – 해마 인덱스를 통한 기억 복원:</strong> 사용자의 질의가 들어오면, HippoRAG는 <strong>부분 단서만으로 전체 기억을 복원</strong>하는 과정을 모방하여 관련 정보를 찾아낸다. 단계는 다음과 같다:</p>
<ul class="simple">
<li><p><strong>질의 개체 추출</strong> (Neocortex 역할): LLM을 한 번 호출하여 사용자 질문에 등장하는 <strong>중요 개체명</strong>이나 <strong>키워드</strong>를 뽑아낸다. 예를 들어 질문이 “Alzheimer 연구를 하는 Stanford 교수는 누구인가?”라면, 추출 결과 <strong>{Stanford, Alzheimer’s}</strong> 같은 집합이 나온다.</p></li>
<li><p><strong>그래프 시드 노드 선택</strong>: 추출된 질의 개체들을 각각 <strong>임베딩</strong>하여, 그래프 내에서 가장 유사한 노드들을 찾는다. 위 예시에서 <strong>Stanford</strong>와 <strong>Alzheimer’s</strong> 노드에 가장 가까운 노드는 아마도 <strong>Thomas Südhof</strong>일 것이다. 이렇게 선택된 노드들이 해마 검색의 <strong>출발 지점</strong> (partial cue) 역할을 한다.</p></li>
<li><p><strong>Personalized PageRank (PPR) 기반 그래프 탐색</strong>: HippoRAG의 핵심 단계이다. 선택된 시드 노드들로부터 그래프 위에서 <strong>개인화 페이지랭크</strong> 알고리즘을 수행한다. PPR은 시드 노드에서 출발하여 인접한 노드로 확률적 “확산”을 하는데, 시드 근처의 노드에 높은 확률이 몰리게 조율된다. 이는 해마가 부분 단서로 관련 기억 전체를 활성화(Pattern Completion)하는 것에 대응된다. PPR 결과로 그래프 내 연관된 노드들이 <strong>가중치</strong>를 부여받는데, 이는 해당 개념이 <strong>현재 질의와 얼마나 관련 깊은지</strong>를 나타낸다.</p></li>
<li><p><strong>관련 원문 조각 복원</strong>: PPR로 얻은 상위 노드들을, <strong>노드-문서 맵핑표</strong>를 사용해 다시 원문 단락으로 매핑한다. 각 단락마다 그 안에 등장한 노드들의 PPR 점수를 합산하여 <strong>최종 순위</strong>를 결정한다. 이렇게 가장 관련성 높은 문단 몇 개를 최종 LLM에 전달하여 답변을 생성한다.</p></li>
</ul>
<p>HippoRAG는 이러한 과정을 통해 <strong>한 번의 검색으로 다중-홉 정보 연결</strong>을 이뤄낸다. 예를 들어 앞의 Stanford/Alzheimer’s 예시에서, 기존 벡터 RAG라면 **”Stanford”**로 한 번 검색하고 결과를 바탕으로 다시 <strong>“Thomas Südhof”</strong> 정보를 찾는 <strong>두 단계 검색</strong>이 필요했겠지만, HippoRAG는 <strong>그래프</strong>를 통해 <strong>한 번에</strong> “Stanford→Thomas Südhof←Alzheimer’s” 연결을 찾아낸다. 실험 결과에 따르면, HippoRAG는 <strong>2단계 이상 지식 통합이 필요한 QA에서 기존 SOTA 대비 최대 20%</strong> 높은 정확도를 보였고, <strong>IRCoT</strong> 같은 반복 검색 기법 대비 <strong>10~20배</strong> 저렴하고 <strong>6~13배</strong> 빠른 성능을 달성했다.</p>
<p>비유하자면, <strong>벡터 RAG</strong>가 각 문서를 섬으로 보고 질문과 비슷한 섬을 몇 개 고르는 방식이라면, <strong>HippoRAG</strong>는 문서들을 잇는 다리를 미리 놓아두고 질문의 단서를 지닌 섬에서 시작해 <strong>다리를 따라 이동</strong>함으로써 목적지 정보를 찾아오는 셈이다. 이러한 <strong>Neuro-symbolic</strong> 결합 덕분에, HippoRAG는 인간처럼 부분 정보로부터 연관된 전체 맥락을 재구성하는 능력을 LLM에 부여한다.</p>
<section id="id2">
<h3>체크포인트 질문<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>HippoRAG의 오프라인 인덱싱 단계에서 <strong>벡터 임베딩</strong> 대신 <strong>지식 그래프</strong>를 구축하는 이유는 무엇인가? 이를 통해 얻는 이점은 무엇일까?</p></li>
<li><p>HippoRAG에서 <strong>Personalized PageRank</strong> (PPR) 알고리즘은 어떤 역할을 하며, 이를 통해 다중 문서 추론이 어떻게 향상되는가?</p></li>
<li><p>HippoRAG와 기존 벡터 RAG 또는 IRCoT와 비교했을 때, <strong>응답 정확도</strong>나 <strong>속도 측면</strong>에서 어떠한 개선이 보고되었는가?</p></li>
</ul>
</section>
</section>
<section id="graphrag-retrieval-augmented-generation">
<h2>3. GraphRAG: 지식 그래프 기반 Retrieval-Augmented Generation<a class="headerlink" href="#graphrag-retrieval-augmented-generation" title="Link to this heading">#</a></h2>
<p><strong>GraphRAG</strong>는 마이크로소프트 등에서 제안한 <strong>그래프 강화 RAG</strong> 아키텍처로, <strong>지식 그래프</strong>를 활용하여 대규모 문서의 <strong>주제별 연관성</strong>을 체계적으로 활용하는 접근이다. 개념적으로 앞서 살펴본 HippoRAG와 유사하게 <strong>문서 간 관계</strong>를 명시적으로 모델링하지만, 구현과 응용 면에서 차이가 있다. GraphRAG의 목표는 <strong>비구조화 텍스트를 구조화된 지식 형태로 변환</strong>하여 <strong>질의응답의 정확성</strong>과 <strong>해석 가능성</strong>을 높이는 것이다.</p>
<p><img alt="GraphRAG 파이프라인" src="../_images/graphrag-pipeline.jpeg" />
<em>그림 4: GraphRAG의 전체 파이프라인 구조</em></p>
<p>GraphRAG의 전형적인 <strong>워크플로우</strong>는 다음과 같이 요약된다:</p>
<ol class="arabic simple">
<li><p><strong>문서 → 텍스트 청크 분할</strong>: 대용량 문서를 일정 길이로 분할한다. (너무 긴 청크는 정보 손실을 유발하고, 너무 짧으면 검색 부정확성과 비용 문제가 생기므로 적절한 <strong>청크 크기</strong>를 선정해야 한다.)</p></li>
<li><p><strong>엔티티 및 관계 추출</strong>: 각 청크를 LLM을 이용해 처리하여 <strong>주요 엔티티</strong> (명사구), <strong>관계</strong> (동사/전치사 구), <strong>사실</strong> (claim, 수치) 등을 추출한다. LLM이 추출한 각 요소에는 간략한 설명을 붙여 <strong>해석 가능성</strong>을 높인다. (예: “NeoChip”이라는 회사 엔티티 추출 시 “저전력 프로세서를 전문으로 하는 공개 기업” 등의 설명을 함께 생성.)</p></li>
<li><p><strong>지식 그래프 구성</strong>: 추출된 엔티티들을 노드로, 관계를 엣지로 하여 <strong>가중치가 부여된 지식 그래프</strong>를 만든다. 만약 동일한 관계가 여러 문서에서 등장하면 해당 엣지 가중치를 높여 <strong>중복 중요도</strong>를 표시한다. 또한 중복된 엔티티나 사실은 <strong>클러스터링 및 요약</strong>하여 그래프를 간결하게 만든다.</p></li>
<li><p><strong>그래프 커뮤니티 분석</strong>: 완성된 그래프에 대해 <strong>커뮤니티 탐지 알고리즘</strong> (예: Leiden 알고리즘)을 적용하여, 노드들이 촘촘히 연결된 <strong>토픽 클러스터</strong>들을 식별한다. 이러한 <strong>계층적 클러스터링</strong>을 통해 거대 지식 그래프를 <strong>주제별 하위 그래프</strong>들로 나누어 다룰 수 있고, 이는 이후 <strong>요약</strong> 및 <strong>검색 범위 축소</strong>에 유용하다.</p></li>
<li><p><strong>커뮤니티별 요약 생성</strong>: 찾아낸 각 커뮤니티(주제)에 대해 그 내용을 대표하는 요약을 생성한다. 이때 해당 토픽에서 <strong>중심 노드</strong> (연결도가 높은 중요 엔티티)들을 우선 포함하도록 하여 <strong>의미 밀도</strong>를 최대화한다. 그래프 계층이 여러 단계인 경우, 상위 계층은 하위 요약들을 포함하는 식으로 <strong>재귀적 요약</strong>을 수행하여, <strong>다중 해상도</strong>의 주제 요약 정보를 얻는다.</p></li>
<li><p><strong>계층적 질의 응답</strong>: 사용자의 질문이 들어오면, 우선 질문에 맞는 <strong>커뮤니티 요약</strong>을 선별하여 (필요하면 여러 주제의 요약 혼합), 이를 LLM에 투입해 <strong>1차 응답들</strong> (Map 단계)을 생성한다. 그런 다음 각 응답의 <strong>유용성 점수</strong>를 평가하고, 상위 응답들을 다시 모아 <strong>최종 답변</strong> (Reduce 단계)을 생성한다. 이러한 <strong>맵-리듀스</strong> 방식은 거대한 문서군에 대해서도 계층적으로 답변을 구성함으로써, <strong>전반적인 정확성과 일관성</strong>을 높여준다.</p></li>
</ol>
<p>위 GraphRAG 파이프라인을 통해, 단순 임베딩 기반 검색 대비 <strong>설명 가능하고 주제 구조화된 검색</strong>이 가능해진다. 예를 들어 대기업 내부 보고서 수천 개를 GraphRAG로 처리하면, <strong>분야별 지식 그래프 + 요약</strong>이 생성되어, 사용자가 “최근 3년간 AI 연구 성과 요약” 같은 질문을 하면 해당 커뮤니티 요약들을 이용해 빠르고 정확한 답변을 생성할 수 있다. 또한 답변 근거로 그래프 경로나 요약을 제시함으로써 <strong>신뢰성</strong>을 높일 수 있다.</p>
<p><img alt="커뮤니티 분석과 요약" src="../_images/community-analysis.jpeg" />
<em>그림 5: 그래프 커뮤니티 분석과 계층적 요약 생성 과정</em></p>
<p>마이크로소프트가 발표한 GraphRAG 사례에서는, **위키백과 소설 “Christmas Carol”**을 GraphRAG 파이프라인으로 구조화하여 <strong>질의응답</strong>을 수행하는 예시를 보였다. 이를 통해 전통적인 RAG보다 <strong>높은 응답 정확도</strong> (최대 99% 정밀도)를 시현하고, 복잡한 대규모 데이터셋에서도 <strong>이해 가능한 추론 과정</strong> (knowledge graph 경로, 요약 내용)을 사용자에게 제공할 수 있음을 보였다.</p>
<p>GraphRAG는 개념상 HippoRAG와 유사하지만, 구현에서는 <strong>오픈소스 툴</strong> (예: Microsoft의 <strong>GraphRAG 라이브러리</strong>)을 통해 <strong>Knowledge Graph + 벡터 DB 결합</strong> 형태로 제공되기도 한다. 또한 GraphRAG는 <strong>데이터 엔지니어링 측면</strong> (그래프 생성, 커뮤니티 분석 등)의 작업이 많아 <strong>사전 구축 비용</strong>이 크지만, 일단 구축된 그래프를 활용하면 <strong>질의응답 단계의 비용은 절감</strong>되는 장점이 있다.</p>
<section id="id3">
<h3>체크포인트 질문<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>GraphRAG 파이프라인에서 <strong>지식 그래프</strong>와 <strong>커뮤니티 탐지</strong>를 도입하는 목적은 무엇인가? 이러한 구조화가 질의응답에 어떻게 기여하는지 설명해보자.</p></li>
<li><p>GraphRAG의 <strong>맵-리듀스 질의응답 전략</strong>은 무엇이며, 왜 이런 접근이 대규모 문서 집합에 효과적일까?</p></li>
<li><p>HippoRAG와 GraphRAG는 모두 그래프를 사용하지만, <strong>구현 방법</strong>이나 <strong>적용 범위</strong> 면에서 어떤 차이가 있을까? 예를 들어 실시간 동적 업데이트 측면에서 두 기법을 비교해보자.</p></li>
</ul>
</section>
</section>
<section id="id4">
<h2>4. 하이브리드 검색: 키워드와 임베딩의 조합<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p><strong>하이브리드 검색</strong>은 전통 <strong>희소</strong> (Lexical) 검색과 <strong>밀집</strong> (Vector) 검색을 결합하여 두 방법의 장점을 모두 활용하는 접근이다. 앞서 다룬 HippoRAG나 GraphRAG가 <strong>지식 구조화</strong>에 초점을 맞췄다면, 하이브리드 검색은 <strong>검색 알고리즘 수준</strong>에서의 개선으로, <strong>정확한 키워드 매칭 능력</strong>과 <strong>의미적 유사도 추론 능력</strong>을 동시에 활용한다.</p>
<p><img alt="하이브리드 검색 구조" src="../_images/hybrid-search-architecture.jpeg" />
<em>그림 6: 하이브리드 검색의 구조와 BM25 + 벡터 검색 결합 방식</em></p>
<p>전통적으로 <strong>BM25</strong>와 같은 <strong>키워드 기반 랭킹</strong>은 질의와 문서 사이의 <strong>정확한 용어 일치</strong>에 강하며, 오타나 약어, 이름 등을 정확히 찾아줄 수 있다. 그러나 의미가 같아도 표현이 다르면 놓치기 쉽고, 문맥적 의미를 반영하지 못하는 단점이 있다. 반대로 <strong>임베딩 기반 벡터 검색</strong>은 문장의 <strong>의미적 유사성</strong>을 잘 포착하여 표현이 달라도 관련있는 결과를 찾지만, 정확한 키워드(예: 코드 스니펫, 고유명사 등)는 지나칠 수 있다.</p>
<p>하이브리드 검색은 <strong>이 두 가지를 병렬로 수행</strong>한 후 결과를 <strong>통합</strong> (score fusion)하는 방식이다. 예를 들어 OpenAI Embedding + BM25 조합의 하이브리드 검색에서는, 하나의 질의에 대해 <strong>BM25 스코어</strong>와 <strong>코사인 유사도 스코어</strong>를 각각 구한 뒤 <strong>가중 합산 또는 Rank-Fusion</strong> 기법으로 최종 순위를 매긴다. 단순 가중합의 경우:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">final_score</span> <span class="o">=</span> <span class="n">α</span> <span class="o">*</span> <span class="p">(</span><span class="n">BM25_score_normalized</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">α</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Vector_score</span><span class="p">)</span>
</pre></div>
</div>
<p>형태로 조정할 수 있다. (α는 두 검색의 중요도 비율) 또는 **Reciprocal Rank Fusion (RRF)**처럼, 각각의 결과 목록에서의 <strong>등수</strong>에 기반하여 점수를 합산하는 기법도 널리 쓰인다. RRF는 각 검색 방법의 스코어 분포 차이에 영향받지 않으면서 두 결과를 섞을 수 있어 자주 사용된다.</p>
<p><strong>LangChain</strong>에서는 이러한 하이브리드 검색을 쉽게 구현하도록 EnsembleRetriever 등을 제공한다. 다음은 LangChain을 활용해 <strong>BM25 + 벡터 임베딩</strong> 하이브리드 검색기를 구성하는 예시 코드다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BM25Retriever</span><span class="p">,</span> <span class="n">EnsembleRetriever</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.embeddings.openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>

<span class="c1"># 1. 문서 리스트 (예시 문장들)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The capital of France is Paris.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Machine learning enables computers to learn from data.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The Mona Lisa was painted by Leonardo da Vinci.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Python is a popular programming language for AI.&quot;</span>
<span class="p">]</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">]</span>

<span class="c1"># 2. BM25 기반 Retriever 생성</span>
<span class="n">bm25_retriever</span> <span class="o">=</span> <span class="n">BM25Retriever</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">bm25_retriever</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 상위 2개 결과 사용</span>

<span class="c1"># 3. 벡터 기반 Retriever 생성 (OpenAI 임베딩 + FAISS)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>  <span class="c1"># 임베딩 모델 (예: text-embedding-ada-002)</span>
<span class="n">vector_store</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">vector_retriever</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>

<span class="c1"># 4. Ensemble (하이브리드) Retriever 생성 (BM25 40%, 벡터 60% 가중치)</span>
<span class="n">hybrid_retriever</span> <span class="o">=</span> <span class="n">EnsembleRetriever</span><span class="p">(</span>
    <span class="n">retrievers</span><span class="o">=</span><span class="p">[</span><span class="n">bm25_retriever</span><span class="p">,</span> <span class="n">vector_retriever</span><span class="p">],</span>
    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># 5. 예시 질의 실행</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Who painted the famous portrait of a woman?&quot;</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">hybrid_retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Result </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>실행 결과:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Result</span> <span class="mi">1</span><span class="p">:</span> <span class="n">The</span> <span class="n">Mona</span> <span class="n">Lisa</span> <span class="n">was</span> <span class="n">painted</span> <span class="n">by</span> <span class="n">Leonardo</span> <span class="n">da</span> <span class="n">Vinci</span><span class="o">.</span>
<span class="n">Result</span> <span class="mi">2</span><span class="p">:</span> <span class="n">The</span> <span class="n">capital</span> <span class="n">of</span> <span class="n">France</span> <span class="ow">is</span> <span class="n">Paris</span><span class="o">.</span>
</pre></div>
</div>
<p>위 결과에서 볼 수 있듯, 질의 “유명한 여성 초상화를 그린 사람은?”에 대해 하이브리드 검색은 <strong>Mona Lisa 관련 문장</strong>을 1순위로 찾아낸다. 순수 벡터 검색만 했다면 “Mona Lisa”라는 정확 단어가 없으면 놓칠 수 있고, BM25만 했다면 “초상화” 등의 키워드 매칭에 의존하여 부정확할 수 있다. 하이브리드 방식은 <strong>키워드 일치</strong> (Mona Lisa→portrait 매칭)와 <strong>의미 유사</strong> (famous woman portrait→Mona Lisa 그림 의미 연결) 양쪽을 다 반영하므로 최적의 결과를 얻는다.</p>
<p><img alt="하이브리드 검색 비교" src="../_images/hybrid-search-comparison.jpeg" />
<em>그림 7: 벡터 검색, BM25 검색, 하이브리드 검색의 결과 비교</em></p>
<p>하이브리드 검색은 특히 <strong>전문 용어나 코드, 고유명사</strong>가 중요한 도메인에서 유용하다. 예컨대 의료 문서에서 <strong>약어</strong> (“BP” vs “Blood Pressure”)처럼 벡터로는 잡기 어려운 항목이나, 소스 코드 질의처럼 <strong>문자 그대로 일치해야 의미 있는 경우</strong>에 BM25가 보완적 역할을 한다. 실제 사례로 Stack Overflow는 기존 TF-IDF 기반 검색을 버리고 <strong>임베딩 + 키워드</strong> 하이브리드로 전환하여, 코드 포함 질문에서 키워드 매칭을 놓치지 않으면서도 의미적 관련 답변을 찾아내 <strong>검색 품질을 크게 향상</strong>시켰다.</p>
<p>물론 하이브리드 검색에도 <strong>고려사항</strong>이 있다. 두 가지 검색을 병렬 수행하므로 <strong>레이턴시</strong> (latency)가 증가할 수 있고, 점수 결합/정규화에 따른 <strong>튜닝 작업</strong>이 필요하다. 하지만 대부분 벡터 DB들이 기본적으로 하이브리드 기능(예: Elastic, Pinecone, Weaviate 등)이나 API 지원을 제공하고 있어 구현 난이도는 낮아지고 있다.</p>
<p>정리하면, 하이브리드 검색은 **”정확히 물어보는 것”**과 <strong>“의도를 이해해서 찾아주는 것”</strong> 사이의 균형을 잡아준다. 이는 RAG 시스템의 <strong>Retriever</strong> 단계 성능을 극대화하여, LLM에게 <strong>더 좋은 컨텍스트</strong>를 공급함으로써 최종 답변의 품질을 높이는 효과가 있다.</p>
<section id="id5">
<h3>체크포인트 질문<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>하이브리드 검색에서 <strong>BM25와 임베딩 검색</strong>은 각각 어떤 타입의 정보를 잘 찾아주는가? 두 방식을 결합하면 시너지 효과가 나는 이유는 무엇인가?</p></li>
<li><p>하이브리드 검색 구현 시, 서로 다른 검색 스코어를 결합해야 한다. <strong>정규화</strong>나 <strong>Rank Fusion</strong> 기법은 이러한 결합 문제를 어떻게 해결해 주는가?</p></li>
<li><p>본인이 RAG 시스템을 구축한다면, 어떠한 상황에서 순수 벡터 검색보다 하이브리드 검색을 채택하는 것이 유리할지 사례를 들어 설명해보자.</p></li>
</ul>
</section>
</section>
<section id="langchain-graphrag">
<h2>5. LangChain을 활용한 GraphRAG 구현 실습<a class="headerlink" href="#langchain-graphrag" title="Link to this heading">#</a></h2>
<p>앞서 소개한 고급 RAG 기법들은 구현 난이도가 높아 보일 수 있지만, 다행히도 <strong>LangChain</strong>과 같은 프레임워크를 활용하면 일부 기능을 비교적 손쉽게 실습해볼 수 있다. 특히 <strong>GraphRAG</strong>의 경우, LangChain에는 <strong>그래프 질의 체인</strong>과 <strong>그래프 상태 머신</strong>을 지원하는 모듈들이 존재한다. 이번 섹션에서는 LangChain으로 <strong>간단한 GraphRAG</strong> 워크플로우를 구현하는 방법을 소개한다.</p>
<p>먼저, LangChain의 <strong>GraphQAChain</strong>을 사용한 기본적인 그래프 질의응답 예제이다. 이 체인은 <strong>그래프 데이터베이스</strong>와 LLM을 연결하여, 그래프 상의 쿼리를 수행하고 결과를 LLM이 자연어로 답변하게 해 준다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_experimental.graph_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMGraphTransformer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">GraphQAChain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.graphs.networkx_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">NetworkxEntityGraph</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># 1. 예시 텍스트와 LLM 준비</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Marie Curie was a physicist. Marie Curie won a Nobel Prize. Marie Curie worked in Paris.&quot;</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;gpt-3.5-turbo&#39;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># OpenAI LLM 예시</span>

<span class="c1"># 2. LLM Graph Transformer로 텍스트를 그래프 triple로 변환</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span>
<span class="n">graph_transformer</span> <span class="o">=</span> <span class="n">LLMGraphTransformer</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span>
<span class="n">graph_docs</span> <span class="o">=</span> <span class="n">graph_transformer</span><span class="o">.</span><span class="n">convert_to_graph_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># 3. NetworkX 그래프 객체 생성 및 노드/엣지 추가</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">NetworkxEntityGraph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph_docs</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">graph_docs</span><span class="o">.</span><span class="n">relationships</span><span class="p">:</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">edge</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">edge</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">relation</span><span class="o">=</span><span class="n">edge</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

<span class="c1"># 4. GraphQAChain 생성 (그래프와 LLM을 연결)</span>
<span class="n">graph_qa_chain</span> <span class="o">=</span> <span class="n">GraphQAChain</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 5. 질의 실행</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;What did Marie Curie win?&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">graph_qa_chain</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>위 코드에서는 한 문단에 대한 간단한 <strong>지식 그래프</strong>를 LLM으로부터 추출하고(convert_to_graph_documents), 이를 NetworkX 그래프로 변환한 후, <strong>GraphQAChain</strong>을 통해 질의응답을 한다. 예시 질의 “Marie Curie가 무엇을 수상했는가?”에 대한 실행 결과는 다음과 같을 수 있다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Marie</span> <span class="n">Curie</span> <span class="n">won</span> <span class="n">a</span> <span class="n">Nobel</span> <span class="n">Prize</span><span class="o">.</span>
</pre></div>
</div>
<p>LangChain의 GraphQAChain은 내부적으로 <strong>질의 분석 → Cypher(그래프 질의어) 생성 → 그래프 DB 조회 → 결과 종합 → LLM 답변</strong>의 단계를 거친다. 덕분에 사용자는 복잡한 Cypher 문법이나 그래프 탐색 로직을 직접 짤 필요 없이, <strong>자연어 질의만으로</strong> 그래프 정보를 활용한 답변을 얻을 수 있다. 이는 GraphRAG의 <strong>온라인 검색 단계</strong>를 LangChain이 추상화해준 예라고 볼 수 있다.</p>
<p>또한 LangChain에는 <strong>그래프+벡터 혼합체인</strong> 구성도 가능하다. 예컨대 <strong>GraphCypherQAChain</strong>은 <strong>벡터 임베딩</strong>을 사용해 질의를 그래프 쿼리로 변환하고, <strong>Neo4j</strong> 등 그래프DB에서 Cypher를 실행한 뒤, <strong>추가적인 임베딩 검색과 결합</strong>하는 <strong>하이브리드 체인</strong>이다. 복잡한 예지만, Microsoft의 LangGraph 확장을 이용하면 아래와 같이 <strong>분기 있는 워크플로우</strong>를 구축할 수 있다:</p>
<p><img alt="LangChain GraphRAG 워크플로우" src="../_images/langchain-graphrag-workflow.jpeg" />
<em>그림 8: LangChain 기반 GraphRAG 체인 구성 - 그래프 질의와 벡터 검색을 분기 처리하는 워크플로우</em></p>
<p>위 그림은 Neo4j에서 공개한 GraphRAG 예제 워크플로우를 단순화하여 표현한 것이다. <strong>START</strong> 노드에서 질문이 들어오면, <strong>route_question</strong> 함수가 질문 내용을 분석해 <strong>벡터 검색이 필요하면 오른쪽 분기</strong>로, <strong>단순 그래프 질의면 왼쪽 분기</strong>로 분기시킨다. 오른쪽 분기에서는 먼저 <strong>Decomposer</strong>가 복잡한 질문을 두 부분으로 쪼갠다 (예: “논문을 찾아라” + “그 논문의 저자를 구하라”). 그런 다음 <strong>VECTOR_SEARCH</strong> 노드에서 첫 번째 부분을 처리해 유사한 문서들을 찾고, 그 결과를 <strong>PROMPT_TEMPLATE_WITH_CONTEXT</strong> 노드에서 두 번째 그래프 질의 부분과 합쳐 Cypher 쿼리를 생성한다. 마지막으로 <strong>GRAPH_QA_WITH_CONTEXT</strong> 노드가 그래프 DB에서 답을 찾아 LLM으로 최종 응답을 준다. 왼쪽 분기는 전통적인 GraphQA 과정을 수행한다.</p>
<p>이러한 LangChain 기반 구현을 통해 얻을 수 있는 교훈은, <strong>고급 RAG 아키텍처들도 충분히 모듈화하여 구성할 수 있다</strong>는 점이다. 즉, <strong>Retriever</strong>를 다단계로 구성하거나 <strong>Chain</strong>을 분기/병합하여, HippoRAG나 GraphRAG에서 인간이 설계한 논리를 그대로 재현 가능하다. 예컨대 HippoRAG를 LangChain으로 구현하려면, 문서 임베딩 대신 <strong>LLM triple 추출 + 그래프 빌드</strong> 단계와, 질의 시 <strong>NER + PPR 그래프 traversal</strong> 단계를 Python 함수로 만들어서 Chain에 결합하면 된다. 실제로 HippoRAG의 공개 코드 또한 이러한 절차를 Python으로 구현하고 있다.</p>
<p>정리하면, <strong>LangChain</strong>은 고급 RAG 아이디어들을 손쉽게 실습해볼 수 있는 <strong>훌륭한 도구</strong>다. 이를 활용하면 학습자가 직접 <strong>고급 RAG 파이프라인</strong>을 만들어 보고, 각 단계가 결과에 미치는 영향을 관찰하며 심층 이해를 얻을 수 있다. 나아가 엔터프라이즈 환경에서 <strong>자신만의 RAG 시스템</strong>을 구축할 때도 LangChain의 모듈들을 활용하여 <strong>프로토타입</strong>을 빠르게 만들어볼 수 있을 것이다.</p>
<section id="id6">
<h3>체크포인트 질문<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>LangChain의 <strong>GraphQAChain</strong>은 어떤 방식으로 자연어 질문을 그래프 질의로 변환하고 답을 생성하는가? 내부 동작을 단계별로 추론해보자.</p></li>
<li><p>위 예시 워크플로우에서, <strong>질문 분기</strong> (route) 로직은 어떤 기준으로 그래프 vs 벡터 경로를 선택할까? 예를 들어 어떤 질문이 들어오면 벡터 검색이 필요한지 결정하는 요소는 무엇일까?</p></li>
<li><p>자신이 가진 도메인 데이터에 HippoRAG/GraphRAG를 적용한다고 가정해보자. LangChain으로 이를 구현하려면 어떤 모듈(체인, retriever, etc)을 활용해야 할지 구상해보자. (예: 연구 논문 데이터에 GraphRAG 적용 시)</p></li>
</ul>
</section>
</section>
<section id="id7">
<h2>6. 고급 RAG 적용 사례와 마무리<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>마지막으로, 실제 <strong>대규모 시스템에서의 RAG 아키텍처 사례</strong>를 간략히 살펴보며 정리한다. 최근 산업계에서는 하루 <strong>수천만 토큰</strong>의 질의를 처리하면서도 응답 지연을 100ms 이내로 유지해야 하는 등, <strong>초대형 스케일의 RAG</strong> 시스템이 운영되고 있다. 이를 위해 앞서 다룬 기술들이 종합적으로 활용된다. 예를 들어:</p>
<ul class="simple">
<li><p><strong>대용량 벡터 DB + Hybrid Search</strong>: 수억 개 문서를 <strong>벡터 임베딩</strong>으로 저장하되, 첫 단계 후보 검색에 <strong>BM25</strong> 등 키워드 필터를 적용해 <strong>검색 공간을 축소</strong>하고, 임베딩 유사도 검색 + RRF로 정밀 순위를 매기는 구조를 사용한다. 이를 통해 <strong>검색 정확도</strong>와 <strong>지연 시간</strong>을 모두 잡는다.</p></li>
<li><p><strong>FlashAttention 기반 장문 컨텍스트 처리</strong>: 검색된 문서 조각을 LLM에 넣어 답변할 때, 한 번 프롬프트에 수만 토큰을 투입하기도 한다. 이런 경우 FlashAttention 같은 메모리 최적화 기법을 적용한 LLM 서버를 사용해, <strong>대용량 컨텍스트</strong>에서도 안정적으로 동작하도록 한다.</p></li>
<li><p><strong>지속 학습과 망각</strong>: 신규 문서가 매일 추가되는 상황에서 HippoRAG 방식을 응용하여, <strong>새 문서들의 triple을 주기적으로 추출</strong>해 지식 그래프 인덱스를 업데이트하고, 오래된 정보는 그래프에서 제거하거나 중요도가 낮아진 엣지는 가중치를 낮추는 <strong>메모리 관리 정책</strong>을 적용한다. 이를 통해 <strong>업데이트 효율</strong>과 <strong>검색 품질</strong>을 함께 유지한다.</p></li>
</ul>
<p>앞서 살펴본 <strong>HippoRAG</strong>, <strong>GraphRAG</strong>, <strong>Hybrid Search</strong> 등의 기법은 이러한 대규모 시스템의 핵심 요소로서, 각각 <strong>장기 메모리</strong>, <strong>연결 추론</strong>, <strong>검색 정밀도</strong>를 담당한다. 학습자들은 이들 아키텍처를 이론적으로 이해하는 것을 넘어서, 실습과 프로젝트를 통해 <strong>직접 구현하고 튜닝</strong>해 봄으로써 심도 있는 통찰을 얻을 수 있을 것이다.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>측면</p></th>
<th class="head"><p>HippoRAG</p></th>
<th class="head"><p>GraphRAG</p></th>
<th class="head"><p>하이브리드 검색</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>핵심 접근법</strong></p></td>
<td><p>해마 이론 기반 생물학적 기억 인덱싱</p></td>
<td><p>커뮤니티 탐지와 함께하는 지식 그래프 구축</p></td>
<td><p>희소(BM25) 및 밀집(벡터) 검색의 결합</p></td>
</tr>
<tr class="row-odd"><td><p><strong>데이터 구조</strong></p></td>
<td><p>시놉시스 링크가 있는 지식 그래프 (PHR 연결)</p></td>
<td><p>커뮤니티 클러스터가 있는 계층적 지식 그래프</p></td>
<td><p>벡터 임베딩 + 역인덱스</p></td>
</tr>
<tr class="row-even"><td><p><strong>인덱싱 방법</strong></p></td>
<td><p>LLM 기반 OpenIE → 그래프 구축 → 시놉시스 연결</p></td>
<td><p>엔티티/관계 추출 → 그래프 빌딩 → 커뮤니티 분석</p></td>
<td><p>문서 임베딩 + 키워드 인덱싱</p></td>
</tr>
<tr class="row-odd"><td><p><strong>검색 전략</strong></p></td>
<td><p>질의 엔티티 추출 → PPR 그래프 탐색 → 기억 복원</p></td>
<td><p>커뮤니티 요약 선택 → 맵-리듀스 답변 생성</p></td>
<td><p>병렬 BM25 + 벡터 검색 → 점수 융합</p></td>
</tr>
<tr class="row-even"><td><p><strong>메모리 관리</strong></p></td>
<td><p>패턴 분리 및 완성 (해마와 유사)</p></td>
<td><p>계층적 요약 및 클러스터링</p></td>
<td><p>주기적 업데이트가 있는 정적 인덱스</p></td>
</tr>
<tr class="row-odd"><td><p><strong>다중-홉 추론</strong></p></td>
<td><p>✅ 우수 (단일 단계 PPR 탐색)</p></td>
<td><p>✅ 양호 (커뮤니티 기반 추론)</p></td>
<td><p>❌ 제한적 (다중 질의 필요)</p></td>
</tr>
<tr class="row-even"><td><p><strong>정확 매칭 능력</strong></p></td>
<td><p>⚠️ 보통 (엔티티 추출에 의존)</p></td>
<td><p>⚠️ 보통 (그래프 구조에 의존)</p></td>
<td><p>✅ 우수 (BM25 구성요소)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>의미적 이해</strong></p></td>
<td><p>✅ 양호 (시놉시스 링크와 PPR을 통해)</p></td>
<td><p>✅ 우수 (LLM 기반 추출 및 요약)</p></td>
<td><p>✅ 양호 (벡터 구성요소)</p></td>
</tr>
<tr class="row-even"><td><p><strong>저장 효율성</strong></p></td>
<td><p>✅ 높음 (25% 절감 보고됨)</p></td>
<td><p>❌ 낮음 (광범위한 그래프 저장 필요)</p></td>
<td><p>⚠️ 보통 (이중 인덱싱 오버헤드)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>질의 속도</strong></p></td>
<td><p>✅ 빠름 (반복 방법 대비 6-13배 빠름)</p></td>
<td><p>⚠️ 보통 (그래프 크기에 의존)</p></td>
<td><p>⚠️ 보통 (병렬 검색 오버헤드)</p></td>
</tr>
<tr class="row-even"><td><p><strong>구축 복잡도</strong></p></td>
<td><p>⚠️ 보통 (그래프 구축 필요)</p></td>
<td><p>❌ 높음 (커뮤니티 탐지가 포함된 복잡한 파이프라인)</p></td>
<td><p>✅ 낮음 (직관적인 구현)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>실시간 업데이트</strong></p></td>
<td><p>⚠️ 보통 (그래프 업데이트 필요)</p></td>
<td><p>❌ 어려움 (비용이 많이 드는 재클러스터링)</p></td>
<td><p>✅ 쉬움 (독립적인 인덱스 업데이트)</p></td>
</tr>
<tr class="row-even"><td><p><strong>해석 가능성</strong></p></td>
<td><p>✅ 양호 (그래프 경로와 노드 활성화)</p></td>
<td><p>✅ 우수 (커뮤니티 요약과 그래프 증거)</p></td>
<td><p>⚠️ 제한적 (점수 결합만)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>최적 사용 사례</strong></p></td>
<td><p>다중 문서 추론, 장기 기억 시스템, 학술 연구</p></td>
<td><p>대규모 문서 분석, 기업 지식베이스, 주제 발견</p></td>
<td><p>기술 문서, 코드 검색, 혼합 콘텐츠 유형</p></td>
</tr>
<tr class="row-even"><td><p><strong>일반적 도메인</strong></p></td>
<td><p>과학 문헌, 법률 문서, 연구 논문</p></td>
<td><p>기업 보고서, 위키피디아 규모 콘텐츠, 뉴스 아카이브</p></td>
<td><p>Stack Overflow, 의료 기록, 소프트웨어 문서</p></td>
</tr>
<tr class="row-odd"><td><p><strong>LLM 통합</strong></p></td>
<td><p>답변 생성을 위한 직접적 컨텍스트 복원</p></td>
<td><p>커뮤니티 요약을 통한 계층적 프롬프팅</p></td>
<td><p>표준 RAG 파이프라인을 위한 향상된 검색</p></td>
</tr>
<tr class="row-even"><td><p><strong>확장성</strong></p></td>
<td><p>✅ 양호 (효율적인 그래프 연산)</p></td>
<td><p>⚠️ 보통 (커뮤니티 탐지 병목)</p></td>
<td><p>✅ 양호 (병렬 처리)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>정확도 개선</strong></p></td>
<td><p>다중-홉 QA에서 20% 높음</p></td>
<td><p>최대 99% 정밀도 보고됨</p></td>
<td><p>단일 방법 접근법 대비 상당한 개선</p></td>
</tr>
</tbody>
</table>
</div>
<p>이번 주차를 마치면 여러분은 <strong>최신 RAG 아키텍처의 구성요소와 동작 원리</strong>를 학습한 것이며, 이는 곧 다가올 <strong>LangChain 기반 RAG 실습</strong>의 밑바탕이 된다. 예를 들어 다음 단계에서는 실제 <strong>사내 위키 문서</strong>에 대해 GraphRAG를 구축하고, 검색 성능을 <strong>벤치마킹 및 개선</strong>하는 과제를 수행할 것이다. 그런 실습을 통해 본 강의에서 배운 개념들이 어떻게 현실 시스템에 적용되는지 체험해 보기 바란다.</p>
<section id="id8">
<h3>체크포인트 질문<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>대규모 엔터프라이즈 RAG 시스템에서 <strong>응답 지연</strong>과 <strong>검색 정확도</strong>를 동시에 만족시키기 위해 어떤 기법들이 조합되어 사용되는가? 구체적인 사례를 들어보자.</p></li>
<li><p>지속적으로 업데이트되는 지식베이스에 RAG를 적용할 때, HippoRAG의 아이디어를 어떻게 활용할 수 있을까? (예: 새로운 정보 통합, 오래된 정보 처리 등)</p></li>
<li><p>이번 강의에서 배운 고급 RAG 개념들을 활용하여, 자신만의 RAG 파이프라인을 설계해본다면 어떤 구조를 구상하겠는가? 주요 단계와 사용할 기법들을 나열해보자.</p></li>
</ul>
</section>
</section>
<section id="id9">
<h2>참고자료<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Chen, J., Lin, H., Han, X., &amp; Sun, L. (2024). HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. arXiv preprint arXiv:2405.14831.</p></li>
<li><p>Microsoft Research (2024). GraphRAG: Transforming Unstructured Text into Explainable, Queryable Intelligence using Knowledge Graph-Enhanced RAG.</p></li>
<li><p>Sharma, T. (2024). HippoRAG: Redefining AI Retrieval emulating the Hippocampus. Medium.</p></li>
<li><p>Sharma, T. (2024). Microsoft GraphRAG: Transforming Unstructured Text into Explainable, Queryable Intelligence using Knowledge Graph-Enhanced RAG. Medium.</p></li>
<li><p>Superlinked (2024). Optimizing RAG with Hybrid Search &amp; Reranking. VectorHub.</p></li>
<li><p>Neo4j (2024). Create a Neo4j GraphRAG Workflow Using LangChain and LangGraph. Neo4j Developer Blog.</p></li>
<li><p>Nixie Search (2024). Search Overview Documentation. GitHub.</p></li>
<li><p>S, Suruthi (2024). Exploring HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. Medium.</p></li>
<li><p>Shrsv (2024). About HippoRAG. DEV Community.</p></li>
<li><p>VectorHub (2024). Hybrid Search &amp; Rerank RAG Documentation. GitHub.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="ko"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week08/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 8: 핵심 복습 및 최신 동향</p>
      </div>
    </a>
    <a class="right-next"
       href="../week10/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 10: 정렬 기법의 발전</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag">1. RAG 진화의 필요성: 장기 기억과 다중 문맥 통합</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hipporag">2. HippoRAG: 생물학적 장기 기억 모방 아키텍처</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag-retrieval-augmented-generation">3. GraphRAG: 지식 그래프 기반 Retrieval-Augmented Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4. 하이브리드 검색: 키워드와 임베딩의 조합</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain-graphrag">5. LangChain을 활용한 GraphRAG 구현 실습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">6. 고급 RAG 적용 사례와 마무리</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">참고자료</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
