
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LLM From Scratch 워크숍 &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'workshops/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1주차 워크숍: LLM 개요 및 개발 환경 구축" href="week01.html" />
    <link rel="prev" title="Week 14: 2025년 NLP 현황" href="../week14/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          한국어 <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    딥러닝자연어처리 (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer 및 차세대 아키텍처</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x와 최신 딥러닝 프레임워크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: 현대적 PEFT 기법을 활용한 효율적 파인튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: 고급 프롬프트 기법과 최적화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM 평가 패러다임과 벤치마크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: 멀티모달 NLP의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: 초장문맥 처리와 효율적 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: 핵심 복습 및 최신 동향</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week09/index.html">Week 9: 고급 RAG 아키텍처</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week10/index.html">Week 10: 정렬 기법의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week11/index.html">Week 11: 프로덕션 에이전트 시스템</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week12/index.html">Week 12: AI 규제와 책임 있는 AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week13/index.html">Week 13: 온톨로지와 AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week14/index.html">Week 14: 2025년 NLP 현황</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LLM From Scratch 워크숍</a></li>
<li class="toctree-l1"><a class="reference internal" href="week01.html">1주차 워크숍: LLM 개요 및 개발 환경 구축</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">프로젝트 운영 가이드라인</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">강의계획서</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/ko/workshops/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fworkshops/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/workshops/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM From Scratch 워크숍</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">워크숍 개요</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm">LLM의 정의와 특징</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">워크숍 로드맵</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1주차: LLM 개요 및 환경 구축</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">환경 구축 실습</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">기본 모델 실행 예시</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">2주차: 데이터 수집 및 정제</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">한국어 데이터셋 수집</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">데이터 정제 및 전처리</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3주차: 토크나이저 설계 및 구축</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">한국어 토크나이저 훈련</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">토크나이저 성능 비교</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">4주차: 모델 아키텍처 탐구</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer 아키텍처 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba">Mamba 아키텍처 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-pre-training">5주차: LLM 사전학습 (Pre-training)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">사전학습 설정 및 구성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">분산 학습 설정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft">6주차: 미세조정 및 PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA 미세조정 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dora">DoRA 미세조정 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">7주차: 모델 평가와 프롬프트 활용</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">모델 성능 평가</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">프롬프트 엔지니어링</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">8주차: 추론 최적화와 배포</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">모델 양자화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradio">Gradio를 활용한 배포</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment">9주차: 모델 정렬(Alignment)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">DPO 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">10주차: 통합 및 마무리</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">전체 파이프라인 통합</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">최종 데모 구축</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">참고자료</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">주요 논문 및 연구 자료</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">기술 문서 및 구현체</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">온라인 리소스 및 블로그</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-from-scratch">
<h1>LLM From Scratch 워크숍<a class="headerlink" href="#llm-from-scratch" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>워크숍 개요<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>현대의 대규모 언어 모델(LLM)은 종종 그 내부 작동 원리가 감춰진 ‘블랙박스’처럼 다뤄진다. 그러나 진정한 전문성은 단순히 도구를 사용하는 것을 넘어, 그 근본 원리를 이해하는 데서 비롯된다. 본 워크숍은 이러한 철학에 기반하여, LLM을 ‘처음부터(from scratch)’ 구축하는 과정을 통해 표면적인 응용을 넘어선 심층적인 이해를 목표로 한다.</p>
<section id="llm">
<h3>LLM의 정의와 특징<a class="headerlink" href="#llm" title="Link to this heading">#</a></h3>
<p>본 워크숍에서 다루는 대규모 언어 모델은 트랜스포머(Transformer) 아키텍처의 등장 이후 새롭게 정의된 개념이다. 이는 단순히 크기만을 의미하는 것이 아니다. 현대 LLM은 세 가지 핵심적인 특징으로 이전의 자연어 처리(NLP) 모델과 구분된다:</p>
<ol class="arabic simple">
<li><p><strong>규모(Scale)</strong>: 수십억에서 수조에 이르는 방대한 매개변수(parameter)</p></li>
<li><p><strong>생성적 사전 훈련(Generative Pre-training)</strong>: 특정 작업에 대한 지도 학습 이전에 대규모 텍스트 코퍼스로부터 언어의 통계적 패턴을 학습</p></li>
<li><p><strong>창발적 능력(Emergent Abilities)</strong>: 별도의 미세조정(fine-tuning) 없이도 몇 가지 예시만으로 새로운 작업을 수행하는 소수샷 학습(few-shot learning) 능력</p></li>
</ol>
<p>비록 교육적인 목적의 소규모 모델일지라도, 직접 구축하는 경험은 LLM의 잠재력, 한계, 그리고 그 행동을 형성하는 설계상의 선택들에 대한 비할 데 없는 통찰력을 제공한다.</p>
</section>
</section>
<section id="id2">
<h2>워크숍 로드맵<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>주차</p></th>
<th class="head text-left"><p>주제</p></th>
<th class="head text-left"><p>실습 목표</p></th>
<th class="head text-left"><p>사용 도구</p></th>
<th class="head text-left"><p>결과물</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1주차</p></td>
<td class="text-left"><p>LLM 개요 및 환경 구축</p></td>
<td class="text-left"><p>LLM 수명주기 이해, NeMo/HF 실습 환경 설정</p></td>
<td class="text-left"><p><strong>NGC 컨테이너</strong>, HF Transformers</p></td>
<td class="text-left"><p>워크숍 환경 준비, 간단 모델 실행 확인</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2주차</p></td>
<td class="text-left"><p>데이터 수집 및 정제</p></td>
<td class="text-left"><p>한국어 말뭉치 수집·전처리, 품질 향상 기법 실습</p></td>
<td class="text-left"><p><strong>NeMo Curator</strong>, HF Datasets</p></td>
<td class="text-left"><p>정제된 학습 코퍼스 (한국어 텍스트)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>3주차</p></td>
<td class="text-left"><p>토크나이저 설계 및 구축</p></td>
<td class="text-left"><p>한국어 토크나이저 훈련, 토큰화 방식 비교 이해</p></td>
<td class="text-left"><p><strong>HF 토크나이저</strong>, SentencePiece</p></td>
<td class="text-left"><p>한국어 BPE 토크나이저 모델</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>4주차</p></td>
<td class="text-left"><p>모델 아키텍처 탐구</p></td>
<td class="text-left"><p>Transformer와 최신 대안(Mamba, RWKV 등) 이해</p></td>
<td class="text-left"><p>PyTorch (HF 또는 NeMo AutoModel)</p></td>
<td class="text-left"><p>소규모 모델 구현 및 특성 비교</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>5주차</p></td>
<td class="text-left"><p>LLM 사전학습 (Pre-training)</p></td>
<td class="text-left"><p>커스텀 GPT 모델 초기화 및 사전학습 진행</p></td>
<td class="text-left"><p><strong>NeMo Run</strong>, Megatron (AutoModel 통합)</p></td>
<td class="text-left"><p>한국어 기반 LLM 초기 모델</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>6주차</p></td>
<td class="text-left"><p>미세조정 및 PEFT</p></td>
<td class="text-left"><p>다운스트림 작업용 모델 미세조정, PEFT 기법 적용</p></td>
<td class="text-left"><p><strong>HF PEFT</strong> (LoRA, WaveFT, DoRA 등)</p></td>
<td class="text-left"><p>과제 특화 모델 (예: 감성분석기)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>7주차</p></td>
<td class="text-left"><p>모델 평가와 프롬프트 활용</p></td>
<td class="text-left"><p>KLUE 등 벤치마크로 성능 평가, 프롬프트 튜닝 실습</p></td>
<td class="text-left"><p><strong>HF 평가</strong>(Metrics), 생성 출력 분석</p></td>
<td class="text-left"><p>평가 보고서 및 응답 향상 팁</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>8주차</p></td>
<td class="text-left"><p>추론 최적화와 배포</p></td>
<td class="text-left"><p>추론 속도/메모리 최적화, 실서비스 배포 환경 구성</p></td>
<td class="text-left"><p><strong>TensorRT-LLM</strong>, Triton, HF Pipelines</p></td>
<td class="text-left"><p>경량화 모델 및 데모 서비스</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>9주차</p></td>
<td class="text-left"><p>모델 정렬(Alignment)</p></td>
<td class="text-left"><p>RLHF/DPO로 사용자 지침 준수 모델로 재훈련</p></td>
<td class="text-left"><p><strong>NeMo Aligner</strong>, RLHF(DPO 알고리즘)</p></td>
<td class="text-left"><p>지침 응답 개선된 LLM (인스트럭트 모델)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>10주차</p></td>
<td class="text-left"><p>통합 및 마무리</p></td>
<td class="text-left"><p>전체 파이프라인 통합, 모델 공유 및 향후 과제 논의</p></td>
<td class="text-left"><p><strong>NeMo &amp; HF 연동</strong>, Gradio 데모</p></td>
<td class="text-left"><p>최종 데모 및 향후 발전 방향 정리</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="id3">
<h2>1주차: LLM 개요 및 환경 구축<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>1주차에는 대형언어모델(LLM)의 전체 수명주기를 개괄하고 실습 환경을 준비한다. NVIDIA의 <strong>NGC 컨테이너</strong>를 활용해 NeMo 프레임워크와 HuggingFace 툴킷이 포함된 환경을 세팅한다. 간단한 HuggingFace <strong>Transformers</strong> 파이프라인으로 예시 모델을 불러와 작동을 확인하며, NeMo와 HF 도구들이 어떻게 함께 활용될 수 있는지 개념을 잡는다. 이를 통해 향후 실습에 필요한 GPU 가속 환경과 라이브러리 호환성을 확보하고, LLM 워크플로의 큰 그림을 이해한다.</p>
<section id="id4">
<h3>환경 구축 실습<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># NVIDIA NGC 컨테이너 실행</span>
docker<span class="w"> </span>run<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>-it<span class="w"> </span>--rm<span class="w"> </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>nvcr.io/nvidia/pytorch:23.10-py3

<span class="c1"># 필요한 라이브러리 설치</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>datasets<span class="w"> </span>accelerate
pip<span class="w"> </span>install<span class="w"> </span>nemo-toolkit<span class="o">[</span>all<span class="o">]</span>
</pre></div>
</div>
</section>
<section id="id5">
<h3>기본 모델 실행 예시<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># 간단한 텍스트 생성 파이프라인</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> 
                    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span> 
                    <span class="n">device</span><span class="o">=</span><span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 텍스트 생성 테스트</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;인공지능의 미래는&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="id6">
<h3>체크포인트 질문<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>LLM의 전체 수명주기에서 가장 중요한 단계는 무엇인가?</p></li>
<li><p>NeMo와 HuggingFace Transformers의 주요 차이점은 무엇인가?</p></li>
<li><p>GPU 환경에서 모델을 실행할 때 고려해야 할 주요 사항들은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="id7">
<h2>2주차: 데이터 수집 및 정제<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>2주차에서는 <strong>한국어 말뭉치 데이터의 수집과 정제</strong>를 다룬다. <strong>NeMo Curator</strong>를 사용하여 위키피디아, 뉴스 등 방대한 한국어 텍스트를 수집하고 중복 제거 및 필터링을 수행한다. 예를 들어 KLUE 말뭉치나 NSMC 감성 코퍼스 등의 공개 데이터셋을 <strong>HuggingFace Datasets</strong>로 불러와 품질 검토 후 훈련 코퍼스에 추가한다. Curator의 분산 처리로 노이즈가 많은 데이터를 걸러내고 균질한 학습 데이터를 구축한다. 결과적으로 LLM 사전학습에 적합한 <strong>정제된 한국어 텍스트 코퍼스</strong>를 확보하고 데이터 구성에 담긴 고려사항을 익힌다.</p>
<section id="id8">
<h3>한국어 데이터셋 수집<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># 공개 한국어 데이터셋 로드</span>
<span class="n">nsmc</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;nsmc&quot;</span><span class="p">)</span>
<span class="n">klue_nli</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;klue&quot;</span><span class="p">,</span> <span class="s2">&quot;nli&quot;</span><span class="p">)</span>

<span class="c1"># 위키피디아 한국어 데이터 수집 예시</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">wiki_ko</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;wikipedia&quot;</span><span class="p">,</span> <span class="s2">&quot;20220301.ko&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train[:10000]&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NSMC 데이터: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">nsmc</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">개&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KLUE NLI 데이터: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">klue_nli</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">개&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;위키피디아 데이터: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">wiki_ko</span><span class="p">)</span><span class="si">}</span><span class="s2">개&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id9">
<h3>데이터 정제 및 전처리<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span>

<span class="k">def</span><span class="w"> </span><span class="nf">clean_korean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;한국어 텍스트 정제 함수&quot;&quot;&quot;</span>
    <span class="c1"># HTML 태그 제거</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;&lt;[^&gt;]+&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="c1"># 특수 문자 정리</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s가-힣]&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="c1"># 연속된 공백 제거</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">filter_by_length</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;길이 기준 필터링&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">text</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span> 
            <span class="k">if</span> <span class="n">min_length</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">]</span>

<span class="c1"># 데이터 정제 적용</span>
<span class="n">cleaned_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">clean_korean_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">raw_texts</span><span class="p">]</span>
<span class="n">filtered_texts</span> <span class="o">=</span> <span class="n">filter_by_length</span><span class="p">(</span><span class="n">cleaned_texts</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id10">
<h3>체크포인트 질문<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>한국어 텍스트 데이터를 수집할 때 고려해야 할 주요 품질 지표는 무엇인가?</p></li>
<li><p>NeMo Curator의 분산 처리 방식이 기존 데이터 정제 방법과 어떻게 다른가?</p></li>
<li><p>LLM 사전학습에 적합한 데이터의 특징은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="id11">
<h2>3주차: 토크나이저 설계 및 구축<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<p>3주차에는 <strong>한국어 토크나이저</strong>를 직접 만들어 본다. 수집한 말뭉치로부터 <strong>SentencePiece BPE</strong>나 WordPiece 기반의 토크나이저를 학습시키고, 토큰화 결과가 한국어의 단어 단위와 문맥을 잘 보존하는지 분석한다. HuggingFace <strong>🤗Tokenizers</strong> 라이브러리를 활용하여 사용자 정의 토크나이저를 훈련하고, 기존 multilingual 모델의 토크나이저와 <strong>토큰 분할 비교</strong>를 수행한다. 예를 들어 “한국어 형태소” 문장이 토큰화되는 방식을 확인하며, 한국어에 최적화된 어휘집 크기와 토큰화 전략을 결정한다. 이번 실습을 통해 LLM 학습 전에 <strong>맞춤형 토크나이저 모델</strong>을 구축하고, 토크나이즈 단계의 중요성을 체감한다.</p>
<section id="id12">
<h3>한국어 토크나이저 훈련<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">pre_tokenizers</span><span class="p">,</span> <span class="n">trainers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">TemplateProcessing</span>

<span class="c1"># BPE 토크나이저 초기화</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">BPE</span><span class="p">())</span>

<span class="c1"># 한국어 전처리 설정</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Whitespace</span><span class="p">()</span>

<span class="c1"># 훈련 설정</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">BpeTrainer</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">],</span>
    <span class="n">min_frequency</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="c1"># 한국어 말뭉치로 훈련</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">korean_texts</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>

<span class="c1"># 후처리 설정</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>
    <span class="n">single</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt; $A &lt;/s&gt;&quot;</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id13">
<h3>토크나이저 성능 비교<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_tokenizers</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizers</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;여러 토크나이저의 성능을 비교한다&quot;&quot;&quot;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="ow">in</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;tokens&#39;</span><span class="p">:</span> <span class="n">tokens</span><span class="o">.</span><span class="n">tokens</span><span class="p">,</span>
            <span class="s1">&#39;count&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">tokens</span><span class="p">),</span>
            <span class="s1">&#39;ids&#39;</span><span class="p">:</span> <span class="n">tokens</span><span class="o">.</span><span class="n">ids</span>
        <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># 비교 예시</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;한국어 자연어 처리는 매우 흥미로운 분야입니다.&quot;</span>
<span class="n">tokenizers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;custom_korean&#39;</span><span class="p">:</span> <span class="n">custom_tokenizer</span><span class="p">,</span>
    <span class="s1">&#39;bert_multilingual&#39;</span><span class="p">:</span> <span class="n">bert_tokenizer</span><span class="p">,</span>
    <span class="s1">&#39;sentencepiece&#39;</span><span class="p">:</span> <span class="n">sp_tokenizer</span>
<span class="p">}</span>

<span class="n">comparison</span> <span class="o">=</span> <span class="n">compare_tokenizers</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizers</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id14">
<h3>체크포인트 질문<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>한국어 토크나이저 설계 시 고려해야 할 주요 요소들은 무엇인가?</p></li>
<li><p>BPE와 WordPiece 토크나이저의 한국어 처리에서의 차이점은 무엇인가?</p></li>
<li><p>토크나이저의 어휘집 크기가 모델 성능에 미치는 영향을 설명하라.</p></li>
</ul>
</section>
</section>
<section id="id15">
<h2>4주차: 모델 아키텍처 탐구<a class="headerlink" href="#id15" title="Link to this heading">#</a></h2>
<p>4주차에서는 <strong>LLM 모델 아키텍처</strong>의 다양성을 탐구한다. 우선 Transformer 구조의 핵심 (셀프어텐션, 피드포워드 등)을 복습하고, 최신 대안 아키텍처들을 검토한다. 예를 들어 <strong>Mamba</strong>는 SSM(State Space Model) 기반으로 긴 시퀀스에 대한 선형적 추론을 가능케 하여, 트랜스포머와 유사한 성능을 내면서도 추론 지연과 메모리 사용이 대폭 개선된 구조이다. 또한 <strong>RWKV</strong>는 100% RNN 기반의 혁신적 LLM 아키텍처로, KV 캐시 없이도 선형 시간복잡도로 동작하면서 트랜스포머 수준의 성능을 달성한다. 이와 함께 중국발 최신 LLM인 <strong>DeepSeek</strong>의 개념도 다룬다. DeepSeek은 Mixture-of-Experts(MoE) 구조로 입력마다 일부 전문가만 활성화해 효율성을 높이고, Multi-Head Latent Attention 등을 도입하여 낮은 자원으로도 높은 성능을 보이는 것이 특징이다. 실습으로는 PyTorch를 통해 소규모 Transformer와 간단한 RNN 모델을 구현해 같은 데이터에서 <strong>학습 속도와 메모리 사용률을 비교</strong>해본다. 이를 통해 다양한 구조상의 trade-off를 파악하고, 최신 연구 동향을 LLM 설계에 반영하는 법을 배운다.</p>
<section id="transformer">
<h3>Transformer 아키텍처 구현<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Linear transformations</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Scaled dot-product attention</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
            
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="c1"># Concatenate heads</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mamba">
<h3>Mamba 아키텍처 구현<a class="headerlink" href="#mamba" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">mamba_ssm</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mamba</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MambaBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_state</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">d_conv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mamba</span> <span class="o">=</span> <span class="n">Mamba</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">d_state</span><span class="o">=</span><span class="n">d_state</span><span class="p">,</span>
            <span class="n">d_conv</span><span class="o">=</span><span class="n">d_conv</span><span class="p">,</span>
            <span class="n">expand</span><span class="o">=</span><span class="n">expand</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Residual connection with layer norm</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mamba</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Mamba 모델 구성</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MambaModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">MambaBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id16">
<h3>체크포인트 질문<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Transformer와 Mamba 아키텍처의 시간 복잡도 차이점은 무엇인가?</p></li>
<li><p>RWKV가 RNN과 Transformer의 장점을 결합한 방식은 무엇인가?</p></li>
<li><p>MoE(Mixture-of-Experts) 구조가 효율성을 높이는 원리는 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="llm-pre-training">
<h2>5주차: LLM 사전학습 (Pre-training)<a class="headerlink" href="#llm-pre-training" title="Link to this heading">#</a></h2>
<p>5주차에는 본격적으로 <strong>한국어 LLM을 사전학습</strong>한다. 지난 주차에 준비된 토크나이저와 말뭉치를 사용하여, GPT 계열의 <strong>기초 언어모델</strong>을 처음부터 훈련시킨다. NVIDIA의 <strong>NeMo Run</strong> 툴과 Megatron 기반 레시피를 활용해 분산 학습을 수행하고, HuggingFace와의 통합을 위해 <strong>NeMo AutoModel</strong> 기능을 적용한다. AutoModel을 통해 HuggingFace 모델 아키텍처를 NeMo에서 바로 불러와 사용할 수 있으며, 모델 병렬화와 PyTorch JIT 최적화 등이 기본 지원된다. 예를 들어 hidden size나 레이어 수 등을 설정한 커스텀 GPT 모델을 random 초기화 후 다중 GPU 환경에서 학습시킨다. 몇 epoch의 훈련을 거치며 손실 감소 추이를 관찰하고, <strong>한국어 문장 생성 예시</strong>를 통해 초기 모델의 언어 생성 특성을 평가한다. 이번 주차를 통해 자체 말뭉치로 <strong>한국어 기반 LLM 초기 모델</strong>을 얻고, 대규모 사전학습 과정과 분산 훈련 기법을 실습하게 된다.</p>
<section id="id17">
<h3>사전학습 설정 및 구성<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># 한국어 토크나이저 로드</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;custom_korean_tokenizer&quot;</span><span class="p">)</span>

<span class="c1"># GPT 모델 초기화</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>  <span class="c1"># 기본 구조 사용</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
<span class="p">)</span>

<span class="c1"># 사전학습 설정</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./korean_llm_pretraining&quot;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">dataloader_num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">remove_unused_columns</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 훈련 데이터 준비</span>
<span class="k">def</span><span class="w"> </span><span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id18">
<h3>분산 학습 설정<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># NeMo를 활용한 분산 학습 설정</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nemo.collections.nlp.models.language_modeling</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTModel</span>

<span class="c1"># NeMo GPT 모델 구성</span>
<span class="n">nemo_model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>
    <span class="n">trainer</span><span class="o">=</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>  <span class="c1"># 4개 GPU 사용</span>
        <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
        <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">,</span>  <span class="c1"># 분산 데이터 병렬</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 한국어 데이터로 훈련</span>
<span class="n">nemo_model</span><span class="o">.</span><span class="n">setup_training_data</span><span class="p">(</span>
    <span class="n">train_file</span><span class="o">=</span><span class="s2">&quot;korean_corpus.txt&quot;</span><span class="p">,</span>
    <span class="n">validation_file</span><span class="o">=</span><span class="s2">&quot;korean_validation.txt&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id19">
<h3>체크포인트 질문<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>LLM 사전학습에서 가장 중요한 하이퍼파라미터들은 무엇인가?</p></li>
<li><p>분산 학습 시 고려해야 할 주요 요소들은 무엇인가?</p></li>
<li><p>한국어 사전학습에서 영어 모델과 다른 점은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="peft">
<h2>6주차: 미세조정 및 PEFT<a class="headerlink" href="#peft" title="Link to this heading">#</a></h2>
<p>6주차에서는 사전학습된 모델을 **다운스트림 과제에 맞게 미세조정(fine-tuning)**한다. 우선 간단한 <strong>지도학습 미세조정</strong>으로 NSMC 영화리뷰 감성분석 데이터에 모델을 특화시켜 본다. HuggingFace의 Trainer를 활용해 전체 파라미터를 업데이트하는 대신, <strong>LoRA</strong>와 같은 PEFT 기법으로 일부 가중치만 조정해 효율을 높인다. LoRA 적용은 HuggingFace <strong>PEFT</strong> 라이브러리를 통해 손쉽게 이루어지며, <strong>NeMo AutoModel</strong>을 사용하면 사전학습한 HF 모델에 바로 LoRA 어댑터를 붙여 훈련할 수도 있다. 이때 <strong>WaveFT</strong>와 <strong>DoRA</strong> 같은 최신 기법도 소개한다. WaveFT는 가중치 잔여행렬의 <strong>웨이블릿 영역</strong>에서 극소 파라미터만 학습하여 LoRA보다 미세한 제어와 고효율 튜닝을 달성하는 방법으로, 매우 적은 변수만으로도 성능을 유지할 수 있음을 실험으로 보여주었다. <strong>DoRA</strong>(Weight-Decomposed LoRA)는 가중치 변화량을 크기와 방향 성분으로 분해하여 학습함으로써, LoRA 대비 <strong>정확도가 원본 풀 파인튜닝에 한층 가까운 결과</strong>를 내는 NVIDIA의 최신 방식이다. 실습에서는 기존 LoRA와 DoRA로 같은 감성분석 태스크를 수행해보고 결과를 비교한다. 이를 통해 적은 자원으로 모델을 효과적으로 <strong>재훈련하는 기술들</strong>을 습득하고, 각 기법의 장단점을 이해하게 된다.</p>
<section id="lora">
<h3>LoRA 미세조정 구현<a class="headerlink" href="#lora" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="c1"># LoRA 설정</span>
<span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;dense&quot;</span><span class="p">],</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span>
<span class="p">)</span>

<span class="c1"># 모델에 LoRA 적용</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;korean_llm_base&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">)</span>

<span class="c1"># 훈련 가능한 파라미터 확인</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="dora">
<h3>DoRA 미세조정 구현<a class="headerlink" href="#dora" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">DoRAConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>

<span class="c1"># DoRA 설정</span>
<span class="n">dora_config</span> <span class="o">=</span> <span class="n">DoRAConfig</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;dense&quot;</span><span class="p">],</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span>
<span class="p">)</span>

<span class="c1"># DoRA 적용</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dora_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id20">
<h3>체크포인트 질문<a class="headerlink" href="#id20" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>PEFT 기법이 전체 파인튜닝보다 효율적인 이유는 무엇인가?</p></li>
<li><p>LoRA와 DoRA의 주요 차이점은 무엇인가?</p></li>
<li><p>미세조정 시 어떤 레이어를 대상으로 선택해야 하는가?</p></li>
</ul>
</section>
</section>
<section id="id21">
<h2>7주차: 모델 평가와 프롬프트 활용<a class="headerlink" href="#id21" title="Link to this heading">#</a></h2>
<p>7주차에는 모델의 <strong>성능 평가와 활용 방법</strong>에 초점을 맞춘다. 우선 미세조정된 모델을 대상으로 <strong>KLUE 벤치마크</strong>의 일부를 사용해 정량 평가를 진행한다. 예를 들어 자연어 추론(NLI)이나 질의응답(MRC) 데이터로 모델의 정확도를 측정하고, <strong>HuggingFace의 evaluate 라이브러리</strong>로 Accuracy, F1 등의 지표를 산출한다. 또한 <strong>생성형 평가</strong>를 위해 준비된 프롬프트에 모델이 응답한 결과를 수동 검토하거나, BLEU/ROUGE 같은 지표로 요약문 정확도를 평가해 본다. 이 과정에서 한국어 평가의 유의점을 다루고, 필요한 경우 GPT-4 등을 활용한 <strong>모델 출력 평점 평가</strong> 기법도 소개한다. 아울러 **프롬프트 최적화(prompt optimization)**에 관한 실습도 병행한다. 동일한 질문에 대해 프롬프트 문구를 조정해보며 모델 응답 내용의 변화를 관찰하고, 원하는 출력 형식을 이끌어내는 <strong>프롬프트 엔지니어링 팁</strong>을 공유한다. 예를 들어 모델에게 단계적 사고를 유도하는 프롬프트를 줘서 추론 과정을 상세히 답변하게 해 보는 식이다. 이번 주를 통해 <strong>모델의 객관적 성능</strong>을 측정하고, <strong>효과적인 프롬프트 설계</strong>로 모델을 활용하는 방법을 익힌다.</p>
<section id="id22">
<h3>모델 성능 평가<a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># KLUE 벤치마크 평가</span>
<span class="k">def</span><span class="w"> </span><span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;모델의 성능을 평가한다&quot;&quot;&quot;</span>
    
    <span class="c1"># 정확도 평가</span>
    <span class="n">accuracy_metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
    <span class="n">f1_metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">)</span>
    
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">references</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_dataset</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">references</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">])</span>
    
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;weighted&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">],</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="n">f1</span><span class="p">[</span><span class="s2">&quot;f1&quot;</span><span class="p">]}</span>
</pre></div>
</div>
</section>
<section id="id23">
<h3>프롬프트 엔지니어링<a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_prompt_variations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;다양한 프롬프트로 모델 응답을 테스트한다&quot;&quot;&quot;</span>
    
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">f</span><span class="s2">&quot;질문: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n</span><span class="s2">답변:&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;다음 질문에 대해 단계별로 생각해보세요.</span><span class="se">\n</span><span class="s2">질문: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n</span><span class="s2">답변:&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;당신은 도움이 되는 AI 어시스턴트입니다.</span><span class="se">\n</span><span class="s2">질문: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n</span><span class="s2">답변:&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    
    <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">responses</span>
</pre></div>
</div>
</section>
<section id="id24">
<h3>체크포인트 질문<a class="headerlink" href="#id24" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>한국어 모델 평가에서 고려해야 할 특별한 요소들은 무엇인가?</p></li>
<li><p>프롬프트 엔지니어링의 핵심 원칙은 무엇인가?</p></li>
<li><p>생성형 모델의 품질을 객관적으로 평가하는 방법은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="id25">
<h2>8주차: 추론 최적화와 배포<a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
<p>8주차에서는 완성된 모델을 <strong>실서비스에 배포</strong>하기 위한 <strong>추론 최적화 기법</strong>을 다룬다. 우선 모델 파라미터를 8-bit 혹은 4-bit로 양자화하여 메모리 사용을 줄이고 CPU/GPU 추론 속도를 높이는 방법을 실습한다. HuggingFace <strong>Transformers</strong>와 <strong>BitsAndBytes</strong> 등을 이용해 INT8/INT4 양자화된 체크포인트를 생성하고, 응답 품질 저하가 최소화되는지 확인한다. 이어서 NVIDIA의 <strong>TensorRT-LLM</strong> 툴킷을 활용한 고속 추론 엔진 구축을 다룬다. TensorRT-LLM은 파이썬 API를 통해 LLM을 정의하면 자동으로 최적화된 TensorRT 엔진을 빌드해주며, NVIDIA GPU에서 효율적으로 추론을 수행한다. 실습으로 사전학습한 모델을 TensorRT-LLM으로 변환한 뒤, <strong>Triton Inference Server</strong>나 <strong>Gradio</strong> 인터페이스를 통해 배포한다. 이때 최적화 전후의 <strong>레이턴시와 Throughput 변화</strong>를 측정하여 성능 향상을 체감한다. 결과적으로 8주차에서는 <strong>경량화된 LLM 서비스</strong>를 구축하는 법을 배우며, 대용량 모델을 실사용 환경에 올릴 때 고려해야 할 최적화 기법들을 숙지한다.</p>
<section id="id26">
<h3>모델 양자화<a class="headerlink" href="#id26" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BitsAndBytesConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># 4비트 양자화 설정</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 양자화된 모델 로드</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;korean_llm_finetuned&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gradio">
<h3>Gradio를 활용한 배포<a class="headerlink" href="#gradio" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">gradio</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gr</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_response</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;사용자 입력에 대한 응답을 생성한다&quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="c1"># Gradio 인터페이스 생성</span>
<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span>
    <span class="n">fn</span><span class="o">=</span><span class="n">generate_response</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;한국어 LLM 챗봇&quot;</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;워크숍에서 구축한 한국어 LLM과 대화해보세요.&quot;</span>
<span class="p">)</span>

<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id27">
<h3>체크포인트 질문<a class="headerlink" href="#id27" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>모델 양자화가 성능에 미치는 영향은 무엇인가?</p></li>
<li><p>실서비스 배포 시 고려해야 할 주요 요소들은 무엇인가?</p></li>
<li><p>추론 최적화 기법들의 장단점은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="alignment">
<h2>9주차: 모델 정렬(Alignment)<a class="headerlink" href="#alignment" title="Link to this heading">#</a></h2>
<p>9주차에서는 <strong>모델 정렬(Alignment)</strong> 단계에 집중하여, LLM을 사용자 지침이나 가치에 맞게 튜닝하는 최신 기법들을 실습한다. 우선 Human Feedback 강화를 통한 지침 준수 모델 생성 개념을 설명하고, 대표적 방법인 <strong>RLHF</strong>(Reinforcement Learning from Human Feedback)의 절차를 알아본다. 여기에는 인간 피드백이 반영된 <strong>보상모델</strong> 학습과, PPO 알고리즘으로 언어모델을 최적화하는 과정이 포함된다. 다만 RLHF는 구현이 복잡하고 비용이 크므로, 대안으로 제시된 <strong>DPO</strong>(Direct Preference Optimization)를 직접 적용해 본다. DPO는 별도의 강화학습 없이도 인간 선호 데이터를 이용해 <strong>모델을 직접 재학습</strong>시키는 기법으로, RLHF에 준하는 성능을 보이면서도 구현이 단순한 장점이 있다. 실습에서는 오픈된 <strong>선호도 데이터셋</strong>(예: 인스트럭션 응답에 대한 랭킹)을 활용하여 DPO 알고리즘으로 우리 모델을 <strong>지침 따라 대화하도록</strong> 재튜닝한다. NVIDIA의 <strong>NeMo-Aligner</strong> 툴킷을 사용하면 RLHF 파이프라인과 DPO 알고리즘을 손쉽게 수행할 수 있으며, 수백 억 규모 모델도 효율적으로 정렬시킬 수 있다. 훈련 완료 후, 모델에게 민감한 질문이나 복합 지시를 프롬프트로 입력하여, <strong>안전하고 도움되는 응답</strong>을 생성하는지 확인한다. 9주차를 통해 참가자들은 <strong>LLM Alignment의 중요성</strong>과 구현 방법을 이해하고, 최종적으로 사용자 친화적인 <strong>인스트럭트 모델</strong>을 얻게 된다.</p>
<section id="dpo">
<h3>DPO 구현<a class="headerlink" href="#dpo" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">DPOTrainer</span><span class="p">,</span> <span class="n">DPOConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span>

<span class="c1"># DPO 설정</span>
<span class="n">dpo_config</span> <span class="o">=</span> <span class="n">DPOConfig</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./dpo_results&quot;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 선호도 데이터 준비</span>
<span class="k">def</span><span class="w"> </span><span class="nf">prepare_dpo_data</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DPO 훈련을 위한 데이터를 준비한다&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">],</span>
        <span class="s2">&quot;chosen&quot;</span><span class="p">:</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;chosen_response&quot;</span><span class="p">],</span>
        <span class="s2">&quot;rejected&quot;</span><span class="p">:</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;rejected_response&quot;</span><span class="p">]</span>
    <span class="p">}</span>

<span class="c1"># DPO 훈련</span>
<span class="n">dpo_trainer</span> <span class="o">=</span> <span class="n">DPOTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">ref_model</span><span class="o">=</span><span class="n">ref_model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">dpo_config</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id28">
<h3>체크포인트 질문<a class="headerlink" href="#id28" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>모델 정렬(Alignment)이 왜 중요한가?</p></li>
<li><p>RLHF와 DPO의 주요 차이점은 무엇인가?</p></li>
<li><p>안전한 AI 모델을 구축하기 위한 고려사항은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="id29">
<h2>10주차: 통합 및 마무리<a class="headerlink" href="#id29" title="Link to this heading">#</a></h2>
<p>마지막 10주차에서는 그동안 다룬 내용을 <strong>통합</strong>하여 최종 결과물을 정리하고, 추가 발전 방향을 모색한다. 먼저 1주차부터 9주차까지의 과정을 하나의 파이프라인으로 연결하여 복습한다. 데이터 준비부터 토크나이징, 사전학습, 미세조정, 평가, 최적화, 정렬까지의 흐름을 정리하고, 각 단계에서 NeMo와 HuggingFace 도구들이 어떻게 협력했는지 되짚는다. 실습 결과 만들어진 <strong>최종 한국어 LLM</strong>을 HuggingFace Hub에 업로드하거나, 팀원들과 공유하여 실제 질의응답 데모를 실행해 본다. 또한 <strong>Gradio</strong> 등을 이용해 간단한 웹 인터페이스를 구성함으로써, 일반 사용자가 질문을 입력하고 모델이 응답하는 <strong>챗봇 데모</strong>를 완성한다. 이 과정에서 프롬프트 설계 최적화나 추가 미세조정을 통해 응답의 유용성과 안정성을 개선하는 마지막 튜닝을 시도할 수 있다. 마무리로, 최신 LLM 연구 동향인 멀티모달 통합, 지속적인 모델 모니터링과 피드백 루프 등의 주제를 짧게 토의하며 워크숍을 끝맺는다. 최종 주차를 통해 참가자들은 <strong>LLM 개발의 전체 사이클</strong>을 직접 경험한 것을 정리하고, 실무 응용 및 향후 학습에 대한 방향을 얻는다.</p>
<section id="id30">
<h3>전체 파이프라인 통합<a class="headerlink" href="#id30" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 전체 워크플로우 통합 예시</span>
<span class="k">def</span><span class="w"> </span><span class="nf">complete_llm_pipeline</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LLM 개발의 전체 파이프라인을 실행한다&quot;&quot;&quot;</span>
    
    <span class="c1"># 1. 데이터 준비</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">prepare_korean_corpus</span><span class="p">()</span>
    
    <span class="c1"># 2. 토크나이저 훈련</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">train_korean_tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    
    <span class="c1"># 3. 모델 사전학습</span>
    <span class="n">base_model</span> <span class="o">=</span> <span class="n">pretrain_llm</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    
    <span class="c1"># 4. 미세조정</span>
    <span class="n">finetuned_model</span> <span class="o">=</span> <span class="n">fine_tune_with_peft</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">task_data</span><span class="p">)</span>
    
    <span class="c1"># 5. 모델 정렬</span>
    <span class="n">aligned_model</span> <span class="o">=</span> <span class="n">align_model_with_dpo</span><span class="p">(</span><span class="n">finetuned_model</span><span class="p">,</span> <span class="n">preference_data</span><span class="p">)</span>
    
    <span class="c1"># 6. 최적화 및 배포</span>
    <span class="n">optimized_model</span> <span class="o">=</span> <span class="n">optimize_for_inference</span><span class="p">(</span><span class="n">aligned_model</span><span class="p">)</span>
    <span class="n">deploy_model</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">optimized_model</span>
</pre></div>
</div>
</section>
<section id="id31">
<h3>최종 데모 구축<a class="headerlink" href="#id31" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">gradio</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gr</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># 최종 모델 로드</span>
<span class="n">final_model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;korean_llm_final&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="s2">&quot;korean_tokenizer&quot;</span>
<span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">chat_with_model</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;최종 모델과 대화하는 함수&quot;&quot;&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">final_model</span><span class="p">(</span>
        <span class="n">message</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">]</span>

<span class="c1"># 최종 데모 인터페이스</span>
<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span>
    <span class="n">fn</span><span class="o">=</span><span class="n">chat_with_model</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;LLM From Scratch 워크숍 - 최종 데모&quot;</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;워크숍에서 처음부터 구축한 한국어 LLM과 대화해보세요!&quot;</span>
<span class="p">)</span>

<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id32">
<h3>체크포인트 질문<a class="headerlink" href="#id32" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>LLM 개발 과정에서 가장 중요한 단계는 무엇인가?</p></li>
<li><p>워크숍을 통해 얻은 가장 큰 인사이트는 무엇인가?</p></li>
<li><p>향후 LLM 연구에서 주목해야 할 분야는 무엇인가?</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id33">
<h2>참고자료<a class="headerlink" href="#id33" title="Link to this heading">#</a></h2>
<section id="id34">
<h3>주요 논문 및 연구 자료<a class="headerlink" href="#id34" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Vaswani, A., et al. (2017). “Attention is all you need.” Advances in neural information processing systems.</p></li>
<li><p>Gu, A., &amp; Dao, T. (2023). “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” arXiv preprint.</p></li>
<li><p>Peng, B., et al. (2023). “RWKV: Reinventing RNNs for the Transformer Era.” arXiv preprint.</p></li>
<li><p>Hu, E. J., et al. (2021). “LoRA: Low-Rank Adaptation of Large Language Models.” ICLR 2022.</p></li>
<li><p>Liu, H., et al. (2024). “DoRA: Weight-Decomposed Low-Rank Adaptation.” arXiv preprint.</p></li>
<li><p>Dettmers, T., et al. (2023). “QLoRA: Efficient Finetuning of Quantized LLMs.” arXiv preprint.</p></li>
<li><p>Rafailov, R., et al. (2023). “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” arXiv preprint.</p></li>
</ul>
</section>
<section id="id35">
<h3>기술 문서 및 구현체<a class="headerlink" href="#id35" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hugging Face Transformers Documentation: <a class="reference external" href="https://huggingface.co/docs/transformers">https://huggingface.co/docs/transformers</a></p></li>
<li><p>NVIDIA NeMo Documentation: <a class="reference external" href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/">https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/</a></p></li>
<li><p>Mamba GitHub Repository: <a class="github reference external" href="https://github.com/state-spaces/mamba">state-spaces/mamba</a></p></li>
<li><p>RWKV GitHub Repository: <a class="github reference external" href="https://github.com/BlinkDL/RWKV-LM">BlinkDL/RWKV-LM</a></p></li>
<li><p>PEFT Library Documentation: <a class="reference external" href="https://huggingface.co/docs/peft">https://huggingface.co/docs/peft</a></p></li>
<li><p>TensorRT-LLM Documentation: <a class="reference external" href="https://docs.nvidia.com/tensorrt-llm/">https://docs.nvidia.com/tensorrt-llm/</a></p></li>
</ul>
</section>
<section id="id36">
<h3>온라인 리소스 및 블로그<a class="headerlink" href="#id36" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“A Visual Guide to Mamba and State Space Models” - Newsletter by Maarten Grootendorst</p></li>
<li><p>“The RWKV language model: An RNN with the advantages of a transformer” - The Good Minima</p></li>
<li><p>“Mamba Explained” - The Gradient</p></li>
<li><p>“Introducing RWKV - An RNN with the advantages of a transformer” - Hugging Face Blog</p></li>
<li><p>“Parameter-Efficient Fine-Tuning: A Comprehensive Guide” - Hugging Face Blog</p></li>
<li><p>“DoRA: A High-Performing Alternative to LoRA” - NVIDIA Developer Blog</p></li>
<li><p>“QLoRA: Making Large Language Models More Accessible” - Hugging Face Blog</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./workshops"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="ko"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week14/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 14: 2025년 NLP 현황</p>
      </div>
    </a>
    <a class="right-next"
       href="week01.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">1주차 워크숍: LLM 개요 및 개발 환경 구축</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">워크숍 개요</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm">LLM의 정의와 특징</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">워크숍 로드맵</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1주차: LLM 개요 및 환경 구축</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">환경 구축 실습</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">기본 모델 실행 예시</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">2주차: 데이터 수집 및 정제</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">한국어 데이터셋 수집</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">데이터 정제 및 전처리</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3주차: 토크나이저 설계 및 구축</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">한국어 토크나이저 훈련</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">토크나이저 성능 비교</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">4주차: 모델 아키텍처 탐구</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer 아키텍처 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mamba">Mamba 아키텍처 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-pre-training">5주차: LLM 사전학습 (Pre-training)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">사전학습 설정 및 구성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">분산 학습 설정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peft">6주차: 미세조정 및 PEFT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA 미세조정 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dora">DoRA 미세조정 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">7주차: 모델 평가와 프롬프트 활용</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">모델 성능 평가</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">프롬프트 엔지니어링</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">8주차: 추론 최적화와 배포</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">모델 양자화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradio">Gradio를 활용한 배포</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment">9주차: 모델 정렬(Alignment)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">DPO 구현</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">10주차: 통합 및 마무리</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">전체 파이프라인 통합</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">최종 데모 구축</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">참고자료</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">주요 논문 및 연구 자료</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">기술 문서 및 구현체</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">온라인 리소스 및 블로그</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
