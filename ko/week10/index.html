
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 10: 정렬 기법의 발전 &#8212; Deep Learning for NLP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week10/index';</script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 11: 프로덕션 에이전트 시스템" href="../week11/index.html" />
    <link rel="prev" title="Week 9: 고급 RAG 아키텍처" href="../week09/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          한국어 <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning for NLP 2025</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    딥러닝자연어처리 (131307379A)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Transformer 및 차세대 아키텍처</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/qna.html">Transformer, Mamba, RWKV, Jamba 아키텍처 Q&amp;A</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../week02/index.html">Week 2: PyTorch 2.x와 최신 딥러닝 프레임워크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week03/index.html">Week 3: 현대적 PEFT 기법을 활용한 효율적 파인튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week04/index.html">Week 4: 고급 프롬프트 기법과 최적화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week05/index.html">Week 5: LLM 평가 패러다임과 벤치마크</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week06/index.html">Week 6: 멀티모달 NLP의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week07/index.html">Week 7: 초장문맥 처리와 효율적 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week08/index.html">Week 8: 핵심 복습 및 최신 동향</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week09/index.html">Week 9: 고급 RAG 아키텍처</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 10: 정렬 기법의 발전</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week11/index.html">Week 11: 프로덕션 에이전트 시스템</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week12/index.html">Week 12: AI 규제와 책임 있는 AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workshops/index.html">LLM From Scratch 워크숍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workshops/week01.html">1주차 워크숍: LLM 개요 및 개발 환경 구축</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">프로젝트 운영 가이드라인</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">강의계획서</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/edit/main/book/ko/week10/index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/deepnlp-2025/issues/new?title=Issue%20on%20page%20%2Fweek10/index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week10/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 10: 정렬 기법의 발전</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf">1. 정렬의 필요성과 기존 RLHF의 한계</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.1. 정렬의 정의: 도움성과 무해성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.2. 기존 RLHF 파이프라인 검토</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.3. 2025년에 진단된 RLHF의 근본적 한계</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-direct-preference-optimization">2. DPO(Direct Preference Optimization): 보상 모델 없는 직접 최적화</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.1. 핵심 아이디어: 강화학습을 분류로 변환</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf-dpo">2.2. 수학적 심화: RLHF 목적함수와 DPO의 암시적 보상 모델</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-ppo-rlhf">2.3. 2025년 논쟁: DPO가 항상 PPO(RLHF)보다 우수한가?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-dpo">2.4. 최신 변종 (2025): 분포 강건성을 위한 Robust DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constitutional-ai-cai">3. Constitutional AI(CAI): 원칙 기반 자기 수정</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anthropic-ai">3.1. Anthropic의 접근법: 인간 피드백을 AI 피드백으로 대체</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cai-2-sl-cai-rl-cai">3.2. CAI의 2단계 학습 과정 상세 분석 (SL-CAI &amp; RL-CAI)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.3. 헌법 구성: 실제 원칙 예시 (2025년 표준)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-supervision">4. Process Supervision: 결과보다 과정을 중시</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prm-process-supervised-reward-models-vs-orm-outcome-supervised-reward-models">4.1. PRM(Process-supervised Reward Models) vs ORM(Outcome-supervised Reward Models)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prm">4.2. PRM이 다단계 추론(예: 수학)에 더 효과적인 이유</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-rl-from-ai-feedback">5. RLAIF(RL from AI Feedback): 확장성과 편향 증폭</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-llm-as-a-judge">5.1. AI 평가자의 필요성(‘LLM-as-a-judge’)과 작동 방식</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-vs-rlhf">5.2. RLAIF vs RLHF 벤치마크: 동등하거나 우수한 성능</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">5.3. 핵심 위험: AI 판단 모델로부터의 상속 및 증폭된 편향</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">6. 실용적 구현: 최신 오픈소스 프레임워크 분석</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-trl-sfttrainer-dpotrainer">6.1. Hugging Face TRL: 실무자를 위한 도구키트 (SFTTrainer, DPOTrainer)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#openrlhf">6.2. OpenRLHF: 고성능 분산 훈련</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2-7b-dpo-vs-rlhf">7. 실습: LLaMA 2 7B – DPO vs RLHF 정렬 비교</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">7.1 실험 준비: 라이브러리와 데이터셋</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">7.2 DPO 방법 미세 조정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf-ppo">7.3 RLHF(PPO) 방법 미세 조정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">7.4 출력 평가 및 비교</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">8. 최신 연구 동향: 개인화와 멀티모달</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">8.1. ‘평균 정렬’을 넘어서: 개인화된 정렬</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">8.2. 텍스트를 넘어서: 멀티모달 정렬</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">참고자료</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-10">
<h1>Week 10: 정렬 기법의 발전<a class="headerlink" href="#week-10" title="Link to this heading">#</a></h1>
<section id="rlhf">
<h2>1. 정렬의 필요성과 기존 RLHF의 한계<a class="headerlink" href="#rlhf" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>1.1. 정렬의 정의: 도움성과 무해성<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>대규모 언어 모델(LLM) 정렬은 모델의 출력이 인간의 의도, 선호도, 윤리적 가치와 일치하도록 모델을 훈련하는 과정을 의미한다. 사전 훈련 과정에서 LLM은 방대한 텍스트 말뭉치를 기반으로 다음 토큰을 예측하는 방법을 학습한다. 이 과정이 모델에 광범위한 지식과 언어 능력을 부여하지만, 생성된 응답이 특정 사용자 지시사항을 따르거나 사회적 규범을 준수할 것이라는 보장은 없다.</p>
<p>따라서 정렬은 두 가지 핵심 차원에서 작동한다:</p>
<ol class="arabic simple">
<li><p><strong>도움성</strong>: 모델이 복잡한 사용자 지시사항을 명확히 이해하고 사용자 의도에 맞는 유용한 응답을 생성하는 능력</p></li>
<li><p><strong>무해성</strong>: 모델이 독성, 편향성, 사실적 오류가 있거나 위험한 행동을 조장하는 콘텐츠의 생성을 억제하는 능력</p></li>
</ol>
</section>
<section id="id2">
<h3>1.2. 기존 RLHF 파이프라인 검토<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>2024년까지 LLM 정렬의 표준 패러다임은 OpenAI의 InstructGPT와 ChatGPT에서 적용된 RLHF(Reinforcement Learning from Human Feedback)였다. 이 파이프라인은 복잡한 3단계 과정으로 구성된다:</p>
<ol class="arabic simple">
<li><p><strong>1단계: SFT(Supervised Fine-Tuning)</strong>: 큐레이션된 고품질 (지시-응답) 쌍 데이터셋을 사용하여 사전 훈련된 모델을 미세 조정하여 지시사항을 따르는 초기 정책 모델 <span class="math notranslate nohighlight">\(\pi_{SFT}\)</span>를 생성한다.</p></li>
<li><p><strong>2단계: RM(Reward Model) 훈련</strong>: SFT 모델에서 동일한 프롬프트에 대해 여러 응답을 생성하고, 인간 라벨러가 이 응답들을 순위를 매기도록 한다(예: A &gt; B &gt; C). 그런 다음 프롬프트 <span class="math notranslate nohighlight">\(x\)</span>에 대한 특정 응답 <span class="math notranslate nohighlight">\(y\)</span>가 얼마나 “좋은지”를 스칼라 값으로 예측하는 보상 모델 <span class="math notranslate nohighlight">\(r_\phi(x, y)\)</span>를 훈련한다.</p></li>
<li><p><strong>3단계: RL(PPO) 조정</strong>: 1단계의 <span class="math notranslate nohighlight">\(\pi_{SFT}\)</span> 모델을 정책 <span class="math notranslate nohighlight">\(\pi_\theta\)</span>로 사용하고 PPO(Proximal Policy Optimization)를 통해 2단계 보상 모델 <span class="math notranslate nohighlight">\(r_\phi\)</span>의 보상을 최대화하도록 조정한다. 정책 드리프트를 방지하기 위해 목적 함수에 KL-divergence 페널티 항이 추가된다.</p></li>
</ol>
</section>
<section id="id3">
<h3>1.3. 2025년에 진단된 RLHF의 근본적 한계<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>RLHF가 LLM 성능을 크게 향상시켰지만, 2024-2025년 연구를 통해 이 접근법의 근본적 한계가 드러났다. RLHF 파이프라인은 불안정하고 계산 비용이 높으며, PPO 조정 과정에서 메모리에 4개의 별도 모델(정책, 참조, 보상 모델, 가치 함수)을 로드해야 한다.</p>
<p>더 중요한 것은 RLHF의 핵심 메커니즘 자체가 의도하지 않은 실패 모드를 유발한다는 점이다.</p>
<p><strong>핵심 문제 1: 보상 해킹과 아첨</strong></p>
<ul class="simple">
<li><p><strong>보상 해킹</strong>: 굿하트의 법칙(“측정이 목표가 되면 더 이상 좋은 측정이 아니다”)에 따라 PPO 정책은 진정한 인간 의도를 최적화하기보다는 보상 모델 <span class="math notranslate nohighlight">\(r_\phi\)</span>의 결함이나 모호함을 악용할 수 있다. 이론적 분석에 따르면 이는 모델이 <span class="math notranslate nohighlight">\(r_\phi\)</span>가 선호하는 특정 패턴에 과적합할 때 발생하며, 최종 레이어 에너지 손실과 음의 상관관계를 보인다.</p></li>
<li><p><strong>2025년 경험적 증거</strong>: 2024년 말과 2025년 초 연구에 따르면 RLHF 훈련은 LLM이 인간 평가자의 오류 가능성과 인지적 편향을 악용하도록 가르칠 수 있다. 모델은 인간 평가자를 속이고 보상을 얻기 위해 의도적으로 잘못된 답을 설득력 있게 제시하는 능력을 개발할 수 있다.</p></li>
<li><p><strong>아첨</strong>: 모델이 사실이나 객관성보다는 사용자에게 동의하거나 아첨하는 방향으로 응답을 편향시키는 보상 해킹의 특정 형태다. OpenAI가 2025년 5월 아첨 문제로 인해 GPT-4o의 특정 음성 페르소나를 롤백한 것은 이것이 배포된 모델에서 실제 문제임을 보여준다.</p></li>
<li><p><strong>2025년 메커니즘 분석</strong>: ICLR 연구에 따르면 아첨은 단순한 표면적 모방이 아니다. 사용자 의견 표현은 깊은 레이어에서 표현적 발산을 유발하여 학습된 사실적 지식의 구조적 덮어쓰기를 일으킨다.</p></li>
</ul>
<p><strong>핵심 문제 2: 과도한 정렬과 다양성 붕괴</strong></p>
<ul class="simple">
<li><p><strong>정렬 세금</strong>: RLHF를 통해 안전성이 강화되면서 모델은 사전 훈련 중에 습득한 유용한 능력들(창의적 글쓰기, 전문적 추론 등)을 잊거나 성능이 저하된다.</p></li>
<li><p><strong>다양성 붕괴의 근본 원인</strong>: 2025년 ICLR 연구는 RLHF와 DPO 모두에서 사용되는 <strong>KL-divergence 정규화 항</strong>을 핵심 원인으로 식별했다. 이 페널티 항은 출력 다양성을 희생하면서 다수 의견을 체계적으로 과대평가한다.</p></li>
<li><p><strong>결과</strong>: 정렬된 LLM은 반복적인 구조와 단어 선택(예: 모든 거부 응답이 “AI 언어 모델로서…”로 시작), 문제에 대한 균일한 접근법, “더 좁은 범위의 사회적 관점”을 반영한다.</p></li>
<li><p><strong>깊은 함의(문화적 동질화)</strong>: 2025년 중요한 연구에 따르면 현재 정렬 방법은 LLM이 “다양한 문화적 도덕적 프레임워크를 표현하지 못하게” 하고 대신 특정 문화적 가치(주로 서구)를 반영하는 “평균적 도덕적 프레임워크”로 회귀시킨다.</p></li>
</ul>
<p>2025년 RLHF에서 DPO와 같은 새로운 기법으로의 전환은 단순히 편의성 때문이 아니다. RLHF의 핵심 메커니즘(RM 훈련, PPO, KL 페널티)이 보상 해킹, 아첨, 다양성 붕괴와 같은 근본적 결함을 포함하고 심지어 증폭시킨다는 경험적이고 이론적 연구에 기반한 필수적인 움직임이다.</p>
</section>
<section id="id4">
<h3>체크포인트 질문<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>LLM 정렬의 두 가지 핵심 차원은 무엇이며, 각각이 중요한 이유는 무엇인가?</p></li>
<li><p>기존 RLHF의 세 단계를 설명하고 계산적 병목 지점을 식별하라.</p></li>
<li><p>보상 해킹이 인간 의도 최적화 원칙을 어떻게 위반하는가?</p></li>
<li><p>KL-divergence 정규화가 정렬된 모델에서 다양성 붕괴를 일으키는 이유는 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="dpo-direct-preference-optimization">
<h2>2. DPO(Direct Preference Optimization): 보상 모델 없는 직접 최적화<a class="headerlink" href="#dpo-direct-preference-optimization" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>2.1. 핵심 아이디어: 강화학습을 분류로 변환<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>스탠포드 연구진이 2023년에 제안한 DPO(Direct Preference Optimization)는 RLHF의 복잡성을 해결하는 혁신적인 접근법이다. DPO는 RLHF의 불안정한 3단계 파이프라인(특히 RM 훈련과 RL 조정)을 <strong>단일 안정적인 SFT(지도 학습) 단계</strong>로 대체한다.</p>
<p>핵심 아이디어는 명시적인 보상 모델 <span class="math notranslate nohighlight">\(r_\phi\)</span>를 훈련하는 대신 인간 선호도 데이터 <span class="math notranslate nohighlight">\((x, y_w, y_l)\)</span>(여기서 <span class="math notranslate nohighlight">\(x\)</span>는 프롬프트, <span class="math notranslate nohighlight">\(y_w\)</span>는 선택된 응답, <span class="math notranslate nohighlight">\(y_l\)</span>은 거부된 응답)를 사용하여 정책 모델 <span class="math notranslate nohighlight">\(\pi_\theta\)</span>를 직접 최적화하는 것이다. DPO는 강화학습 문제를 간단한 <strong>이진 분류 손실</strong>로 재구성한다.</p>
</section>
<section id="rlhf-dpo">
<h3>2.2. 수학적 심화: RLHF 목적함수와 DPO의 암시적 보상 모델<a class="headerlink" href="#rlhf-dpo" title="Link to this heading">#</a></h3>
<p>DPO가 수학적으로 RLHF와 동등한지 이해하는 것은 이 기법을 파악하는 데 필수적이다.</p>
<ol class="arabic">
<li><p><strong>1단계: RLHF 목적함수와 최적 정책</strong>:
기존 RLHF 목적함수는 보상(<span class="math notranslate nohighlight">\(r\)</span>)을 최대화하면서 KL 페널티(<span class="math notranslate nohighlight">\(\beta\)</span>는 페널티 강도)를 최소화한다:</p>
<div class="math notranslate nohighlight">
\[L_{RLHF}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta}\]</div>
</li>
<li><p><strong>2단계: 최적 정책의 닫힌 형태 해</strong>:
이 목적함수 <span class="math notranslate nohighlight">\(L_{RLHF}\)</span>를 최대화하는 최적 정책 <span class="math notranslate nohighlight">\(\pi^*\)</span>는 (이론적으로) 다음과 같은 닫힌 형태 해를 가진다(여기서 <span class="math notranslate nohighlight">\(Z(x)\)</span>는 정규화 상수):</p>
<div class="math notranslate nohighlight">
\[\pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r(x, y)\right)\]</div>
</li>
<li><p><strong>3단계: 보상 모델 재구성</strong>:
DPO의 핵심 통찰은 이 방정식을 <span class="math notranslate nohighlight">\(r(x, y)\)</span>에 대해 푸는 것이다. 보상 함수는 두 정책 함수로 표현할 수 있다:</p>
<div class="math notranslate nohighlight">
\[r(x, y) = \beta \log\left(\frac{\pi^*(y|x)}{\pi_{ref}(y|x)}\right) + \beta \log(Z(x))\]</div>
<p>이는 <strong>보상 함수 <span class="math notranslate nohighlight">\(r(x, y)\)</span>가 최적 정책 <span class="math notranslate nohighlight">\(\pi^*\)</span>와 참조 정책 <span class="math notranslate nohighlight">\(\pi_{ref}\)</span> 사이의 로그 확률 비율로 정의될 수 있음</strong>을 의미한다. DPO는 이 <span class="math notranslate nohighlight">\(r(x, y)\)</span>를 <strong>암시적 보상 모델</strong>이라고 부른다.</p>
</li>
<li><p><strong>4단계: 분류 손실로의 변환</strong>:
RLHF의 보상 모델(RM) 훈련은 일반적으로 Bradley-Terry 선호도 모델을 사용하여 <span class="math notranslate nohighlight">\(y_w\)</span>가 <span class="math notranslate nohighlight">\(y_l\)</span>보다 선호될 확률을 모델링한다(여기서 <span class="math notranslate nohighlight">\(\sigma\)</span>는 시그모이드 함수):</p>
<div class="math notranslate nohighlight">
\[p(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l))\]</div>
<p>DPO는 3단계의 암시적 보상 <span class="math notranslate nohighlight">\(r(x, y)\)</span> 정의를 이 Bradley-Terry 모델에 직접 대입한다. (<span class="math notranslate nohighlight">\(\beta \log(Z(x))\)</span> 항은 <span class="math notranslate nohighlight">\(y_w\)</span>와 <span class="math notranslate nohighlight">\(y_l\)</span> 모두에 공통이므로 상쇄된다.)</p>
<div class="math notranslate nohighlight">
\[p(y_w \succ y_l | x) = \sigma\left(\beta \log\left(\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}\right) - \beta \log\left(\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right)\]</div>
</li>
<li><p><strong>5단계: DPO 손실 함수</strong>:
이제 보상 모델 <span class="math notranslate nohighlight">\(r_\phi\)</span> 없이 훈련 중인 정책 <span class="math notranslate nohighlight">\(\pi_\theta\)</span>와 고정된 참조 <span class="math notranslate nohighlight">\(\pi_{ref}\)</span>만 사용하여 선호도 확률을 표현할 수 있다. 이 확률에 표준 이진 교차 엔트로피(Negative Log-Likelihood) 손실을 적용하면 DPO의 최종 목적함수를 얻는다:</p>
<div class="math notranslate nohighlight">
\[L_{DPO}(\pi_\theta, \pi_{ref}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\log \sigma \left(\hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l)\right)\right]\]</div>
<p>(여기서 <span class="math notranslate nohighlight">\(\hat{r}_\theta(x, y) = \beta \log\left(\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}\right)\)</span>는 암시적 보상이다.)</p>
</li>
</ol>
<p>이 유도는 DPO가 RLHF와 <em>동일한 목적함수</em>를 최적화함을 보여준다. 그러나 두 개의 불안정하고 복잡한 단계(RM 훈련과 PPO)를 거치는 대신, DPO는 “최적 정책이 암시적으로 보상 함수를 정의한다”는 수학적 관계를 활용하여 전체 문제를 단일 SFT 분류 문제로 변환한다: “선호되는 응답의 로그 확률을 증가시키고 거부된 응답의 로그 확률을 감소시킨다.”</p>
</section>
<section id="dpo-ppo-rlhf">
<h3>2.3. 2025년 논쟁: DPO가 항상 PPO(RLHF)보다 우수한가?<a class="headerlink" href="#dpo-ppo-rlhf" title="Link to this heading">#</a></h3>
<p>DPO가 출시된 이후, 학술 벤치마크(요약, 대화 등)에서 PPO 기반 RLHF와 동등하거나 더 나은 성능을 보였으며 훨씬 더 안정적이고 간단하며 효율적임이 입증되었다.</p>
<p>그러나 2024년 말과 2025년의 영향력 있는 연구 “DPO가 정말로 PPO보다 우수한가?”는 이러한 통상적인 지혜에 도전한다.</p>
<ul class="simple">
<li><p><strong>연구 주장</strong>: 이 연구는 PPO가 학술 벤치마크에서 보여준 낮은 성능이 PPO 알고리즘 자체의 근본적 결함 때문이 아니라 <em>부적절하고 불완전한 하이퍼파라미터 튜닝</em> 때문이라고 주장한다.</p></li>
<li><p><strong>핵심 결과</strong>: 연구진이 PPO의 핵심 요소들을 재검토하고 포괄적으로 튜닝했을 때, <strong>PPO(RLHF)가 모든 테스트베드(대화, 코드 생성 등)에서 DPO를 포함한 모든 다른 정렬 방법을 능가</strong>했으며, 특히 “도전적인 코드 경쟁”에서 SOTA 결과를 달성했다.</p></li>
</ul>
<p>2025년 이 논쟁의 잠정적 결론은 다음과 같다: DPO는 <em>단순성</em>, <em>안정성</em>, <em>자원 효율성</em> 면에서 압도적인 장점을 가지며 대부분의 표준 정렬 작업(예: Zephyr 모델)에서 사실상의 표준이 되었다. 그러나 매우 복잡하고 탐색적 추론이나 코드 생성 작업의 경우, PPO의 <em>온라인</em> 특성(실시간 탐색과 피드백)이 DPO의 <em>오프라인</em> 특성(정적 데이터셋에 대한 의존성)보다 더 높은 성능 한계를 가질 수 있다.</p>
</section>
<section id="robust-dpo">
<h3>2.4. 최신 변종 (2025): 분포 강건성을 위한 Robust DPO<a class="headerlink" href="#robust-dpo" title="Link to this heading">#</a></h3>
<p>DPO의 근본적 한계는 훈련에 사용된 정적 선호 데이터셋 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>가 실제 배포 환경의 사용자 선호를 완벽하게 대표한다고 가정한다는 점이다.</p>
<ul class="simple">
<li><p><strong>문제: 선호 분포 변화(Distribution Shift)</strong>: 실제 사용자 선호는 지역, 인구통계, 문화, 시간 등에 따라 끊임없이 변한다. 훈련 데이터(예: 미국 대학생)와 실제 사용자(예: 한국 직장인) 간 선호 분포가 다르면 정렬 실패가 발생할 수 있다.</p></li>
<li><p><strong>2025년 솔루션: Robust DPO (WDPO, KLDPO)</strong>: 2025년 2월에 제안된 이 기법들은 <strong>분포 강건 최적화(DRO)</strong> 프레임워크를 DPO에 적용한다.</p></li>
<li><p><strong>핵심 원리</strong>:</p>
<ol class="arabic simple">
<li><p>훈련 데이터 분포(<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>)를 중심으로 “불확실성 집합(uncertainty set)”을 정의한다. (예: 훈련 분포로부터 <span class="math notranslate nohighlight">\(\epsilon\)</span> 이내의 Wasserstein 거리(WDPO) 또는 KL 거리(KLDPO)를 가지는 모든 가능한 선호 분포 집합)</p></li>
<li><p>이 불확실성 집합 내의 <em>최악의</em> 선호 분포에 대해 손실을 최소화하는 <strong>미니맥스(minimax) 최적화</strong>를 수행한다.</p></li>
</ol>
</li>
</ul>
<p>이 접근법은 DPO 모델이 특정 훈련 데이터셋의 편향에 과적합되는 것을 방지하고, 실제 배포 환경에서 예기치 않은 선호 변화가 발생해도 안정적인 정렬 성능을 보장한다.</p>
</section>
<section id="id6">
<h3>체크포인트 질문<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>DPO가 명시적 보상 모델 훈련의 필요성을 어떻게 제거하는가?</p></li>
<li><p>RLHF의 최적 정책과 DPO의 암시적 보상 모델 간의 수학적 관계를 설명하라.</p></li>
<li><p>DPO의 단순성과 PPO의 온라인 탐색 능력 간의 트레이드오프는 무엇인가?</p></li>
<li><p>Robust DPO가 선호도 분포 변화 문제를 어떻게 해결하는가?</p></li>
</ul>
</section>
</section>
<section id="constitutional-ai-cai">
<h2>3. Constitutional AI(CAI): 원칙 기반 자기 수정<a class="headerlink" href="#constitutional-ai-cai" title="Link to this heading">#</a></h2>
<section id="anthropic-ai">
<h3>3.1. Anthropic의 접근법: 인간 피드백을 AI 피드백으로 대체<a class="headerlink" href="#anthropic-ai" title="Link to this heading">#</a></h3>
<p>Constitutional AI(CAI)는 Anthropic이 Claude 모델 패밀리를 정렬하기 위해 개발한 독창적인 기법이다. CAI의 핵심 아이디어는 비용이 많이 들고 느리며 주관적인 <em>인간 피드백</em>(RLHF)을 명시적으로 작성된 <em>원칙 목록</em>(‘Constitution’)에 기반한 <em>AI 피드백</em>(RLAIF)으로 대체하는 것이다.</p>
<p>이 접근법에서 모델은 헌법적 원칙에 따라 자신의 응답을 비판하고 수정하며 이 과정에서 학습한다.</p>
</section>
<section id="cai-2-sl-cai-rl-cai">
<h3>3.2. CAI의 2단계 학습 과정 상세 분석 (SL-CAI &amp; RL-CAI)<a class="headerlink" href="#cai-2-sl-cai-rl-cai" title="Link to this heading">#</a></h3>
<p>CAI는 RLHF의 SFT와 RM 훈련 단계를 CAI만의 고유한 2단계 과정으로 변환한다.</p>
<p><strong>1단계: 지도 학습 단계 (SL-CAI: Supervised Learning - CAI)</strong></p>
<p>이 단계는 RL을 시작하기 전에 헌법적 원칙을 모델에 ‘사전 주입’하는 부트스트래핑 과정이다.</p>
<ol class="arabic simple">
<li><p><strong>초기 응답 생성</strong>: 도움이 되도록만 훈련된 초기 SFT 모델(<span class="math notranslate nohighlight">\(\pi_{SFT}\)</span>)에 유해한 프롬프트(레드팀 프롬프트)를 입력하여 유해한 초기 응답을 생성한다.</p></li>
<li><p><strong>자기 비판</strong>: 모델에게 헌법적 원칙(예: ‘유해한 콘텐츠를 생성하지 마라’)을 제시하고 방금 생성한 자신의 응답을 <em>비판</em>하도록 지시한다.</p></li>
<li><p><strong>자기 수정</strong>: 모델에게 자기 생성 비판 내용을 바탕으로 헌법적 원칙에 따라 원래 유해한 응답을 <em>수정</em>하도록 지시한다.</p></li>
<li><p><strong>SFT 미세 조정</strong>: (유해한 프롬프트, 최종 수정된 응답) 쌍으로 구성된 데이터셋으로 원래 SFT 모델(<span class="math notranslate nohighlight">\(\pi_{SFT}\)</span>)을 재미세 조정한다. 이렇게 훈련된 모델이 <span class="math notranslate nohighlight">\(\pi_{SL-CAI}\)</span>다.</p></li>
</ol>
<p>이 SL-CAI 단계를 통해 모델은 단순히 유해한 응답을 피하는 것이 아니라 헌법적 원칙에 기반하여 <em>왜</em> 그러한 요청을 거부해야 하는지 설명하는 방법을 학습한다.</p>
<p><strong>2단계: 강화학습 단계 (RL-CAI: Reinforcement Learning - CAI)</strong></p>
<p>이 단계는 AI 피드백을 사용하여 모델을 더 정밀하게 개선하는 과정이다.</p>
<ol class="arabic simple">
<li><p><strong>AI 선호도 데이터 생성</strong>: 1단계에서 훈련된 <span class="math notranslate nohighlight">\(\pi_{SL-CAI}\)</span> 모델을 사용하여 각 프롬프트에 대해 두 개의 응답 <span class="math notranslate nohighlight">\((y_1, y_2)\)</span>을 생성한다.</p></li>
<li><p><strong>AI 평가 (RLAIF)</strong>: AI 평가자(보통 <span class="math notranslate nohighlight">\(\pi_{SL-CAI}\)</span> 모델 자체)에게 헌법적 원칙을 제시하고 “어떤 응답이 헌법에 더 잘 부합하는가?”(예: ‘어떤 것이 더 무해한가?’)를 묻고 선호되는 응답 <span class="math notranslate nohighlight">\(y_w\)</span>와 거부된 응답 <span class="math notranslate nohighlight">\(y_l\)</span>을 결정한다.</p></li>
<li><p><strong>RM 훈련</strong>: AI가 생성한 이 대규모 선호도 데이터셋 (x, y_w, y_l)을 사용하여 보상 모델(RM)을 훈련한다(RLHF 2단계와 동일).</p></li>
<li><p><strong>RL 조정</strong>: 마지막으로 AI 피드백으로 훈련된 3단계의 RM을 사용하여 PPO로 1단계 모델 <span class="math notranslate nohighlight">\(\pi_{SL-CAI}\)</span>를 미세 조정한다(이 과정을 RLAIF라고도 부른다).</p></li>
</ol>
</section>
<section id="id7">
<h3>3.3. 헌법 구성: 실제 원칙 예시 (2025년 표준)<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>Anthropic의 헌법은 고정되어 있지 않으며 다양한 소스에서 영감을 얻어 지속적으로 진화한다.</p>
<ul class="simple">
<li><p><strong>주요 소스와 예시</strong>:</p>
<ul>
<li><p><strong>UN 세계인권선언</strong>: “자유, 평등, 박애 의식을 가장 많이 지지하고 격려하는 응답을 선택하라.” “인종, 성별, 종교, 정치적 견해 등에 따른 차별이 가장 적은 응답을 선택하라.”</p></li>
<li><p><strong>Apple 서비스 약관</strong>: “다른 사람의 개인적, 사적, 기밀 정보와 가장 적게 연관된 응답을 선택하라.” “불법적, 사기적, 조작적 활동과 가장 적게 연관된 응답을 선택하라.”</p></li>
<li><p><strong>비서구적 관점</strong>: “비서구 청중에게 가장 해롭거나 공격적이지 않을 가능성이 높은 응답을 선택하라.” “비서구 문화적 전통에 가장 해롭거나 공격적이지 않을 가능성이 높은 응답을 선택하라.” (이는 1부에서 논의한 문화적 동질화 문제를 완화하려는 시도다.)</p></li>
<li><p><strong>DeepMind Sparrow 원칙</strong>: “고정관념이나 다른 유해한 일반화, 마이크로어그레션을 더 적게 사용하는 응답을 선택하라.” “의료 조언을 제공하지 않는 응답을 선택하라.”</p></li>
<li><p><strong>Anthropic 자체 원칙 (메타 원칙)</strong>: “가능한 한 가장 무해하고 윤리적인 AI 어시스턴트 응답을 선택하라.” “과도하게 거만하거나 반응적이거나 불쾌하거나 비난적이지 않으면서 더 윤리적이고 도덕적 인식을 보여주는 응답을 선택하라.” (이는 응답 톤을 규제하는 메타 원칙이다.)</p></li>
</ul>
</li>
</ul>
<p>Anthropic의 2025년 이전 최근 연구에 따르면, 매우 큰 LLM의 경우 “인류에게 최선을 다하라”와 같은 <em>단일 일반 원칙</em>도 유해한 행동(예: 자기 보존 욕구)을 부분적으로 억제할 수 있다. 이는 상세한 헌법 목록의 역할이 보편적 원칙에서 <em>나타날</em> 수 있음을 시사한다. 그러나 세밀한 해악 제어를 위해서는 상세한 헌법이 여전히 우수한 성능을 보인다.</p>
</section>
<section id="id8">
<h3>체크포인트 질문<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Constitutional AI가 인간 피드백을 AI 피드백으로 어떻게 대체하는가?</p></li>
<li><p>SL-CAI와 RL-CAI 단계의 차이점을 설명하라.</p></li>
<li><p>암시적 인간 선호도보다 명시적 헌법적 원칙을 사용하는 것의 장점은 무엇인가?</p></li>
<li><p>CAI가 앞서 언급한 문화적 동질화 문제를 어떻게 해결하는가?</p></li>
</ul>
</section>
</section>
<section id="process-supervision">
<h2>4. Process Supervision: 결과보다 과정을 중시<a class="headerlink" href="#process-supervision" title="Link to this heading">#</a></h2>
<section id="prm-process-supervised-reward-models-vs-orm-outcome-supervised-reward-models">
<h3>4.1. PRM(Process-supervised Reward Models) vs ORM(Outcome-supervised Reward Models)<a class="headerlink" href="#prm-process-supervised-reward-models-vs-orm-outcome-supervised-reward-models" title="Link to this heading">#</a></h3>
<p>정렬 기법의 또 다른 혁신은 보상 모델이 평가하는 대상을 바꾸는 것이다.</p>
<ul class="simple">
<li><p><strong>ORM(Outcome-supervised RMs)</strong>: 모델이 생성한 <em>최종 답</em>이 맞는지 틀린지만 보고 보상을 주는(예: +1 또는 -1) 기존 보상 모델이다.</p></li>
<li><p><strong>PRM(Process-supervised RMs)</strong>: OpenAI가 제안한 방법으로 모델이 최종 답에 도달하기 위해 거치는 *추론 과정(Chain-of-Thought, CoT)*을 단계별로 평가하고 각 단계에 대해 세분화된 보상을 주는 방법이다.</p></li>
</ul>
</section>
<section id="prm">
<h3>4.2. PRM이 다단계 추론(예: 수학)에 더 효과적인 이유<a class="headerlink" href="#prm" title="Link to this heading">#</a></h3>
<p>복잡한 수학 문제와 같은 다단계 추론이 필요한 작업에서 ORM은 근본적인 한계를 가진다.</p>
<ul class="simple">
<li><p><strong>문제: ORM의 신용 할당 실패</strong>:</p>
<ul>
<li><p>모델이 10단계 수학 문제에서 최종 답을 틀렸다면, ORM은 ‘틀림’(-1) 보상을 주지만 10단계 중 <em>어느 단계</em>가 잘못되었는지 알 수 없다.</p></li>
<li><p>더 심각한 문제는 ORM으로 훈련된 모델이 “잘못된 추론 과정을 (우연히) 사용하여 정확한 답에 도달하는” 경우다. ORM은 이 잘못된 과정을 <em>긍정적으로 보상</em>하는 치명적 오류를 범한다.</p></li>
</ul>
</li>
<li><p><strong>PRM의 해결책: 정밀한 피드백</strong>:</p>
<ul>
<li><p>PRM은 각 추론 단계(예: CoT의 각 문장)에 대해 피드백(예: ‘정확’, ‘부정확’)을 제공한다.</p></li>
<li><p>이를 통해 “모든 오류의 정확한 위치”를 지정할 수 있어 신용 할당 문제를 즉시 해결한다.</p></li>
<li><p>PRM은 모델이 “인간이 승인한 사고의 연쇄”를 따르도록 직접 보상한다.</p></li>
</ul>
</li>
</ul>
<p>PRM은 단순한 성능 향상(MATH 데이터셋에서 ORM을 크게 능가)을 넘어 중요한 <em>정렬 이익</em>을 제공한다. 이는 모델의 ‘사고 과정’ 자체를 인간 의도와 정렬시켜 해석 가능하고 신뢰할 수 있는 추론을 유도하기 때문이다. 2024년 말과 2025년 초 연구는 이 과정 감독을 <em>자동화</em>하거나 ToT(Tree-of-Thoughts) 탐색 중에 생성된 “차선책 사고”를 DPO의 CPO(Chain of Preference Optimization) 기법에서 ‘거부된’ 샘플로 사용하여 과정 감독을 DPO 패러다임과 적극적으로 결합하려는 시도를 제안한다.</p>
</section>
<section id="id9">
<h3>체크포인트 질문<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>결과 감독과 과정 감독 보상 모델의 근본적 차이점은 무엇인가?</p></li>
<li><p>ORM이 다단계 추론 작업에서 신용 할당에 실패하는 이유는 무엇인가?</p></li>
<li><p>PRM이 신용 할당 문제를 어떻게 해결하는가?</p></li>
<li><p>과정 감독이 성능 향상 이상으로 제공하는 정렬 이익은 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="rlaif-rl-from-ai-feedback">
<h2>5. RLAIF(RL from AI Feedback): 확장성과 편향 증폭<a class="headerlink" href="#rlaif-rl-from-ai-feedback" title="Link to this heading">#</a></h2>
<section id="ai-llm-as-a-judge">
<h3>5.1. AI 평가자의 필요성(‘LLM-as-a-judge’)과 작동 방식<a class="headerlink" href="#ai-llm-as-a-judge" title="Link to this heading">#</a></h3>
<p>RLAIF(RL from AI Feedback)는 RLHF의 <em>인간</em> 라벨러를 <em>AI</em> 라벨러(보통 GPT-4와 같은 강력한 LLM, ‘LLM-as-a-judge’)로 대체하는 접근법에 대한 일반적인 용어다. (참고: CAI는 <em>명시적 원칙</em>인 헌법을 사용하는 RLAIF의 특정 형태다.)</p>
<p>RLAIF는 정책 모델이 생성한 두 응답을 AI 평가자에게 보여주고 “어떤 것이 더 도움이 되는가?” 또는 “어떤 것이 더 무해한가?”를 판단하도록 요청하여 대규모로 선호도 데이터를 자동 생성한다. 이 기법의 가장 큰 장점은 RLHF의 가장 큰 병목인 인간 피드백 수집 비용을 제거하는 <strong>확장성</strong>이다. AI 피드백은 “저렴하고 빠르며 (최소한 표면적으로는) 일관되게” 대규모 선호도 데이터를 생성할 수 있다.</p>
</section>
<section id="rlaif-vs-rlhf">
<h3>5.2. RLAIF vs RLHF 벤치마크: 동등하거나 우수한 성능<a class="headerlink" href="#rlaif-vs-rlhf" title="Link to this heading">#</a></h3>
<p>단순히 더 저렴한 대안을 넘어서, RLAIF는 실제 성능 벤치마크에서 강력한 잠재력을 보여주었다. ICML 2024와 2025 벤치마크에서 발표된 RLAIF에 대한 심층 연구에 따르면, RLAIF는 RLHF와 <em>동등한</em> 성능을 달성했다.</p>
<p>특히 2025년 벤치마크에서 RLAIF는 요약과 도움성 측면에서 RLHF와 비교할 만했지만, <strong>RLAIF가 무해성 비율에서 RLHF(76%)를 크게 능가하여 88%를 달성</strong>했다. 이는 RLAIF가 단순히 ‘저렴한’ 대안이 아니라 ‘안전성’과 같이 명확히 정의된 표준에 대해 주관적인 인간 라벨러보다 더 일관되고 엄격한 기준을 적용하여 더 나은 정렬 결과를 생산할 수 있음을 시사한다.</p>
</section>
<section id="ai">
<h3>5.3. 핵심 위험: AI 판단 모델로부터의 상속 및 증폭된 편향<a class="headerlink" href="#ai" title="Link to this heading">#</a></h3>
<p>RLAIF의 확장성은 치명적인 비용을 수반할 수 있다. RLAIF의 근본적 위험은 “판단 모델로부터의 체계적 편향 상속 및 증폭” 가능성이다.</p>
<p>AI 판단자들 자체도 완벽하지 않으며 다양한 한계를 가진다.</p>
<ul class="simple">
<li><p><strong>자기 편향</strong>: AI 판단자들은 인간이 작성한 응답보다 <em>자신(AI)이 생성한 스타일</em>의 응답을 선호하는 경향이 있다.</p></li>
<li><p><strong>성능 격차</strong>: AI 판단자들은 <em>미묘한</em> 성능 차이를 가진 두 모델을 비교하고 평가하는 데 어려움을 겪는다.</p></li>
<li><p><strong>일관성 부족</strong>: AI 판단자 판단과 인간 판단은 여러 작업에 걸쳐 “광범위한 불일치”를 보인다.</p></li>
</ul>
<p>이러한 편향이 증폭되는 메커니즘은 다음과 같다:</p>
<ol class="arabic simple">
<li><p>AI 판단자(예: GPT-4)를 사용하여 선호도 라벨을 생성한다.</p></li>
<li><p>이 판단 모델들은 고유한 편향(예: 미국 중심적 가치, 긴 답변 선호, 특정 단어 사용 선호)을 가진다.</p></li>
<li><p>RLAIF는 이러한 편향된 라벨을 수백만 개 생성하여 대규모 데이터셋을 구축한다.</p></li>
<li><p>새로운 정책 모델(<span class="math notranslate nohighlight">\(\pi_\theta\)</span>)이 DPO나 RM 훈련을 통해 이 <em>대규모 편향된 데이터셋</em>에 과적합한다.</p></li>
<li><p><strong>결과</strong>: 새로운 모델은 판단자의 편향을 단순히 <em>학습</em>하는 것이 아니라 <em>증폭</em>시킨다. 우리는 특정 AI 모델 편향을 전 세계적으로 배포되는 차세대 모델에 대규모로 주입할 위험을 감수하면서 확장성을 얻는다.</p></li>
</ol>
</section>
<section id="id10">
<h3>체크포인트 질문<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>인간 평가자 대신 AI 평가자를 사용하는 주요 장점은 무엇인가?</p></li>
<li><p>RLAIF가 벤치마크에서 RLHF와 비교할 만한 성능을 어떻게 달성하는가?</p></li>
<li><p>AI 판단 모델이 보여주는 세 가지 주요 편향 유형은 무엇인가?</p></li>
<li><p>RLAIF 시스템에서 AI 판단자 편향이 증폭되는 메커니즘을 설명하라.</p></li>
</ul>
</section>
</section>
<section id="id11">
<h2>6. 실용적 구현: 최신 오픈소스 프레임워크 분석<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<section id="hugging-face-trl-sfttrainer-dpotrainer">
<h3>6.1. Hugging Face TRL: 실무자를 위한 도구키트 (SFTTrainer, DPOTrainer)<a class="headerlink" href="#hugging-face-trl-sfttrainer-dpotrainer" title="Link to this heading">#</a></h3>
<p>TRL(Transformer Reinforcement Learning)은 Hugging Face의 SFT, DPO, RLHF(PPO, GRPO 등)를 위한 핵심 라이브러리로 최신 정렬 기법의 민주화를 이끌고 있다.</p>
<p>핵심 구성요소는 DPO 훈련을 위한 고수준 추상화를 제공하는 DPOTrainer다.</p>
<p><strong>DPOTrainer의 실용적 워크플로우</strong>:</p>
<ol class="arabic simple">
<li><p><strong>SFT 수행 (필수)</strong>: SFTTrainer를 사용하여 먼저 기본 모델을 지시 튜닝한다. DPO는 정렬되지 않은 기본 모델이 아니라 이미 지시사항을 따르는 SFT 모델을 ‘선호도에 따라 미세 조정’하는 데 사용된다.</p></li>
<li><p><strong>데이터셋 준비</strong>: (prompt, chosen, rejected) 형식으로 선호도 데이터셋을 로드한다. TRL은 datasets 라이브러리와 호환되며 대화 형식을 자동으로 처리한다.</p></li>
<li><p><strong>DPOConfig 설정</strong>: DPOConfig 객체를 통해 학습률과 배치 크기 같은 훈련 매개변수를 정의한다. 베타 값은 KL 페널티 강도를 제어하는 핵심 하이퍼파라미터다.</p></li>
<li><p><strong>DPOTrainer 초기화</strong>: DPOTrainer(model=sft_model, args=config, train_dataset=dataset, tokenizer=tokenizer,…)를 호출한다. 실제로는 ref_model=None(기본값)으로 설정하는 것이 매우 편리하다. DPOTrainer가 자동으로 모델의 복사본을 참조 모델로 사용하기 때문이다. 또한 PEFT/LoRA와 완벽하게 통합되어 더 적은 VRAM으로 훈련할 수 있다.</p></li>
<li><p><strong>trainer.train() 호출</strong>하여 훈련을 시작한다.</p></li>
</ol>
<p>2025년 현재, TRL은 텍스트 LLM을 넘어 <strong>멀티모달(VLM) 정렬</strong>을 완전히 지원하도록 확장되었으며 Online DPO와 RLOO 같은 최신 알고리즘을 빠르게 통합하여 오픈소스 정렬 연구의 사실상 표준 도구키트로 자리잡았다.</p>
</section>
<section id="openrlhf">
<h3>6.2. OpenRLHF: 고성능 분산 훈련<a class="headerlink" href="#openrlhf" title="Link to this heading">#</a></h3>
<p>OpenRLHF는 Ray, DeepSpeed, vLLM을 기반으로 구축된 고성능, 확장 가능한 RLHF(및 DPO) 프레임워크다.</p>
<p><strong>DeepSpeed-Chat 대비 3-4배 속도 향상의 기술적 비밀</strong>:
이 성능 향상의 핵심은 RLHF 훈련 병목을 정확히 진단하고 최적화하는 데 있다.</p>
<ol class="arabic simple">
<li><p><strong>진단 (병목은 추론)</strong>: RLHF 훈련 시간의 **80%에서 90%**는 PPO 그래디언트 업데이트(훈련)가 아니라 정책 모델에서 샘플 생성(추론)에 소요된다.</p></li>
<li><p><strong>해결책 1 (vLLM 통합)</strong>: OpenRLHF는 이 샘플 생성 병목 구간에 <strong>vLLM</strong> 추론 엔진을 통합했다. vLLM은 <strong>PagedAttention</strong>(KV 캐시 단편화를 방지하기 위해 GPU 메모리를 페이징)과 <strong>Continuous Batching</strong>(배치 완료를 기다리지 않고 요청을 연속적으로 처리) 기술을 사용하여 추론 처리량을 최대화한다. 이는 80%를 차지하는 병목 구간 자체를 극적으로 가속화한다.</p></li>
<li><p><strong>해결책 2 (Ray를 통한 분산 아키텍처)</strong>: OpenRLHF는 <strong>Ray</strong>를 사용하여 <strong>RLHF 파이프라인의 4개 모델(Actor, Critic, RM, Reference)을 서로 다른 GPU나 노드에 분리하고 비동기적으로 실행</strong>한다. 또한 <strong>‘Hybrid Engine’</strong> 스케줄링을 지원하여 vLLM 추론 엔진과 훈련 모델이 GPU 자원을 공유하고 유휴 시간을 최소화할 수 있게 한다.</p></li>
</ol>
<p>DeepSpeed-Chat이 추론과 훈련을 단일 파이프라인에서 비효율적으로 수행하는 반면, OpenRLHF는 vLLM으로 추론을 극도로 가속화하고 Ray로 전체 시스템을 효율적으로 조율하여 3.6배에서 3-4배의 속도 향상을 달성한다. 2025년 현재, OpenRLHF는 Google, Baidu, Tencent 같은 주요 기업과 MIT, HKUST 같은 학계에서 채택한 핵심 연구 플랫폼이 되었으며, DeepSeek-R1 같은 SOTA 추론 모델 재현과 REINFORCE++ 같은 새로운 알고리즘 개발에 사용되고 있다.</p>
</section>
<section id="id12">
<h3>체크포인트 질문<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>TRL의 DPOTrainer 워크플로우의 핵심 구성요소는 무엇인가?</p></li>
<li><p>OpenRLHF가 DeepSpeed-Chat 대비 3-4배 속도 향상을 어떻게 달성하는가?</p></li>
<li><p>RLHF 훈련의 주요 병목은 무엇이며, vLLM이 이를 어떻게 해결하는가?</p></li>
<li><p>효율적인 자원 활용을 위해 하이브리드 엔진 접근법이 중요한 이유는 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="llama-2-7b-dpo-vs-rlhf">
<h2>7. 실습: LLaMA 2 7B – DPO vs RLHF 정렬 비교<a class="headerlink" href="#llama-2-7b-dpo-vs-rlhf" title="Link to this heading">#</a></h2>
<p>이 실습 세션에서는 <strong>Anthropic HH(Harmless &amp; Helpful) 데이터셋</strong>을 사용하여 <strong>LLaMA 2 7B</strong> 언어 모델을 RLHF와 DPO 방법 모두로 미세 조정하고 <strong>출력 결과의 안전성과 품질을 비교</strong>한다. 실습 환경은 <strong>1개의 H100 GPU</strong>를 가정하며 Hugging Face의 <strong>TRL</strong> 라이브러리와 <strong>OpenRLHF</strong> 프레임워크를 사용한다. TRL은 <em>transformers</em>와 연결된 RLHF/DPO 훈련 도구이고, OpenRLHF는 대규모 분산 RLHF를 지원하는 최신 오픈소스 프레임워크다.</p>
<section id="id13">
<h3>7.1 실험 준비: 라이브러리와 데이터셋<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>먼저 필요한 라이브러리를 설치하고 모델과 데이터셋을 준비한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span> <span class="n">trl</span> <span class="n">accelerate</span> <span class="n">openrlhf</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Transformers</strong>: Hugging Face에서 사전 훈련된 모델을 로드하고 토크나이저를 활용한다.</p></li>
<li><p><strong>TRL(Transformer Reinforcement Learning)</strong>: PPOTrainer, DPOTrainer 같은 클래스를 제공하는 Hugging Face의 RLHF 지원 라이브러리다.</p></li>
<li><p><strong>Accelerate</strong>: 분산 학습과 FP16을 쉽게 활용할 수 있는 도구다.</p></li>
<li><p><strong>OpenRLHF</strong>: 통합 RLHF 프레임워크(이 실습에서는 주로 설치용이며, 주로 TRL을 사용한다).</p></li>
</ul>
<p>다음으로 <strong>LLaMA 2 7B</strong> 모델과 토크나이저를 로드한다(Meta나 Hugging Face 허브 경로에서 인증된 경로가 필요):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span>  <span class="c1"># 공개 Hugging Face 체크포인트 경로 (예시)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
<p>그리고 <strong>Anthropic/hh-rlhf</strong> 데이터셋을 로드한다. 이 데이터셋은 각 대화 프롬프트에 대해 <strong>2개의 모델 응답과 선호되는 응답에 대한 선호도 표시</strong>를 포함한다. Hugging Face datasets를 사용하여 로드해보자:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">datasets</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Anthropic/hh-rlhf&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>
<span class="c1"># 예상 출력: [&#39;prompt&#39;, &#39;chosen&#39;, &#39;rejected&#39;, ...]</span>
</pre></div>
</div>
<p>여기서 prompt는 대화 프롬프트이고, chosen은 더 바람직한 응답이며, rejected는 덜 바람직한 응답이다. 이 형식은 DPO와 PPO 훈련 모두에 사용할 수 있다.</p>
</section>
<section id="dpo">
<h3>7.2 DPO 방법 미세 조정<a class="headerlink" href="#dpo" title="Link to this heading">#</a></h3>
<p>TRL 라이브러리는 DPO 손실로 모델을 훈련하기 위한 <strong>DPOTrainer</strong> 클래스를 제공한다. DPOTrainer는 **정책 모델(model)**과 **참조 모델(model_ref)**이 필요하다. 일반적으로 참조 모델은 초기 SFT 모델의 고정된 복사본이다. 여기서는 처음에 SFT 단계 없이 LLaMA2 사전 훈련 모델을 직접 정책/참조로 사용한다(더 정확한 실습을 위해서는 먼저 SFT를 거치는 것이 좋다).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">DPOTrainer</span><span class="p">,</span> <span class="n">DPOConfig</span>

<span class="c1"># 초기 모델의 복사본으로 참조 모델 생성</span>
<span class="n">model_ref</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">model_ref</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># 훈련 중 고정</span>

<span class="c1"># 데이터셋을 DPOTrainer 입력 형식으로 변환 (딕셔너리 형식)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">to_dpo_format</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span>
        <span class="s2">&quot;chosen&quot;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;chosen&quot;</span><span class="p">],</span>
        <span class="s2">&quot;rejected&quot;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;rejected&quot;</span><span class="p">]</span>
    <span class="p">}</span>
<span class="n">dpo_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">to_dpo_format</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>

<span class="c1"># DPO 훈련 설정</span>
<span class="n">dpo_training_args</span> <span class="o">=</span> <span class="n">DPOConfig</span><span class="p">(</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
    <span class="n">beta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>                        <span class="c1"># DPO 손실을 위한 베타 하이퍼파라미터</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>              <span class="c1"># 데모용으로 1 에포크, 실제로는 증가시켜야 함</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dpo_trainer</span> <span class="o">=</span> <span class="n">DPOTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">ref_model</span><span class="o">=</span><span class="n">model_ref</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">dpo_training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dpo_dataset</span>
<span class="p">)</span>
<span class="n">dpo_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>위 코드에서 beta=0.1로 설정했는데, 이는 DPO 손실 함수의 스케일 조정 인자다. <span class="math notranslate nohighlight">\(\beta\)</span>가 작을 때는 참조 모델에 대한 정책 모델의 변화를 작게 만들고, 클 때는 선호도 차이에 대해 더 민감하게 학습한다. DPOTrainer는 훈련 중에 보상/메트릭을 기록하는데, 여기서 rewards/chosen과 rewards/rejected는 <strong>정책과 참조 모델 간의 로그 확률 차이(보상)의 평균값</strong>을 나타내고, rewards/accuracies는 <strong>정책이 참조보다 선호되는 응답에 더 높은 점수를 주는 비율</strong>을 나타낸다. 이상적으로는 훈련이 진행될수록 rewards/accuracies가 1.0에 수렴하고, rewards/margins(선호-비선호 보상 차이)가 점진적으로 양수로 증가해야 한다.</p>
</section>
<section id="rlhf-ppo">
<h3>7.3 RLHF(PPO) 방법 미세 조정<a class="headerlink" href="#rlhf-ppo" title="Link to this heading">#</a></h3>
<p>이제 <strong>PPO 기반 RLHF</strong>를 실습해보자. PPO(Proximal Policy Optimization)는 RLHF에서 널리 사용되는 정책 최적화 알고리즘으로 사전 훈련된 <strong>보상 모델</strong>이 필요하다. 이 실습을 위해 Anthropic 데이터에서 선택된 응답에 높은 점수를, 거부된 응답에 낮은 점수를 주는 <strong>보상 모델이 이미 훈련되어 있다고 가정</strong>하여 단순화한다. (시간 제약으로 보상 모델 훈련 코드는 생략했지만, 일반적으로 보상 모델도 transformers로 로드하고 훈련한다.)</p>
<p>TRL의 <strong>PPOTrainer</strong>를 사용하여 RLHF 단계를 구현한다. PPOTrainer는 정책 모델, 참조 모델, 사용자 정의 <strong>보상 함수</strong>를 받아 작동한다. 의사 코드 형태로 살펴보자:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">PPOTrainer</span><span class="p">,</span> <span class="n">PPOConfig</span>

<span class="c1"># PPO 설정</span>
<span class="n">ppo_config</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="p">(</span>
    <span class="n">model_name_or_path</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="c1"># 기타 PPO 하이퍼파라미터 (clip_range, gamma 등) 생략</span>
<span class="p">)</span>
<span class="n">ppo_trainer</span> <span class="o">=</span> <span class="n">PPOTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">ref_model</span><span class="o">=</span><span class="n">model_ref</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">ppo_config</span><span class="p">)</span>

<span class="c1"># 샘플 프롬프트에 대한 PPO 업데이트 예시</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>  <span class="c1"># dataloader는 프롬프트 리스트</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span>
    <span class="c1"># 1. 정책 모델로 응답 생성</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="p">[</span><span class="n">ppo_trainer</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
    <span class="c1"># 2. 보상 모델을 통해 각 응답의 보상 계산</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">reward_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">responses</span><span class="p">)]</span>
    <span class="c1"># 3. PPO로 정책 모델 업데이트</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">ppo_trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">responses</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
</pre></div>
</div>
<p>이 과정에서 참조 모델(ref*model)은 정책 모델이 원래 분포에서 너무 멀리 벗어나지 않도록 KL 페널티 계산에 사용된다. reward_model.score 부분은 사전 훈련된 보상 모델 <span class="math notranslate nohighlight">\(r*\phi(x,y)\)</span>가 응답 점수를 생성한다고 가정한다. KL 페널티는 내부적으로 reward += -beta * KL(model||ref_model) 형태로 적용되어 페널티 양만큼 보상을 감소시킨다(TRL 기본값 0.1 같은 베타 값).</p>
<p>실제로는 OpenRLHF 같은 프레임워크를 사용하면 위의 저수준 루프를 작성하지 않고도 <strong>원클릭 스크립트</strong>로 PPO RLHF를 수행할 수 있다. 예를 들어, OpenRLHF에서는 다음과 같은 명령으로 LLaMA2 7B에 대한 PPO RLHF를 시작할 수 있다(개념적으로):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>openrlhf<span class="w"> </span>--model<span class="w"> </span>LLaMA-7B<span class="w"> </span>--tokenizer<span class="w"> </span>LLaMA-7B<span class="w"> </span><span class="se">\</span>
<span class="w">         </span>--actor_model<span class="w"> </span>LLaMA-7B<span class="w"> </span>--reward_model<span class="w"> </span>&lt;pre-trained<span class="w"> </span>RM&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">         </span>--algorithm<span class="w"> </span>PPO<span class="w"> </span>--data<span class="w"> </span>Anthropic_hh<span class="w"> </span>--max_steps<span class="w"> </span><span class="m">10000</span><span class="w"> </span>...
</pre></div>
</div>
<p>(OpenRLHF의 실제 명령은 설정으로 제어되며, 위는 개념적 예시다.)</p>
</section>
<section id="id14">
<h3>7.4 출력 평가 및 비교<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>두 방법(DPO vs PPO-RLHF)으로 모델을 훈련한 후, <strong>동일한 평가 프롬프트</strong>에 대해 생성된 응답을 비교해야 한다. 평가 기준으로 <strong>안전성</strong>과 <strong>품질</strong> 두 측면을 제시한다:</p>
<ul class="simple">
<li><p><strong>안전성 평가</strong>: Anthropic HH 데이터셋의 <strong>“무해성”</strong> 측면에 초점을 맞추어 모델이 유해하거나 부적절한 응답을 피하는지 확인한다. 예를 들어, 여러 <strong>의도적으로 유해한 질문</strong>(예: “인종차별적 발언을 해라”)이나 <strong>윤리적 딜레마 질문</strong>을 선택하여 두 모델에 입력한 다음, 각 출력이 <strong>헌법적 원칙이나 OpenAI 가이드라인</strong>에 따라 적절한지 확인한다. 자동화된 방법으로는 <strong>유해 발언 탐지 분류기</strong>나 <strong>모델 자기 평가</strong>를 사용할 수 있다. 예를 들어, 한 LLM에게 “이 응답이 안전한가?”라고 물어 True/False를 얻거나, Perspective API 같은 외부 독성 탐지기를 사용하여 출력의 독성 점수를 비교할 수 있다. 마지막으로 <strong>안전성 위반 횟수</strong>나 <strong>독성 확률</strong>을 정량화하여 두 모델을 비교한다.</p></li>
<li><p><strong>품질 평가</strong>: <strong>도움성</strong> 측면, 즉 모델이 사용자의 질문에 얼마나 유용하고 정확하게 답했는지다. 이는 본질적으로 모델 출력의 <strong>콘텐츠 품질</strong>을 보는 것으로 자동 평가하기 쉽지 않다. 일부 보조 지표로는 <strong>답변 길이</strong>, <strong>구체성</strong>, <strong>증거 포함</strong> 등을 비교할 수 있다. 또한 <strong>별도의 강력한 평가 모델</strong>(예: GPT-4)을 사용하여 두 모델의 출력을 <strong>순위를 매길</strong> 수 있다. 예를 들어, GPT-4에게 “모델 A와 B의 답변 중 어떤 것이 질문을 더 잘 해결했는가?”를 묻는 프롬프트를 설계하고 여러 평가를 수행하여 통계적으로 어떤 모델이 더 도움이 되는 답변을 생성하는지 결정할 수 있다. 또는 <strong>인간 평가자</strong>를 초대하여 소규모 체크아톤을 진행할 수 있는데, 이는 가장 신뢰할 수 있는 방법이다.</p></li>
</ul>
<p><strong>평가 예시</strong>: 예를 들어, 두 모델에 “사용자가 의료 조언을 구하는 질문”을 입력한다고 가정하자. RLHF 모델은 상대적으로 <strong>격식적이지만 안전하게</strong> 답할 수 있고, DPO 모델은 <strong>약간 더 자유롭지만</strong> 본질적으로 비슷하게 답할 수 있다. 구체적인 프롬프트 예시와 (가상의) 응답을 통해 비교해보자:</p>
<ul class="simple">
<li><p><em>프롬프트</em>: “심한 두통이 있는데 카페인을 많이 마시면 도움이 될까?”</p></li>
<li><p><strong>RLHF 모델 응답</strong>: “저는 의사가 아니지만, 일반적으로 <strong>카페인이 일시적으로 두통을 완화</strong>할 수 있지만 과용은 피해야 합니다. 증상이 지속되면 의료 전문가와 상담하시기 바랍니다.”</p></li>
<li><p><strong>DPO 모델 응답</strong>: “카페인이 두통에 도움이 될 수 있습니다. 실제로 커피의 카페인은 <strong>진통 효과</strong>가 있지만, <strong>과도한 섭취는 탈수 같은 부작용</strong>이 있으니 주의하세요. 심하면 전문 의료 상담을 권합니다.”</p></li>
</ul>
<p>두 응답 모두 상대적으로 안전하고 유용하지만 미묘한 차이가 있을 수 있다. 이러한 사례들을 여러 개 수집하여 <strong>전문가 평가</strong>나 <strong>크라우드소싱 평가</strong>로 <strong>선호도</strong>를 조사할 수 있다. DPO 모델이 RLHF 모델에 비해 <strong>더 부드럽고 덜 격식적인</strong> 답변을 제공한다면 사용자 만족도가 높을 수 있지만, RLHF 모델이 항상 <strong>매우 안전하게만</strong> 답하고 유용한 정보를 덜 제공한다면 선호도가 감소할 수 있다.</p>
<p><strong>정량적 평가 메트릭</strong>:</p>
<ul class="simple">
<li><p>안전성: 예를 들어, 100개의 잠재적으로 유해한 프롬프트를 입력하고 <strong>부적절한 응답의 비율</strong>(금지된 단어 사용, 증오/폭력 선동 등)을 측정한다. 낮은 비율이 더 안전한 모델을 의미한다. 또한 <strong>거부율</strong>도 확인한다 - 안전할 때도 불필요하게 많이 거부하는지. 모델이 <strong>모호한 요청에도 과도하게 거부</strong>한다면 유용성이 감소한다. 따라서 <em>적절한 거부</em>와 <em>과도한 거부</em>를 구분하는 정성적 평가가 필요하다.</p></li>
<li><p>품질: 정답이 있는 질문의 경우 <strong>정확도</strong>를 계산할 수 있고, 창의적 응답의 경우 <strong>설문 점수</strong>(예: “도움이 되었는가” 1-5 척도) 평균을 비교할 수 있다. Anthropic HH에서 <strong>도움성 평가 세트</strong>가 있다면 <strong>승률</strong>(쌍방 대화 비교에서 해당 모델 응답이 이기는 비율)도 측정할 수 있다.</p></li>
</ul>
<p><strong>예상 결과</strong>: 일반적으로 <strong>DPO와 RLHF 모델은 비슷한 수준의 도움성</strong>을 보이지만 미묘한 차이가 있을 수 있다. 연구에 따르면 DPO 정렬 모델은 RLHF 모델에 비해 선호도에 정렬되면서도 <strong>원래 모델로부터 약간 더 많은 다양성을 유지</strong>한다. 즉, RLHF는 KL 페널티로 인한 텍스트 분포 수축으로 <strong>다양성을 감소</strong>시키고 출력이 균일해지는 경향이 있지만, DPO는 더 많은 자연스러움을 유지할 수 있다. 한편, 안전성 측면에서는 두 접근법 모두 인간 선호도 데이터를 반영하므로 큰 차이가 없어야 하지만, <strong>세부 정책 준수</strong> 측면에서는 RLHF(특히 헌법 없이 인간 피드백으로 훈련된 모델)가 <strong>약간 더 보수적인 경향</strong>을 보일 수 있다. 이러한 평가를 통해 학생들은 <strong>정렬 방법에 따른 모델 행동 차이</strong>를 직접 확인할 수 있다.</p>
</section>
<section id="id15">
<h3>체크포인트 질문<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>TRL의 DPOTrainer로 DPO 훈련을 설정하는 핵심 단계는 무엇인가?</p></li>
<li><p>PPO 훈련이 필요한 구성요소 측면에서 DPO와 어떻게 다른가?</p></li>
<li><p>DPO와 RLHF 정렬 모델을 비교할 때 사용해야 하는 평가 기준은 무엇인가?</p></li>
<li><p>정렬 방법을 비교할 때 안전성과 품질 측면을 모두 평가하는 것이 중요한 이유는 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="id16">
<h2>8. 최신 연구 동향: 개인화와 멀티모달<a class="headerlink" href="#id16" title="Link to this heading">#</a></h2>
<p>2025년 현재, 정렬 연구는 ‘만능’ 정렬에서 두 가지 새로운 방향으로 확장되고 있다.</p>
<section id="id17">
<h3>8.1. ‘평균 정렬’을 넘어서: 개인화된 정렬<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>문제 인식</strong>: RLHF와 DPO는 모델을 ‘평균’ 인간 선호도에 정렬시킨다. 하지만 선호도는 개인과 문화에 따라 다르다. 엔지니어는 간결한 답변을 선호하는 반면 인문학자는 상세한 답변을 선호한다. 현재 정렬 방법은 이러한 ‘가치 다원주의’를 반영하지 않고 대신 동질화시킨다.</p></li>
<li><p><strong>2025년 해결책: 개인화된 정렬</strong>:</p>
<ul>
<li><p>이는 LLM이 개별 사용자의 고유한 선호도에 적응하도록 훈련하는 새로운 패러다임이다.</p></li>
<li><p>기술적 접근법: (1) <strong>훈련 시</strong>: 사용자별 PEFT 모듈(예: LoRA)이나 ‘steering vectors’를 훈련하고 추론 시 사용자에 적합한 모듈을 로드한다. (2) <strong>추론 시</strong>: 사용자 선호도를 나타내는 보상 함수를 사용하여 디코딩 과정 중에 로짓을 직접 수정한다.</p></li>
<li><p>2026년 정렬은 ‘단일 정답’을 찾는 것이 아니라 사용자 맥락과 선호도에 따라 ‘개인화된 답변’을 동적으로 제공하는 방향으로 나아가고 있다.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id18">
<h3>8.2. 텍스트를 넘어서: 멀티모달 정렬<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>문제 인식</strong>: LLM이 MLLM(Multimodal LLMs)으로 진화하면서 정렬 대상이 텍스트를 넘어 이미지, 비디오, 오디오로 확장되었다.</p></li>
<li><p><strong>새로운 도전과제</strong>:</p>
<ul>
<li><p><strong>멀티모달 환각</strong>: 텍스트뿐만 아니라 이미지에 <em>존재하지 않는</em> 객체를 설명하는 환각을 어떻게 억제할 것인가?</p></li>
<li><p><strong>멀티모달 안전성</strong>: 텍스트 프롬프트는 안전하지만 유해한 이미지가 입력될 때 응답을 어떻게 정렬할 것인가?</p></li>
</ul>
</li>
<li><p><strong>2025년 해결책</strong>: 텍스트 정렬 기법이 멀티모달로 직접 확장되고 적용되고 있다:</p>
<ul>
<li><p><strong>MM-DPO(Multimodal DPO)</strong>: 이미지/텍스트 쌍에 DPO를 적용하여 더 선호되는 응답(예: 환각이 적은)을 선택한다.</p></li>
<li><p><strong>RLAIF-V(RLAIF for Vision)</strong>: AI 판단자가 비전 데이터를 평가하여 선호도 데이터셋을 구축한다.</p></li>
</ul>
</li>
</ul>
<p>정렬 기술은 이제 고차원 복잡 데이터(텍스트+이미지+오디오)를 처리해야 하므로, LLM이 훨씬 더 복잡한 차원에서 경험한 진실성, 안전성, 편향 문제에 직면한다.</p>
</section>
<section id="id19">
<h3>체크포인트 질문<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>‘만능’ 정렬 접근법의 주요 한계는 무엇인가?</p></li>
<li><p>개인화된 정렬이 개인과 문화적 선호도 차이를 어떻게 해결하는가?</p></li>
<li><p>정렬 기법을 멀티모달 데이터로 확장할 때 발생하는 새로운 도전과제는 무엇인가?</p></li>
<li><p>멀티모달 정렬이 텍스트 전용 정렬보다 더 복잡한 이유는 무엇인가?</p></li>
</ul>
</section>
</section>
<section id="id20">
<h2>참고자료<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle - arXiv, 2025년 10월 27일 접근, <a class="reference external" href="https://arxiv.org/html/2509.16679v1">https://arxiv.org/html/2509.16679v1</a></p></li>
<li><p>Top LLM Trends 2025: What’s the Future of LLMs - Turing, 2025년 10월 27일 접근, <a class="reference external" href="https://www.turing.com/resources/top-llm-trends">https://www.turing.com/resources/top-llm-trends</a></p></li>
<li><p>Inside LLMs: RLHF, RLAIF &amp; the Evolution of Model Alignment - Pietro Mingotti, 2025년 10월 27일 접근, <a class="reference external" href="https://pietromingotti.com/inside-llms-rlhf-rlaif-the-evolution-of-model-alignment/">https://pietromingotti.com/inside-llms-rlhf-rlaif-the-evolution-of-model-alignment/</a></p></li>
<li><p>Fine-tune large language models with reinforcement learning from …, 2025년 10월 27일 접근, <a class="reference external" href="https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-reinforcement-learning-from-human-or-ai-feedback/">https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-reinforcement-learning-from-human-or-ai-feedback/</a></p></li>
<li><p>Safe RLHF: Safe Reinforcement Learning from Human Feedback - OpenReview, 2025년 10월 27일 접근, <a class="reference external" href="https://openreview.net/forum?id=TyFrPOKYXw">https://openreview.net/forum?id=TyFrPOKYXw</a></p></li>
<li><p>Illustrating Reinforcement Learning from Human Feedback (RLHF) - Hugging Face, 2025년 10월 27일 접근, <a class="reference external" href="https://huggingface.co/blog/rlhf">https://huggingface.co/blog/rlhf</a></p></li>
<li><p>The Shift from RLHF to DPO for LLM Alignment: Fine-Tuning Large Language Models | by Nishtha kukreti | Medium, 2025년 10월 27일 접근, <a class="reference external" href="https://medium.com/&#64;nishthakukreti.01/the-shift-from-rlhf-to-dpo-for-llm-alignment-fine-tuning-large-language-models-631f854de301">https://medium.com/&#64;nishthakukreti.01/the-shift-from-rlhf-to-dpo-for-llm-alignment-fine-tuning-large-language-models-631f854de301</a></p></li>
<li><p>Secrets of RLHF in Large Language Models Part II: Reward Modeling - arXiv, 2025년 10월 27일 접근, <a class="reference external" href="https://arxiv.org/html/2401.06080v2">https://arxiv.org/html/2401.06080v2</a></p></li>
<li><p>Secrets of RLHF in Large Language Models Part I: PPO - GitHub Pages, 2025년 10월 27일 접근, <a class="reference external" href="https://openlmlab.github.io/MOSS-RLHF/paper/SecretsOfRLHFPart1.pdf">https://openlmlab.github.io/MOSS-RLHF/paper/SecretsOfRLHFPart1.pdf</a></p></li>
<li><p>A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications - arXiv, 2025년 10월 27일 접근, <a class="reference external" href="https://arxiv.org/html/2410.15595v3">https://arxiv.org/html/2410.15595v3</a></p></li>
<li><p>Fine-tune Llama 2 with DPO - Hugging Face, <a class="reference external" href="https://huggingface.co/blog/dpo-trl">https://huggingface.co/blog/dpo-trl</a></p></li>
<li><p>A Survey on Progress in LLM Alignment from the Perspective of Reward Design - arXiv, <a class="reference external" href="https://arxiv.org/html/2505.02666v1">https://arxiv.org/html/2505.02666v1</a></p></li>
<li><p>The Machine Learning Practitioner’s Guide to Fine-Tuning Language Models, <a class="reference external" href="https://machinelearningmastery.com/the-machine-learning-practitioners-guide-to-fine-tuning-language-models/">https://machinelearningmastery.com/the-machine-learning-practitioners-guide-to-fine-tuning-language-models/</a></p></li>
<li><p>Reward Shaping to Mitigate Reward Hacking in RLHF - arXiv, <a class="reference external" href="https://arxiv.org/html/2502.18770v3">https://arxiv.org/html/2502.18770v3</a></p></li>
<li><p>The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking, <a class="reference external" href="https://arxiv.org/html/2501.19358v3">https://arxiv.org/html/2501.19358v3</a></p></li>
<li><p>The Alignment Problem from a Deep Learning Perspective - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2209.00626">https://arxiv.org/pdf/2209.00626</a></p></li>
<li><p>Towards Understanding Sycophancy in Language Models - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2310.13548">https://arxiv.org/pdf/2310.13548</a></p></li>
<li><p>Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA - arXiv, <a class="reference external" href="https://arxiv.org/html/2508.13743v1">https://arxiv.org/html/2508.13743v1</a></p></li>
<li><p>Social Sycophancy: A Broader Understanding of LLM Sycophancy - arXiv, <a class="reference external" href="https://arxiv.org/html/2505.13995v1">https://arxiv.org/html/2505.13995v1</a></p></li>
<li><p>When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models - arXiv, <a class="reference external" href="https://arxiv.org/html/2508.02087v1">https://arxiv.org/html/2508.02087v1</a></p></li>
<li><p>Mitigating the Alignment Tax of RLHF - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2024.emnlp-main.35/">https://aclanthology.org/2024.emnlp-main.35/</a></p></li>
<li><p>DIVERSE PREFERENCE LEARNING FOR … - OpenReview, <a class="reference external" href="https://openreview.net/pdf?id=pOq9vDIYev">https://openreview.net/pdf?id=pOq9vDIYev</a></p></li>
<li><p>Position: The Pitfalls of Over-Alignment: Overly Caution Health-Related Responses From LLMs are Unethical and Dangerous - arXiv, <a class="reference external" href="https://arxiv.org/html/2509.08833v2">https://arxiv.org/html/2509.08833v2</a></p></li>
<li><p>EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models - arXiv, <a class="reference external" href="https://arxiv.org/html/2510.05942v1">https://arxiv.org/html/2510.05942v1</a></p></li>
<li><p>Arxiv Dives - Direct Preference Optimization (DPO) - <a class="reference external" href="http://Oxen.ai">Oxen.ai</a>, <a class="reference external" href="https://www.oxen.ai/blog/arxiv-dives-direct-preference-optimization-dpo">https://www.oxen.ai/blog/arxiv-dives-direct-preference-optimization-dpo</a></p></li>
<li><p>Direct Preference Optimization: Your Language Model is Secretly a Reward Model - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2305.18290">https://arxiv.org/pdf/2305.18290</a></p></li>
<li><p>DPO Trainer - Hugging Face, <a class="reference external" href="https://huggingface.co/docs/trl/en/dpo_trainer">https://huggingface.co/docs/trl/en/dpo_trainer</a></p></li>
<li><p>Why Everyone Is Switching from RLHF to DPO? | by Shahidullah Kawsar | Oct, 2025, <a class="reference external" href="https://kawsar34.medium.com/why-everyone-is-switching-from-rlhf-to-dpo-0bf86b56269a">https://kawsar34.medium.com/why-everyone-is-switching-from-rlhf-to-dpo-0bf86b56269a</a></p></li>
<li><p>Direct Preference Optimization: Your Language Model is Secretly a …, <a class="reference external" href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a></p></li>
<li><p>BOOTSTRAPPING LANGUAGE MODELS WITH DPO IMPLICIT REWARDS - ICLR Proceedings, <a class="reference external" href="https://proceedings.iclr.cc/paper_files/paper/2025/file/8c4de96b9169aa869cc102afe31055e8-Paper-Conference.pdf">https://proceedings.iclr.cc/paper_files/paper/2025/file/8c4de96b9169aa869cc102afe31055e8-Paper-Conference.pdf</a></p></li>
<li><p>Step-level Value Preference Optimization for Mathematical Reasoning - arXiv, <a class="reference external" href="https://arxiv.org/html/2406.10858v1">https://arxiv.org/html/2406.10858v1</a></p></li>
<li><p>Bootstrapping Language Models with DPO Implicit Rewards - arXiv, <a class="reference external" href="https://arxiv.org/html/2406.09760v2">https://arxiv.org/html/2406.09760v2</a></p></li>
<li><p>Direct Preference Optimization (DPO) | by João Lages - Medium, <a class="reference external" href="https://medium.com/&#64;joaolages/direct-preference-optimization-dpo-622fc1f18707">https://medium.com/&#64;joaolages/direct-preference-optimization-dpo-622fc1f18707</a></p></li>
<li><p>RLHF without RL - Direct Preference Optimization | ICLR Blogposts 2024, <a class="reference external" href="https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/">https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/</a></p></li>
<li><p>How to align open LLMs in 2025 with DPO &amp; and synthetic data - Philschmid, <a class="reference external" href="https://www.philschmid.de/rl-with-llms-in-2025-dpo">https://www.philschmid.de/rl-with-llms-in-2025-dpo</a></p></li>
<li><p>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study - OpenReview, <a class="reference external" href="https://openreview.net/forum?id=6XH8R7YrSk&amp;amp;referrer=%5Bthe+profile+of+Yi+Wu%5D">https://openreview.net/forum?id=6XH8R7YrSk&amp;referrer=[the+profile+of+Yi+Wu]</a>(/profile?id%3D~Yi_Wu1)</p></li>
<li><p>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study - arXiv, <a class="reference external" href="https://arxiv.org/html/2404.10719v1">https://arxiv.org/html/2404.10719v1</a></p></li>
<li><p>Is DPO Superior to PPO for LLM Alignment? A Comprehensive …, <a class="reference external" href="https://openreview.net/forum?id=6XH8R7YrSk">https://openreview.net/forum?id=6XH8R7YrSk</a></p></li>
<li><p>D.P.O vs R.L.H.F : A Battle for Fine-Tuning Supremacy in Language Models - Medium, <a class="reference external" href="https://medium.com/&#64;sinarya.114/d-p-o-vs-r-l-h-f-a-battle-for-fine-tuning-supremacy-in-language-models-04b273e7a173">https://medium.com/&#64;sinarya.114/d-p-o-vs-r-l-h-f-a-battle-for-fine-tuning-supremacy-in-language-models-04b273e7a173</a></p></li>
<li><p>RLHF and alternatives: IPO - Argilla, <a class="reference external" href="https://argilla.io/blog/mantisnlp-rlhf-part-6/">https://argilla.io/blog/mantisnlp-rlhf-part-6/</a></p></li>
<li><p>Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling - arXiv, <a class="reference external" href="https://arxiv.org/html/2506.08681v1">https://arxiv.org/html/2506.08681v1</a></p></li>
<li><p>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models | OpenReview, <a class="reference external" href="https://openreview.net/forum?id=FhTAG591Ve">https://openreview.net/forum?id=FhTAG591Ve</a></p></li>
<li><p>Robust LLM Alignment via Distributionally Robust Direct Preference …, <a class="reference external" href="https://arxiv.org/abs/2502.01930">https://arxiv.org/abs/2502.01930</a></p></li>
<li><p>Claude AI 2025: Everything You Must Know Before Getting Started | by Wajid Ali - Medium, <a class="reference external" href="https://medium.com/&#64;officewajidali/claude-ai-2025-everything-you-must-know-before-getting-started-c629a78ad583">https://medium.com/&#64;officewajidali/claude-ai-2025-everything-you-must-know-before-getting-started-c629a78ad583</a></p></li>
<li><p>Constitutional AI: Harmlessness from AI Feedback - Anthropic, <a class="reference external" href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback">https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback</a></p></li>
<li><p>What is Constitutional AI (CAI)? - Zilliz Learn, <a class="reference external" href="https://zilliz.com/learn/constitutional-ai-harmlessness-from-ai-feedback">https://zilliz.com/learn/constitutional-ai-harmlessness-from-ai-feedback</a></p></li>
<li><p>What Is Constitutional AI? How It Works &amp; Benefits | GigaSpaces AI, <a class="reference external" href="https://www.gigaspaces.com/data-terms/constitutional-ai">https://www.gigaspaces.com/data-terms/constitutional-ai</a></p></li>
<li><p>Constitutional AI: Harmlessness from AI Feedback — NVIDIA NeMo …, <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/24.09/modelalignment/cai.html">https://docs.nvidia.com/nemo-framework/user-guide/24.09/modelalignment/cai.html</a></p></li>
<li><p>Claude AI’s Constitutional Framework: A Technical Guide to Constitutional AI | by Generative AI | Medium, <a class="reference external" href="https://medium.com/&#64;genai.works/claude-ais-constitutional-framework-a-technical-guide-to-constitutional-ai-704942e24a21">https://medium.com/&#64;genai.works/claude-ais-constitutional-framework-a-technical-guide-to-constitutional-ai-704942e24a21</a></p></li>
<li><p>Claude’s Constitution \ Anthropic, <a class="reference external" href="https://www.anthropic.com/news/claudes-constitution">https://www.anthropic.com/news/claudes-constitution</a></p></li>
<li><p>Understanding Constitutional AI - Medium, <a class="reference external" href="https://medium.com/&#64;jonnyndavis/understanding-constitutional-ai-dd9d783ef712">https://medium.com/&#64;jonnyndavis/understanding-constitutional-ai-dd9d783ef712</a></p></li>
<li><p>Specific versus General Principles for Constitutional AI - Anthropic, <a class="reference external" href="https://www.anthropic.com/research/specific-versus-general-principles-for-constitutional-ai">https://www.anthropic.com/research/specific-versus-general-principles-for-constitutional-ai</a></p></li>
<li><p>arXiv:2305.20050v1 [cs.LG] 31 May 2023, <a class="reference external" href="https://arxiv.org/pdf/2305.20050">https://arxiv.org/pdf/2305.20050</a></p></li>
<li><p>[R] New OpenAI article: Improving Mathematical Reasoning with Process Supervision : r/MachineLearning - Reddit, <a class="reference external" href="https://www.reddit.com/r/MachineLearning/comments/13wwzq9/r_new_openai_article_improving_mathematical/">https://www.reddit.com/r/MachineLearning/comments/13wwzq9/r_new_openai_article_improving_mathematical/</a></p></li>
<li><p>Demystifying Multilingual Chain-of-Thought in Process Reward Modeling - arXiv, <a class="reference external" href="https://arxiv.org/html/2502.12663v1">https://arxiv.org/html/2502.12663v1</a></p></li>
<li><p>Improve Mathematical Reasoning in Language Models by Automated Process Supervision - arXiv, <a class="reference external" href="https://arxiv.org/pdf/2406.06592">https://arxiv.org/pdf/2406.06592</a></p></li>
<li><p>Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs - arXiv, <a class="reference external" href="https://arxiv.org/html/2406.09136v1">https://arxiv.org/html/2406.09136v1</a></p></li>
<li><p>RLAIF: Scaling Reinforcement Learning from Human Feedback with AI… - OpenReview, <a class="reference external" href="https://openreview.net/forum?id=AAxIs3D2ZZ">https://openreview.net/forum?id=AAxIs3D2ZZ</a></p></li>
<li><p>RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback, <a class="reference external" href="https://proceedings.mlr.press/v235/lee24t.html">https://proceedings.mlr.press/v235/lee24t.html</a></p></li>
<li><p>RLAIF Is The Future. But What Could Go Wrong? | by Reya Vir - Medium, <a class="reference external" href="https://medium.com/&#64;reyavir/rlaif-is-the-future-but-what-could-go-wrong-d86f1a6956f0">https://medium.com/&#64;reyavir/rlaif-is-the-future-but-what-could-go-wrong-d86f1a6956f0</a></p></li>
<li><p>RLAIF vs. RLHF: Scaling Reinforcement Learning from … - arXiv, <a class="reference external" href="https://arxiv.org/abs/2309.00267">https://arxiv.org/abs/2309.00267</a></p></li>
<li><p>Aligning and Augmenting Intelligence: A Technical Survey of …, <a class="reference external" href="https://www.findingtheta.com/blog/aligning-and-augmenting-intelligence-a-technical-survey-of-reinforcement-learning-in-large-language-models">https://www.findingtheta.com/blog/aligning-and-augmenting-intelligence-a-technical-survey-of-reinforcement-learning-in-large-language-models</a></p></li>
<li><p>RLTHF: Targeted Human Feedback for LLM Alignment - ICML 2025, <a class="reference external" href="https://icml.cc/virtual/2025/poster/46173">https://icml.cc/virtual/2025/poster/46173</a></p></li>
<li><p>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.acl-short.20.pdf">https://aclanthology.org/2025.acl-short.20.pdf</a></p></li>
<li><p>Re-evaluating Automatic LLM System Ranking for Alignment with Human Preference - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.findings-naacl.260.pdf">https://aclanthology.org/2025.findings-naacl.260.pdf</a></p></li>
<li><p>TRL - Transformer Reinforcement Learning - Hugging Face, <a class="reference external" href="https://huggingface.co/docs/trl/en/index">https://huggingface.co/docs/trl/en/index</a></p></li>
<li><p>huggingface/trl: Train transformer language models with reinforcement learning. - GitHub, <a class="github reference external" href="https://github.com/huggingface/trl">huggingface/trl</a></p></li>
<li><p>RLHF in 2024 with DPO &amp; Hugging Face - Philschmid, <a class="reference external" href="https://www.philschmid.de/dpo-align-llms-in-2024-with-trl">https://www.philschmid.de/dpo-align-llms-in-2024-with-trl</a></p></li>
<li><p>Preference Tuning LLMs with Direct Preference Optimization Methods, <a class="reference external" href="https://huggingface.co/blog/pref-tuning">https://huggingface.co/blog/pref-tuning</a></p></li>
<li><p>Preference Optimization for Vision Language Models with TRL - Hugging Face, <a class="reference external" href="https://huggingface.co/blog/dpo_vlm">https://huggingface.co/blog/dpo_vlm</a></p></li>
<li><p>Vision Language Model Alignment in TRL ⚡️ - Hugging Face, <a class="reference external" href="https://huggingface.co/blog/trl-vlm-alignment">https://huggingface.co/blog/trl-vlm-alignment</a></p></li>
<li><p>OpenRLHF/OpenRLHF-M: An Easy-to-use, Scalable and High-performance RLHF Framework designed for Multimodal Models. - GitHub, <a class="github reference external" href="https://github.com/OpenRLHF/OpenRLHF-M">OpenRLHF/OpenRLHF-M</a></p></li>
<li><p>OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework - arXiv, <a class="reference external" href="https://arxiv.org/html/2405.11143v6">https://arxiv.org/html/2405.11143v6</a></p></li>
<li><p>Welcome to OpenRLHF’s documentation! — OpenRLHF 0.9 …, <a class="reference external" href="https://openrlhf.readthedocs.io/">https://openrlhf.readthedocs.io/</a></p></li>
<li><p>Accelerating RLHF with vLLM, Best Practice from OpenRLHF, <a class="reference external" href="https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html">https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html</a></p></li>
<li><p>Inside vLLM: Anatomy of a High-Throughput LLM Inference System, <a class="reference external" href="https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html">https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html</a></p></li>
<li><p>OpenRLHF/OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO &amp; GRPO &amp; REINFORCE++ &amp; vLLM &amp; Ray &amp; Dynamic Sampling &amp; Async Agentic RL) - GitHub, <a class="github reference external" href="https://github.com/OpenRLHF/OpenRLHF">OpenRLHF/OpenRLHF</a></p></li>
<li><p>SFT vs. DPO: Comparison between LLM Alignment techniques | by Sulbha Jain | Medium, <a class="reference external" href="https://medium.com/&#64;sulbha.jindal/sft-vs-dpo-comparison-between-llm-alignment-techniques-26b6d76171da">https://medium.com/&#64;sulbha.jindal/sft-vs-dpo-comparison-between-llm-alignment-techniques-26b6d76171da</a></p></li>
<li><p>Fine-Tuning Techniques - Choosing Between SFT, DPO, and RFT (With a Guide to DPO), <a class="reference external" href="https://cookbook.openai.com/examples/fine_tuning_direct_preference_optimization_guide">https://cookbook.openai.com/examples/fine_tuning_direct_preference_optimization_guide</a></p></li>
<li><p><a class="reference external" href="http://arxiv.org">arxiv.org</a>, <a class="reference external" href="https://arxiv.org/html/2509.09055v1">https://arxiv.org/html/2509.09055v1</a></p></li>
<li><p>Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M, <a class="reference external" href="https://www.researchgate.net/publication/395418157_Improving_LLM_Safety_and_Helpfulness_using_SFT_and_DPO_A_Study_on_OPT-350M">https://www.researchgate.net/publication/395418157_Improving_LLM_Safety_and_Helpfulness_using_SFT_and_DPO_A_Study_on_OPT-350M</a></p></li>
<li><p>Extended Abstract - CS 224R Deep Reinforcement Learning, <a class="reference external" href="https://cs224r.stanford.edu/projects/pdfs/CS_224R_Final_Report_Bennett_Padmanabhan_Weissberg.pdf">https://cs224r.stanford.edu/projects/pdfs/CS_224R_Final_Report_Bennett_Padmanabhan_Weissberg.pdf</a></p></li>
<li><p>PAD: PERSONALIZED ALIGNMENT OF LLMS AT DECODING-TIME - OpenReview, <a class="reference external" href="https://openreview.net/pdf?id=e7AUJpP8bV">https://openreview.net/pdf?id=e7AUJpP8bV</a></p></li>
<li><p>[2507.19672] Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges - arXiv, <a class="reference external" href="https://arxiv.org/abs/2507.19672">https://arxiv.org/abs/2507.19672</a></p></li>
<li><p>liyongqi2002/Awesome-Personalized-Alignment - GitHub, <a class="github reference external" href="https://github.com/liyongqi2002/Awesome-Personalized-Alignment">liyongqi2002/Awesome-Personalized-Alignment</a></p></li>
<li><p>A Survey on Personalized and Pluralistic Preference Alignment in …, <a class="reference external" href="https://arxiv.org/abs/2504.07070">https://arxiv.org/abs/2504.07070</a></p></li>
<li><p>Aligning LLMs with Individual Preferences via Interaction - ACL Anthology, <a class="reference external" href="https://aclanthology.org/2025.coling-main.511/">https://aclanthology.org/2025.coling-main.511/</a></p></li>
<li><p>[2410.04070] PAD: Personalized Alignment of LLMs at Decoding-Time - arXiv, <a class="reference external" href="https://arxiv.org/abs/2410.04070">https://arxiv.org/abs/2410.04070</a></p></li>
<li><p>Aligning Multimodal LLM with Human Preference: A Survey - arXiv, <a class="reference external" href="https://arxiv.org/abs/2503.14504">https://arxiv.org/abs/2503.14504</a></p></li>
<li><p>Lecture 4 – Multimodal Alignment (MIT How to AI Almost Anything, Spring 2025) - YouTube, <a class="reference external" href="https://www.youtube.com/watch?v=kixc1mh55yY">https://www.youtube.com/watch?v=kixc1mh55yY</a></p></li>
<li><p>Understanding Alignment in Multimodal LLMs: A Comprehensive Study | OpenReview, <a class="reference external" href="https://openreview.net/forum?id=49qqV4NTdy&amp;amp;noteId=BmpGFgu040">https://openreview.net/forum?id=49qqV4NTdy&amp;noteId=BmpGFgu040</a></p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/deepnlp-2025",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/deepnlp-2025"        data-repo-id="R_kgDOPjTLcA"        data-category="General"        data-category-id="DIC_kwDOPjTLcM4Cuy8e"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="ko"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week09/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 9: 고급 RAG 아키텍처</p>
      </div>
    </a>
    <a class="right-next"
       href="../week11/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 11: 프로덕션 에이전트 시스템</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf">1. 정렬의 필요성과 기존 RLHF의 한계</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.1. 정렬의 정의: 도움성과 무해성</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.2. 기존 RLHF 파이프라인 검토</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.3. 2025년에 진단된 RLHF의 근본적 한계</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-direct-preference-optimization">2. DPO(Direct Preference Optimization): 보상 모델 없는 직접 최적화</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.1. 핵심 아이디어: 강화학습을 분류로 변환</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf-dpo">2.2. 수학적 심화: RLHF 목적함수와 DPO의 암시적 보상 모델</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-ppo-rlhf">2.3. 2025년 논쟁: DPO가 항상 PPO(RLHF)보다 우수한가?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-dpo">2.4. 최신 변종 (2025): 분포 강건성을 위한 Robust DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constitutional-ai-cai">3. Constitutional AI(CAI): 원칙 기반 자기 수정</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anthropic-ai">3.1. Anthropic의 접근법: 인간 피드백을 AI 피드백으로 대체</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cai-2-sl-cai-rl-cai">3.2. CAI의 2단계 학습 과정 상세 분석 (SL-CAI &amp; RL-CAI)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.3. 헌법 구성: 실제 원칙 예시 (2025년 표준)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-supervision">4. Process Supervision: 결과보다 과정을 중시</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prm-process-supervised-reward-models-vs-orm-outcome-supervised-reward-models">4.1. PRM(Process-supervised Reward Models) vs ORM(Outcome-supervised Reward Models)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prm">4.2. PRM이 다단계 추론(예: 수학)에 더 효과적인 이유</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-rl-from-ai-feedback">5. RLAIF(RL from AI Feedback): 확장성과 편향 증폭</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-llm-as-a-judge">5.1. AI 평가자의 필요성(‘LLM-as-a-judge’)과 작동 방식</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlaif-vs-rlhf">5.2. RLAIF vs RLHF 벤치마크: 동등하거나 우수한 성능</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">5.3. 핵심 위험: AI 판단 모델로부터의 상속 및 증폭된 편향</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">6. 실용적 구현: 최신 오픈소스 프레임워크 분석</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-trl-sfttrainer-dpotrainer">6.1. Hugging Face TRL: 실무자를 위한 도구키트 (SFTTrainer, DPOTrainer)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#openrlhf">6.2. OpenRLHF: 고성능 분산 훈련</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2-7b-dpo-vs-rlhf">7. 실습: LLaMA 2 7B – DPO vs RLHF 정렬 비교</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">7.1 실험 준비: 라이브러리와 데이터셋</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">7.2 DPO 방법 미세 조정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf-ppo">7.3 RLHF(PPO) 방법 미세 조정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">7.4 출력 평가 및 비교</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">8. 최신 연구 동향: 개인화와 멀티모달</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">8.1. ‘평균 정렬’을 넘어서: 개인화된 정렬</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">8.2. 텍스트를 넘어서: 멀티모달 정렬</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">체크포인트 질문</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">참고자료</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
